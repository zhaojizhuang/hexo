<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[k8s iptables]]></title>
    <url>%2F2020%2F03%2F28%2Fk8s-iptageles%2F</url>
    <content type="text"><![CDATA[k8s svc之 iptables规则iptables规则 参考 http://www.zsythink.net/archives/tag/iptables/ 用户空间，例如从pod中流出的流量就是从ouput链流出 上图表示的iptables的链，链 和表的关系如下，以PREROUTING链为例 这幅图是什么意思呢？它的意思是说，prerouting”链”只拥有nat表、raw表和mangle表所对应的功能，所以，prerouting中的规则只能存放于nat表、raw表和mangle表中。 NATNAT的三种类型: SNAT iptables -t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3# 目标流向eth0，源地址是xxx的，做SNAT，源地址改为xxx DNAT iptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3-192.168.5.5 MASQUERADE 是SNAT的一种，可以自动获取网卡的ip来做SNAT，如果是ADSL这种动态ip的，如果用SNAT需要经常更改iptables规则 iptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j MASQUERADE# 源地址是xxx，流向eth0的，流向做自动化SNAT masquerade 应为英文伪装 iptabels 常用命令iptables [-t 表名] 管理选项 [链名] [匹配条件] [-j 控制类型]# 控制类型包括 ACCETP REJECT DROP LOG 还有自定义的链（k8s的链）等iptabels -t nat（表名） -nvL POSTROUTING(链的名字) https://www.jianshu.com/p/ee4ee15d3658 分析k8s下的iptables规则以如下 service 为例 Name: testapi-smzdm-comNamespace: zhongce-v2-0Labels: &lt;none&gt;Selector: zdm-app-owner=testapi-smzdm-comType: LoadBalancerIP: 172.17.185.22LoadBalancer Ingress: 10.42.162.216Port: &lt;unset&gt; 809/TCPTargetPort: 809/TCPNodePort: &lt;unset&gt; 39746/TCPEndpoints: 10.42.147.255:809,10.42.38.222:809Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; 即 cluster ip 为 172.17.185.22 后端 podip 为 10.42.147.25510.42.38.222此外还有1个 loadbalancer ip 10.42.162.216 svc的访问路径- 集群内部，通过 `clusterip` 到访问到后端 `pod - 集群外部，通过直接访问`nodeport`；或者通过 `elb` 负载均衡到 `node` 上再通过 `nodeport` 访问 cluster ip 的基本原理如果是集群内的应用访问 cluster ip，那就是从用户空间访问内核空间网络协议栈,走的是 OUTPUT 链 从OUTPUT 链开始 [root@10-42-8-102 ~]# iptables -t nat -nvL OUTPUTChain OUTPUT (policy ACCEPT 4 packets, 240 bytes) pkts bytes target prot opt in out source destination 3424K 209M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ OUTPUT下的规则 直接把流量交给 KUBE-SERVICES 链 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICESChain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 172.17.185.22 /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809 0 0 KUBE-SVC-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 172.17.185.22 /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809 10 520 KUBE-FW-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 10.42.162.216 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809 0 0 KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTY 上述3条规则是顺序执行的： 第1条规则匹配发往 Cluster IP 172.17.185.22 的流量，跳转到了 KUBE-MARK-MASQ 链进一步处理，其作用就是打了一个 MARK ，稍后展开说明。 第2条规则匹配发往 Cluster IP 172.17.185.22 的流量，跳转到了 KUBE-SVC-G3OM5DSD2HHDMN6U 链进一步处理，稍后展开说明。 第3条规则匹配发往集群外 LB IP 的 10.42.162.216 的流量，跳转到了KUBE-FW-G3OM5DSD2HHDMN6U 链进一步处理，稍后展开说明。 第4条 KUBE-NODEPORTS的规则在末尾，只要dst ip是node 本机ip的话 （（–dst-type LOCAL），就跳转到KUBE-NODEPORTS做进一步判定：） 第2条规则要做dnat转发到后端具体的后端pod上 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SVC-G3OM5DSD2HHDMN6UChain KUBE-SVC-G3OM5DSD2HHDMN6U (3 references) pkts bytes target prot opt in out source destination 18 936 KUBE-SEP-JT2KW6YUTVPLLGV6 all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.50000000000 21 1092 KUBE-SEP-VETLC6CJY2HOK3EL all -- * * 0.0.0.0/0 0.0.0.0/0 两条 对应 后端pod的链 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-JT2KW6YUTVPLLGV6Chain KUBE-SEP-JT2KW6YUTVPLLGV6 (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 10.42.147.255 0.0.0.0/0 26 1352 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp to:10.42.147.255:809[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-VETLC6CJY2HOK3ELChain KUBE-SEP-VETLC6CJY2HOK3EL (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 10.42.38.222 0.0.0.0/0 2 104 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp to:10.42.38.222:809 流量经过路由表从eth0出去，在流量流出本机之前会经过POSTROUTING 链 在流量离开本机的时候会经过 POSTROUTING 链[root@10-42-8-102 ~]# iptables -t nat -nvL POSTROUTINGChain POSTROUTING (policy ACCEPT 274 packets, 17340 bytes) pkts bytes target prot opt in out source destination 632M 36G KUBE-POSTROUTING all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes postrouting rules */[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-POSTROUTINGChain KUBE-POSTROUTING (1 references) pkts bytes target prot opt in out source destination 526 27352 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service traffic requiring SNAT */ mark match 0x4000/0x4000 其实直接就跳转到了 KUBE-POSTROUTING，然后匹配打过0x4000 MARK 的流量，将其做 SNAT 转换，而这个 MARK 其实就是之前没说的 KUBE-MARK-MASQ 做的事情 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-MARK-MASQChain KUBE-MARK-MASQ (183 references) pkts bytes target prot opt in out source destination 492 25604 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 MARK or 0x4000 当流量离开本机时，src IP会被修改为node的IP，而不是发出流量的POD IP了 通过loadbalance ip进行访问最后还有一个KUBE-FW-G3OM5DSD2HHDMN6U链没有讲，从本机发往LB IP的流量要做啥事情呢？ 其实也是让流量直接发往具体某个Endpoints，就别真的发往LB了，这样才能获得最佳的延迟：[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-FW-G3OM5DSD2HHDMN6UChain KUBE-FW-G3OM5DSD2HHDMN6U (1 references) pkts bytes target prot opt in out source destination 2 104 KUBE-MARK-MASQ all -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ 2 104 KUBE-SVC-G3OM5DSD2HHDMN6U all -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ 0 0 KUBE-MARK-DROP all -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ 通过nodeport 来访问回顾一下 KUBE_SERVICES规则 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICESChain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ tcp -- * * !172.17.0.0/16 172.17.185.22 /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809 0 0 KUBE-SVC-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 172.17.185.22 /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809 10 520 KUBE-FW-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 10.42.162.216 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809 0 0 KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL KUBE-NODEPORTS 是最后一条规则 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-NODEPORTSChain KUBE-NODEPORTS (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746 0 0 KUBE-SVC-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746 第1条匹配dst port如果是39746，那么就打mark。第2条匹配dst port如果是39746，那么就跳到负载均衡链做DNAT改写。 总结 KUBE-SERVICES 链的规则存在于 OUTPUT POSTROUTING PREROUTING 三个链上 对于 KUBE-SERVICES KUBE-NDOEPORTS-xxx KUBE-SEP-xxx 下都会对符合条件（匹配条件）的规则打上MARK 可以重复打MARK 在流量出node的时候做SNAT 从集群内出去的流量怎么回来出node的流量在做SNAT的时候，netfilter有个连接跟踪机制，保存在 conntrack记录中 这就是Netfilter的连接跟踪（conntrack）功能了。对于TCP协议来讲，肯定是上来先建立一个连接，可以用源/目的IP+源/目的端口 （四元组），唯一标识一条连接，这个连接会放在conntrack表里面。当时是这台机器去请求163网站的，虽然源地址已经Snat成公网IP地址了，但是 conntrack 表里面还是有这个连接的记录的。当163网站返回数据的时候，会找到记录，从而找到正确的私网IP地址。 参考文档k8s 的iptales规则详解]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器网络]]></title>
    <url>%2F2020%2F03%2F25%2Fcontainer-network%2F</url>
    <content type="text"><![CDATA[容器网络vxlanvxlan原理 overlay网络VXLAN通过MAC-in-UDP的报文封装，实现了二层报文在三层网络上的透传 Flannel首先，flannel利用Kubernetes-API(这里就是取node.spec.podCIDR)或者etcd用于存储整个集群的网络配置，其中最主要的内容为设置集群的网络地址空间。例如，设定整个集群内所有容器的IP都取自网段“10.1.0.0/16”。 接着，flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配。 flannel 的 UDP 模式和 Vxlan 模式 host-gw 模式 UDP 模式是 三层 overlay,即，将原始数据包的三层包（IP包）装在 UDP 包里,通过 ip+端口 传到目的地，ip为目标node ip 端口为目标节点上flanneld进程监听的8285端口，解析后传入flannel0设备进入内核网络协议栈，UDP模式下 封包解包是在 flanneld里进行的也就是用户态下 重要！！！ 《深入解析kubernetes》 33章 https://time.geekbang.org/column/article/65287 VxLan 模式 是二层 overlay,即将原始Ethernet包（MAC包）封装起来，通过vtep设备发到目的vtep，vxlan是内核模块，vtep是flannneld创建的，vxlan封包解封完全是在内核态完成的 注意点 inner mac 为 目的vtep的mac outer ip为目的node的ip 这一点和UDP有区别下一跳ip对应的mac地址是ARP表里记录的，inner mac对应的arp记录是 flanneld维护的，outer mac arp表是node自学习的 host-gw 模式的工作原理,是在 节点上加路由表，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。 $ ip route...&lt;目的容器IP地址段&gt; via &lt;网关的IP地址&gt; dev eth0# 网关的 IP 地址，正是目的容器所在宿主机的 IP 地址 Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。如果分布在不同的子网里是不行的，只是三层可达 POD IP的分配使用CNI后，即配置了 kubelet 的 --network-plugin=cni，容器的IP分配：kubelet 先创建pause容器生成network namespace调用 网络driver CNI driverCNI driver 根据配置调用具体的cni 插件cni 插件给pause 容器配置网络pod 中其他的容器都使用 pause 容器的网络 CNM模式Pod IP是docker engine分配的，Pod也是以docker0为网关，通过veth连接network namespace flannel的两种方式 CNI CNM总结CNI中，docker0的ip与Pod无关，Pod总是生成的时候才去动态的申请自己的IP，而CNM模式下，Pod的网段在docker engine启动时就已经决定。CNI只是一个网络接口规范，各种功能都由插件实现，flannel只是插件的一种，而且docker也只是容器载体的一种选择，Kubernetes还可以使用其他的， cluster IP的分配是在kube-apiserver中 `pkg/registry/core/service/ipallocator`中分配的 network policyingress:- from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client 像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系，表示的是yaml数组里的两个元素 ingress:- from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client 像上面这样定义的 namespaceSelector 和 podSelector，是“与”（AND）的关系，yaml里表示的是一个数组元素的两个字段 Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。 通过NodePort来访问service的话，client的源ip会被做SNAT client \ ^ \ \ v \ node 1 &lt;--- node 2 | ^ SNAT | | ---&gt; v |endpoint 流程： 客户端发送数据包到 node2:nodePort node2 使用它自己的 IP 地址替换数据包的源 IP 地址（SNAT） node2 使用 pod IP 地址替换数据包的目的 IP 地址 数据包被路由到 node 1，然后交给 endpoint Pod 的回复被路由回 node2 Pod 的回复被发送回给客户端 可以将 service.spec.externalTrafficPolicy 的值为 Local，请求就只会被代理到本地 endpoints 而不会被转发到其它节点。这样就保留了最初的源 IP 地址 不会对访问NodePort的client ip做 SNAT了。如果没有本地 endpoints，发送到这个节点的数据包将会被丢弃。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>容器网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式锁]]></title>
    <url>%2F2020%2F02%2F10%2Ffen-bu-shi-suo%2F</url>
    <content type="text"><![CDATA[分布式锁k8s 的选主机制通过生成一个 k8s资源来实现 https://blog.csdn.net/weixin_39961559/article/details/81877056 etcd自己实现的锁和选主https://yq.aliyun.com/articles/70546 Etcd 的 v3 版本官方 client 里有一个 concurrency 的包，里面实现了分布式锁和选主。本文分析一下它是如何实现的。 锁的code https://github.com/coreos/etcd/blob/master/clientv3/concurrency/mutex.go#L26 选主的实现与锁的实现非常类似 https://github.com/coreos/etcd/blob/master/clientv3/concurrency/election.go#L31 etcd 的分布式锁，利用 etcd 的租约机制https://blog.51cto.com/5660061/2381931 package mainimport ( "context" "fmt" "github.com/coreos/etcd/clientv3" "time")func main() &#123; config := clientv3.Config&#123; Endpoints: []string&#123;"127.0.0.1:2379"&#125;, DialTimeout: 5 * time.Second, &#125; client, err := clientv3.New(config) if err != nil &#123; fmt.Println(err) &#125; lease := clientv3.NewLease(client) // Grant：分配一个租约。 // Revoke：释放一个租约。 // TimeToLive：获取剩余TTL时间。 // Leases：列举所有etcd中的租约。 // KeepAlive：自动定时的续约某个租约。 leaseResp, err := lease.Grant(context.TODO(), 10) //创建一个租约，它有10秒的TTL： if err != nil &#123; fmt.Println(err) &#125; leaseID := leaseResp.ID ctx, cancelFunc := context.WithCancel(context.TODO()) // 两个defer用于释放锁 defer cancelFunc() defer lease.Revoke(context.TODO(), leaseID) // 抢锁和占用期间，需要不停的续租，续租方法返回一个只读的channel keepChan, err := lease.KeepAlive(ctx, leaseID) if err != nil &#123; fmt.Println(err) &#125; // 处理续租返回的信息 go func() &#123; for &#123; select &#123; case keepResp := &lt;-keepChan: if keepChan == nil &#123; fmt.Println("lease out") goto END &#125; else &#123; fmt.Println("get resp", keepResp.ID) &#125; &#125; &#125; END: &#125;() kv := clientv3.NewKV(client) //put一个kv，让它与租约关联起来，从而实现10秒后自动过期 putResp, err := kv.Put(context.TODO(), "/cron/lock/job1", "", clientv3.WithLease(leaseID)) if err != nil &#123; fmt.Println(err) return &#125; fmt.Println("写入成功:", putResp.Header.Revision) //定时看key过期没 for &#123; getResp, err := kv.Get(context.TODO(), "/cron/lock/job1") if err != nil &#123; fmt.Println(err) return &#125; if getResp.Count == 0 &#123; fmt.Println("kv过期了") break &#125; fmt.Println("还没过期:", getResp.Kvs) time.Sleep(time.Second) &#125;&#125; package mainimport ( "context" "fmt" "go.etcd.io/etcd/clientv3" "time")type ETCDMutex struct &#123; Ttl int64 Conf clientv3.Config Key string cancel context.CancelFunc lease clientv3.Lease leaseID clientv3.LeaseID txn clientv3.Txn&#125;func (em *ETCDMutex) init() error &#123; client, err := clientv3.New(em.Conf) if err != nil &#123; return err &#125; em.txn = clientv3.KV(client).Txn(context.TODO()) if err != nil &#123; return err &#125; em.lease = clientv3.NewLease(client) leaseResp, err := em.lease.Grant(context.TODO(), em.Ttl) if err != nil &#123; return err &#125; var ctx context.Context ctx, em.cancel = context.WithCancel(context.TODO()) em.leaseID = leaseResp.ID _, err = em.lease.KeepAlive(ctx, em.leaseID) return err&#125;func (em *ETCDMutex) lock() error &#123; err := em.init() if err != nil &#123; return err &#125; // CreateRevision ==0 表示key不存在 //LOCK: txnResp, err := em.txn.If(clientv3.Compare(clientv3.CreateRevision(em.Key), "=", 0)). Then(clientv3.OpPut(em.Key, "", clientv3.WithLease(em.leaseID))).Commit() if err != nil &#123; return err &#125; if !txnResp.Succeeded &#123; //判断txn.if条件是否成立 return fmt.Errorf("抢锁失败") &#125; return nil&#125;func (em *ETCDMutex) UnLock() &#123; em.cancel() em.lease.Revoke(context.TODO(), em.leaseID) fmt.Println("释放了锁")&#125;func main() &#123; var conf = clientv3.Config&#123; Endpoints: []string&#123;"172.16.196.129:2380", "192.168.50.250:2380"&#125;, DialTimeout: 5 * time.Second, &#125; eMutex1 := &amp;ETCDMutex&#123; Conf: conf, Ttl: 10, Key: "lock", &#125; eMutex2 := &amp;ETCDMutex&#123; Conf: conf, Ttl: 10, Key: "lock", &#125; //groutine1 go func() &#123; err := eMutex1.lock() if err != nil &#123; fmt.Println("groutine1抢锁失败") fmt.Println(err) return &#125; //可以做点其他事，比如访问和操作分布式资源 fmt.Println("groutine1抢锁成功") time.Sleep(10 * time.Second) defer eMutex1.UnLock() &#125;() //groutine2 go func() &#123; err := eMutex2.lock() if err != nil &#123; fmt.Println("groutine2抢锁失败") fmt.Println(err) return &#125; //可以做点其他事，比如访问和操作分布式资源 fmt.Println("groutine2抢锁成功") defer eMutex2.UnLock() &#125;() time.Sleep(30 * time.Second)&#125; https://studygolang.com/articles/16307?fr=sidebar]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go 内存管理]]></title>
    <url>%2F2019%2F11%2F12%2Fneicunguanliyufenpei%2F</url>
    <content type="text"><![CDATA[Go这门语言抛弃了C/C++中的开发者管理内存的方式：主动申请与主动释放，增加了逃逸分析和GC，将开发者从内存管理中释放出来，让开发者有更多的精力去关注软件设计，而不是底层的内存问题。这是Go语言成为高生产力语言的原因之一 引自【 Go内存分配那些事，就这么简单！】 堆内存的分配先看下面这段代码，思考下 smallStruct 会被分配在堆上还是栈上: package maintype smallStruct struct &#123; a, b int64 c, d float64&#125;func main() &#123; smallAllocation()&#125;//go:noinlinefunc smallAllocation() *smallStruct &#123; return &amp;smallStruct&#123;&#125;&#125; 通过 annotation //go:noinline 禁用内联函数，不然这里不会产生堆内存的分配 【逃逸分析】 Inline 内联: 是在编译期间发生的，将函数调用调用处替换为被调用函数主体的一种编译器优化手段。 将文件保存为 main.go, 并执行 go tool compile &quot;-m&quot; main.go ,查看Go 堆内存的分配 【逃逸分析】的过程 如果不加 annotation //go:noinline 可以用go build -gcflags &#39;-m -l&#39; main.go -l可以禁止内联函数，效果是一样的，下面是逃逸分析的结果： main.go:14:9: &amp;smallStruct literal escapes to heap 再来看这段代码生成的汇编指令来详细的展示内存分配的过程, 执行下面 go tool compile -S main.go 0x001d 00029 (main.go:14) LEAQ type."".smallStruct(SB), AX0x0024 00036 (main.go:14) PCDATA $0, $00x0024 00036 (main.go:14) MOVQ AX, (SP)0x0028 00040 (main.go:14) CALL runtime.newobject(SB) runtime.newobject 是 Go 内置的申请堆内存的函数，对于堆内存的分配，Go 中有两种策略: 大内存的分配和小内存的分配 小内存的分配从 P 的 mcache 中分配对于小于 32kb的小内存，Go 会尝试在 P 的 本地缓存 mcache 中分配, mcache 保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以 无锁访问 每个 M 绑定一个 P 来运行一个goroutine, 在分配内存时，当前的 goroutine 在当前 P 的本地缓存 mcache 中查找对应的span， 从 span list 中来查找第一个可用的空闲 span span class 分为 8 bytes ~ 32k bytes 共 66 种类型（还有个大小为0的 size class0，并未用到，用于大对象的堆内存分配），分别对应不同的内存大小,mspan里保存对应大小的object， 1个 size class 对应2个 span class，2个 span class 的 span 大小相同，只是功能不同，1个用来存放包含指针的对象，一个用来存放不包含指针的对象，不包含指针对象的 Span 就无需 GC 扫描了。 前面的例子里，struct的大小为 (64bit/8)*4=8bit*4=32b 所以会在 span class 大小为 32bytes 的 mspan 里分配 从全局的缓存 mcentral 中分配当 mcache中没有空闲的 span 时怎么办呢，Go 还维护了一个全局的缓存 mcentral, mcentral 和 mcache 一样，都134个 span class 级别(67个 size class )，但每个级别都保存了2个span list，即2个span链表： nonempty：这个链表里的span，所有span都至少有1个空闲的对象空间。这些span是mcache释放span时加入到该链表的。 empty：这个链表里的span，所有的span都不确定里面是否有空闲的对象空间。当一个span交给mcache的时候，就会加入到empty链表。 mcache从mcentral获取和归还mspan的流程：引自【 图解Go语言内存分配|码农桃花源】 获取：加锁；从 nonempty 链表找到一个可用的 mspan ；并将其从 nonempty 链表删除；将取出的 mspan 加入到 empty 链表；将 mspan 返回给工作线程；解锁。 归还：加锁；将 mspan 从 empty 链表删除；将 mspan 加入到 nonempty 链表；解锁。 另外，GC 扫描的时候会把部分 mspan 标记为未使用，并将对应的 mspan 加入到 nonempty list 中 mcache 从 mcentral获取 过程如下： mcentral 从 heap 中分配当 mcentral 中的 nonempty list 没有可分配的对象的时候，Go会从 mheap 中分配对象，并链接到 nonempty list 上, mheap 必要时会向系统申请内存 mheap 中还有 arenas ,主要是为了大块内存需要，arena 也是用 mspan 组织的 大内存的分配大内存的分配就比较简单了，大于 32kb 的内存都会在 mheap 中直接分配 总结go 内存分配的概览 参考文章 Go内存分配那些事，就这么简单！:https://lessisbetter.site/2019/07/06/go-memory-allocation 图解Go语言内存分配|码农桃花源:https://qcrao.com/2019/03/13/graphic-go-memory-allocation 聊一聊goroutine stack：https://zhuanlan.zhihu.com/p/28409657]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【左耳听风】系列专栏]]></title>
    <url>%2F2019%2F07%2F10%2Fzuoertingfeng%2F</url>
    <content type="text"><![CDATA[【左耳听风】系列专栏 洞悉技术的本质，享受科技的乐趣陈皓网名“左耳朵耗子”，资深技术专家，骨灰级程序员 专栏介绍“左耳听风”专栏是由陈皓（网名“左耳朵耗子”）贡献和维护的全年付费专栏。 陈皓，人称耗子叔，是一位四十多岁的中年老男人，体型肥胖，白发苍苍，观点鲜明，个性十足。 他有 20 年软件开发及相关工作经验，先后在阿里巴巴、亚马逊、汤森路透等知名公司任职，对 IT底层技术平台有深入的了解，尤其在大规模分布式系统的基础架构方面颇有研究。此外，他在团队管理、项目管理，以及程序员个人成长等方面也有自己一套独特的见解和方法。 从 2002 年开始写技术博客，到 2009 年左右在独立域名 CoolShell.cn（酷壳）上分享技术观点和实践总结，陈皓通过一篇篇观点鲜明、文风犀利的文章吸引了大量IT从业人员的关注，影响了成千上万程序员在技术选型、求职就业、个人成长等方面的思考和发展。 虽然有些会被读者认同，有些则会引来争议，但他一直坚持观点鲜明的写作风格，并希望这些观点能引起大家的讨论和点评。他认为，只有这样，分享才更有意义，也能让大家都能从中收获更多。 除了继续保持观点鲜明、犀利的行文风格，在“左耳听风”专栏中的每篇文章都是陈皓对自己多年“堵过的枪眼儿”“填过的坑儿”的深入思考和凝练，是一些与个人或企业切身利益相关的内容，或者说是更具指导性、更为商业化的内容。用他自己的话说，是一些非常来之不易的宝贵经验。 福利地址https://www.zybuluo.com/zhaojizhuang/note/1478325]]></content>
      <tags>
        <tag>分布式</tag>
        <tag>人生思考</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈 epoll]]></title>
    <url>%2F2019%2F05%2F10%2Fepoll%2F</url>
    <content type="text"><![CDATA[epollIO 多路复用目前支持I/O多路复用的系统调用有 select，pselect，poll，epoll ，I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作 select调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写），或者超时，函数返回。当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。 select的流程 假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，操作系统把进程A分别加入这三个socket的等待队列中 当任何一个socket收到数据后，中断程序将唤起进程,将进程从所有fd（socket）的等待队列中移除，再将进程加入到工作队列里面 进程A被唤醒后，它知道至少有一个socket接收了数据。程序需遍历一遍socket列表，可以得到就绪的socket 缺点： 其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。 其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。 poll与select一样，只是去掉了 1024的限制epollepoll 事先通过 epoll_ctl() 来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个文件描述符，当进程调用 epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。) epoll使用一个文件描述符(eventpoll)管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次 int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int epfd = epoll_create(...);epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中while(1)&#123; int n = epoll_wait(...) for(接收到数据的socket)&#123; //处理 &#125;&#125; 流程：首先创建 epoll对象创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket 假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程 当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，进程A可以知道哪些socket发生了变化。 参考文章 https://www.jianshu.com/p/dfd940e7fca2 2 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(1) 3 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(2) 4 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(3)]]></content>
      <tags>
        <tag>linux</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go 学习笔记]]></title>
    <url>%2F2019%2F04%2F05%2Fgo%20shen%20ru%20fen%20xi%2F</url>
    <content type="text"><![CDATA[Go 学习笔记go程序是如何运行的参考链接1 defer 源码分析参考链接 defer、return、返回值三者的执行逻辑应该是：return最先执行，return负责将结果写入返回值中；接着defer开始执行一些收尾工作；最后函数携带当前返回值退出 逃逸分析 堆栈分配参考链接 go build -gcflags &#39;-m -l&#39; xxx.go 就可以看到逃逸分析的过程和结果 go性能大杀器 pprof参考链接1参考链接2 ### pprof中自带 web 火焰图，需要安装graphvizgo tool pprof -http=:8181 xxx,pprof 下面的语句 可以结合代码查看哪个函数用时最多go tool pprof main.go xxxx.prof 进入pprof后执行 list &lt;函数名&gt; 对于web开放的pprof （在http的go程序中 添加 _ &quot;net/http/pprof&quot;的import,会增加 debug/pprof 的endpoint),结束后将默认进入 pprof 的交互式命令模式go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60go tool pprof http://localhost:6060/debug/pprof/heap go性能大杀器 trace同pprof 对于web开放的pprof （在http的go程序中 添加 _ &quot;net/http/pprof&quot;的import curl http://127.0.0.1:6060/debug/pprof/trace\?seconds\=20 &gt; trace.outgo tool trace trace.out # 此处和pprof不同，不用加 -http=:8181 这里他会自动选择端口 对于后台应用,后台程序main启动时添加 trace.Start(os.Stderr)直接运行下面的命令即可 go run main.go 2&gt; trace.out 它能够跟踪捕获各种执行中的事件，例如 Goroutine 的创建/阻塞/解除阻塞，Syscall 的进入/退出/阻止，GC 事件，Heap 的大小改变，Processor 启动/停止等等 interface 不含有任何方法的 interface type eface struct &#123; // 16 bytes _type *_type data unsafe.Pointer&#125; 含有 方法的 interface type iface struct &#123; // 16 bytes tab *itab data unsafe.Pointer&#125; 变量类型 结构体实现接口 结构体指针实现接口 结构体初始化变量 通过 不通过 结构体指针初始化变量 通过 通过 不通过的如下 type Duck interface &#123; Quack()&#125;type Cat struct&#123;&#125;func (c *Cat) Quack() &#123; fmt.Println("meow")&#125;func main() &#123; var c Duck = Cat&#123;&#125; // 将结构体变量传到指针类型接受的函数是不行的，反过来可行 c.Quack()&#125;$ go build interface.go./interface.go:20:6: cannot use Cat literal (type Cat) as type Duck in assignment: Cat does not implement Duck (Quack method has pointer receiver) Go中函数调用都是值拷贝，使用 c.Quack() 调用方法时都会发生值拷贝： 对于 &amp;Cat{} 来说，这意味着拷贝一个新的 &amp;Cat{} 指针，这个指针与原来的指针指向一个相同并且唯一的结构体，所以编译器可以隐式的对变量解引用（dereference）获取指针指向的结构体； 对于 Cat{} 来说，这意味着 Quack 方法会接受一个全新的 Cat{}，因为方法的参数是*Cat，编译器不会无中生有创建一个新的指针；即使编译器可以创建新指针，这个指针指向的也不是最初调用该方法的结构体； panicpanic只会调用当前Goroutine的defer（） func main() &#123; defer println("in main") go func() &#123; defer println("in goroutine") panic("") &#125;() time.Sleep(1 * time.Second)&#125;$ go run main.goin goroutinepanic: make newnew 返回的是指针，指向一个type类型内存空间的指针new等价于 var a typeA // tpyeA的零值b:=&amp;a 但是 new不能对 chanel map slice进行初始化 ，这几个必须经过make进行结构体的初始化才能用 epoll参考文章 https://www.jianshu.com/p/dfd940e7fca2 2 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(1) 3 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(2) 4 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(3) IO 多路复用 &amp; epoll目前支持I/O多路复用的系统调用有 select，pselect，poll，epoll ，I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作 select调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写），或者超时，函数返回。当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。 select的流程 假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，操作系统把进程A分别加入这三个socket的等待队列中 当任何一个socket收到数据后，中断程序将唤起进程,将进程从所有fd（socket）的等待队列中移除，再将进程加入到工作队列里面 进程A被唤醒后，它知道至少有一个socket接收了数据。程序需遍历一遍socket列表，可以得到就绪的socket 缺点： 其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。 其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。 poll与select一样，只是去掉了 1024的限制epollepoll 事先通过 epoll_ctl() 来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个文件描述符，当进程调用 epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。) epoll使用一个文件描述符(eventpoll)管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次 int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int epfd = epoll_create(...);epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中while(1)&#123; int n = epoll_wait(...) for(接收到数据的socket)&#123; //处理 &#125;&#125; 流程：首先创建 epoll对象创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket 假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程 当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，进程A可以知道哪些socket发生了变化。]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>epoll</tag>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈 Go map]]></title>
    <url>%2F2019%2F04%2F02%2Fgomap%2F</url>
    <content type="text"><![CDATA[map原理分析map 结构体type hmap struct &#123; count int // 元素的个数 flags uint8 // 状态标志 B uint8 // 可以最多容纳 6.5 * 2 ^ B 个元素，6.5为装载因子 noverflow uint16 // 溢出的个数 hash0 uint32 // 哈希种子 buckets unsafe.Pointer // 桶的地址 oldbuckets unsafe.Pointer // 旧桶的地址，用于扩容 nevacuate uintptr // 搬迁进度，小于nevacuate的已经搬迁 overflow *[2]*[]*bmap &#125;// A bucket for a Go map.type bmap struct &#123; // 每个元素hash值的高8位，如果tophash[0] &lt; minTopHash，表示这个桶的搬迁状态 tophash [bucketCnt]uint8 // bucketCnt是常量8,接下来是8个key、8个value，但是我们不能直接看到；为了优化对齐，go采用了key放在一起，value放在一起的存储方式，8个k，8个v得内存地址 // 再接下来是hash冲突发生时，下一个溢出桶的地址&#125; bmap不只tophash还有两个方法 overflow 和setoverflow func (b *bmap) overflow(t *maptype) *bmap &#123; return *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize))&#125;func (b *bmap) setoverflow(t *maptype, ovf *bmap) &#123; *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize)) = ovf&#125; hmap中的buckets中的原色bucket就是bmap，即 buckets[0],bucket[1],… bucket[2^B-1]如下图 bucket就是bmap bmap 是存放 k-v 的地方，我们把视角拉近，仔细看 bmap 的内部组成。 key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32 例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是： 10010111 | 000011110110110010001111001010100010010110010101010 │ 01010 用最后的 5 个 bit 位，也就是 01010，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。 再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。 buckets 编号就是桶编号，当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key hash冲突的两种表示方式： 开放寻址法（hash冲突时，在当前index往后查找第一个空的位置即可） 拉链法 map在写入过程会发生扩容，runtime.mapassign 函数会在以下两种情况发生时触发哈希的扩容： 装载因子已经超过 6.5；装载因子=总数量/桶的数量 哈希使用了太多溢出桶；溢出捅的数量 超过正常桶的数量 即 noverflow 大于 1&lt;&lt;B buckets 每次都会将桶的数量翻倍 扩容机制： 翻倍扩容：哈希在存储元素过多时状态会触发扩容操作，每次都会将桶的数量翻倍，整个扩容过程并不是原子的，而是通过 runtime.growWork 增量触发的，在扩容期间访问哈希表时会使用旧桶，向哈希表写入数据时会触发旧桶元素的分流； 等量扩容，为了解决大量写入、删除造成的内存泄漏问题，哈希引入了 sameSizeGrow这一机制，在出现较多溢出桶时会对哈希进行『内存整理』减少对空间的占用。 参考链接 https://www.jianshu.com/p/aa0d4808cbb8 https://segmentfault.com/a/1190000018387055]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解思科 Tetration 平台]]></title>
    <url>%2F2017%2F06%2F12%2Fcisco-platform%2F</url>
    <content type="text"><![CDATA[介绍 思科推出了 Tetration Analytics平台，Tetration 平台主要是对大规模数据中心和云平台上的网络流量的实时采集、存储和分析。 Tetration 平台搭配基于Cloud Scale技术的硬件设备，流动在数据中心的任何一个数据包的元信息都可以被实时记录和存储下来。 Tetration 平台可以辅助用户在应用关系梳理、应用访问策略制定、模拟和实时验证、应用云端迁移访问策略制定、白名单安全模型等方面脱离传统手工和被动的工作方式。 Tetration Analytics平台主要由 数据采集部分、数据存储部分、和数据分析部分组成。 数据采集部分：包括安装在实体服务器或者虚拟机中的软件数据采集器、以太网交换机转发芯片的硬件数据采集逻辑和第三方数据接口组成。软件数据采集器通过 libpcap（一个网络数据包捕获函数库，linux抓包工具tcpdump就是基于此的）来对数据进行采集。 存储和分析部分：由基于思科UCS计算平台的服务器集群组成。 下图是思科Tetration Analytics平台架构 存储和分析部分是该平台的精髓所在，针对万亿个数据的无监督机器学习算法的采用，为网络访问行为基线设立、网络访问异常检测、应用访问关系的动态甄别、聚类动态划分等提供了方便的工具。平台为用户提供了网络数据完善的、全面的大数据来源。 Tetration Analytics平台提供了存储和分析的接口，用户可以根据数据进行相应的网络数据分析，提供的接口包括 开放式 API、REST、推送事件、用户应用 平台特性思科 Tetration Analytics 能够分析应用行为，并准确地反映出应用之间的依赖关系。它采用机器学习技术构建动态分层策略模型，从而实现应用分段和自动策略实施 Tetration Analytics 可大幅简化零信任模式的实施。它可以针对数据中心内的任何对象实时提供可视性。它使用基于行为的应用洞察和机器学习技术来构建动态策略模型，实现自动策略实施。此外，它还通过 REST API 支持开放式访问，客户可以编写个性化应用。 遥感勘测快上加快 Tetration Analytics 使用无需监管的机器学习技术，以线速处理收集的遥感勘测数据。借助自然语言技术，搜索和浏览数百亿条数据流记录。只需不到一秒即可获得切实可行的见解。 切实可行的应用见解 依据应用组件和行为分析算法获取实时数据。标识应用组及其通信模式和服务依赖性。获取自动化白名单策略建议，实现零信任安全性。 应用分段 在本地数据中心以及公共云和私有云中实施一致的策略，实现应用分段。持续监控合规性偏差，可在几分钟内发现生产网络中的违规情况。 开放式 API 利用全面精细的遥感勘测数据，轻松打造个性化的定制应用。生成个性化的定制通知和查询。监控应用层的延迟情况并获取通知。此平台使用 REST API。 强大的可扩展性 Tetration Analytics 从数据中心的每个数据包收集遥感勘测数据。它可以在几秒钟内分析数百万个事件并从数十亿条记录中提供切实可行的见解。它可以长期保留数据，而不会丢失细节。ddd]]></content>
      <categories>
        <category>数据中心</category>
      </categories>
      <tags>
        <tag> Analytics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 介绍及其应用]]></title>
    <url>%2F2017%2F06%2F11%2FDocker%2F</url>
    <content type="text"><![CDATA[重要链接！！！知乎上的深入浅出 1.Docker 介绍便于入题，首先用 Docker 的logo解释下： 那个大鲸鱼（或者是货轮）就是操作系统 把要交付的应用程序看成是各种货物，原本要将各种各样形状、尺寸不同的货物放到大鲸鱼上，你得为每件货物考虑怎么安放（就是应用程序配套的环境），还得考虑货物和货物是否能叠起来（应用程序依赖的环境是否会冲突）。 现在使用了集装箱（容器）把每件货物都放到集装箱里，这样大鲸鱼可以用同样地方式安放、堆叠集装了，省事省力。参考知乎 步入正题： Docker 是 PaaS 提供商 dotCloud 开源的一个基于 LXC 的高级容器引擎，源代码托管在 Github 上, 基于go语言并遵从Apache2.0协议开源。 Docker可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。 如图所示，Docker 使用客户端-服务器 (C/S) 架构模式。 Docker 客户端会与 Docker 守护进程进行通信。 Docker 守护进程会处理复杂繁重的任务，例如建立、运行、发布你的 Docker 容器。 Docker 客户端和守护进程 Daemon 可以运行在同一个系统上，当然你也可以使用 Docker 客户端去连接一个远程的 Docker 守护进程。Docker 客户端和守护进程之间通过 socket 或者 RESTful API 进行通信，就像下图。 1.1 Docker 守护进程如上图所示，Docker 守护进程运行在一台主机上。用户并不直接和守护进程进行交互，而是通过 Docker 客户端间接和其通信。 1.2 Docker 客户端Docker 客户端，实际上是 docker 的二进制程序，是主要的用户与 Docker 交互方式。它接收用户指令并且与背后的 Docker 守护进程通信，如此来回往复。 1.3 Docker 内部要理解 Docker 内部构建，需要理解以下三种部件： Docker 镜像 - Docker imagesDocker 仓库 - Docker registeriesDocker 容器 - Docker containers Docker 镜像:Docker 镜像是 Docker 容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成。Docker 使用 UnionFS 来将这些层联合到单独的镜像中。UnionFS 允许独立文件系统中的文件和文件夹(称之为分支)被透明覆盖，形成一个单独连贯的文件系统。正因为有了这些层的存在，Docker 是如此的轻量。当你改变了一个 Docker 镜像，比如升级到某个程序到新的版本，一个新的层会被创建。因此，不用替换整个原先的镜像或者重新建立(在使用虚拟机的时候你可能会这么做)，只是一个新 的层被添加或升级了。现在你不用重新发布整个镜像，只需要升级，层使得分发 Docker 镜像变得简单和快速。 Docker 仓库:Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。同样的，Docker 仓库也有公有和私有的概念。公有的 Docker 仓库名字是 Docker Hub。Docker Hub 提供了庞大的镜像集合供使用。这些镜像可以是自己创建，或者在别人的镜像基础上创建。Docker 仓库是 Docker 的分发部分。 Docker 容器:Docker 容器和文件夹很类似，一个Docker容器包含了所有的某个应用运行所需要的环境。每一个 Docker 容器都是从 Docker 镜像创建的。Docker 容器可以运行、开始、停止、移动和删除。每一个 Docker 容器都是独立和安全的应用平台，Docker 容器是 Docker 的运行部分。 2. Docker 8个的应用场景 本小节介绍了常用的8个Docker的真实使用场景，分别是简化配置、代码流水线管理、提高开发效率、隔离应用、整合服务器、调试能力、多租户环境、快速部署 一些Docker的使用场景，它为你展示了如何借助Docker的优势，在低开销的情况下，打造一个一致性的环境。 1.简化配置这是Docker公司宣传的Docker的主要使用场景。虚拟机的最大好处是能在你的硬件设施上运行各种配置不一样的平台（软件、系统），Docker在降低额外开销的情况下提供了同样的功能。它能让你将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度。 2. 代码流水线（Code Pipeline）管理前一个场景对于管理代码的流水线起到了很大的帮助。代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。 3. 提高开发效率不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。 理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，之前使用虚拟的时候，我们经常需要为开发环境的机器加内存，而现在Docker可以轻易的让几十个服务在Docker中跑起来。 4. 隔离应用有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等。 我们经常需要考虑两点，一是因为要降低成本而进行服务器整合，二是将一个整体式的应用拆分成松耦合的单个服务（译者注：微服务架构）。 5. 整合服务器正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。 6. 调试能力Docker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。 7. 多租户环境另外一个Docker有意思的使用场景是在多租户的应用中，它可以避免关键应用的重写。我们一个特别的关于这个场景的例子是为IoT（物联网）的应用开发一个快速、易用的多租户环境。这种多租户的基本代码非常复杂，很难处理，重新规划这样一个应用不但消耗时间，也浪费金钱。 使用Docker，可以为每一个租户的应用层的多个实例创建隔离的环境，这不仅简单而且成本低廉，当然这一切得益于Docker环境的启动速度和其高效的diff命令。 8. 快速部署在虚拟机之前，引入新的硬件资源需要消耗几天的时间。虚拟化技术（Virtualization）将这个时间缩短到了分钟级别。而Docker通过为进程仅仅创建一个容器而无需启动一个操作系统，再次将这个过程缩短到了秒级。这正是Google和Facebook都看重的特性。 你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。 本文的几个概念 LXC: LXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。属于操作系统层次之上的虚拟化]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 中的并发]]></title>
    <url>%2F2017%2F06%2F10%2Fjava-create-thread%2F</url>
    <content type="text"><![CDATA[Java 中的并发如何创建一个线程按 Java 语言规范中的说法，创建线程只有一种方式，就是创建一个 Thread 对象。而从 HotSpot 虚拟机的角度看，创建一个虚拟机线程 有两种方式，一种是创建 Thread 对象，另一种是创建 一个本地线程，加入到虚拟机线程中。 如果从 Java 语法的角度。有两种方法。 第一是继承 Thread 类，实现 run 方法，并创建子类对象。 public void startThreadUseSubClass() &#123; class MyThread extends Thread &#123; public void run() &#123; System.out.println("start thread using Subclass of Thread"); &#125; &#125; MyThread thread = new MyThread(); thread.start();&#125; 另一种是传递给 Thread 构造函数一个 Runnable 对象。 public void startThreadUseRunnalbe() &#123; Thread thread = new Thread(new Runnable() &#123; public void run() &#123; System.out.println("start thread using runnable"); &#125; &#125;); thread.start();&#125; 当然， Runnalbe 对象，也不是只有这一种形式，例如如果我们想要线程执行时返回一个值，就需要用到另一种 Runnalbe 对象，它 对原来的 Runnalbe 对象进行了包装。 public void startFutureTask() &#123; FutureTask&lt;Integer&gt; task = new FutureTask&lt;&gt;(new Callable&lt;Integer&gt;() &#123; public Integer call() &#123; return 1; &#125; &#125;); new Thread(task).start(); try &#123; Integer result = task.get(); System.out.println("future result " + result); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125;&#125; 结束线程wait 与 sleepsleep 会使得当前线程休眠一段时间，但并不会释放已经得到的锁。 wait 会阻塞住，并释放已经得到的锁。一直到有人调用 notify 或者 notifyAll，它会重新尝试得到锁，然后再唤醒。 线程池好处 复用 线程池中有一系列线程，这些线程在执行完任务后，并不会被销毁，而会从任务队列中取出任务，执行这些任务。这样，就避免为每个任务 都创建线程，销毁线程。 在有大量短命线程的场景下，如果创建线程和销毁线程的时间比线程执行任务的时间还长，显然是不划算的，这时候，使用线程池就会有明显 的好处。 流控 同时，可以设置线程数目，这样，线程不会增大到影响系统整体性能的程度。当任务太多时，可以在队列中排队， 如果有空闲线程，他们会从队列中取出任务执行。 使用 线程数目 那么，线程的数目要设置成多少呢？这需要根据任务类型的不同来设置，假如是大量计算型的任务，他们不会阻塞，那么可以将线程数目设置 为处理器数目。而如果任务中涉及大量IO，有些线程会阻塞住，这样就要根据阻塞线程数目与运行线程数目的比例，以及处理器数目来设置 线程总数目。例如阻塞线程数目与运行线程数目之比为n, 处理器数目为p，那么可以设置 n * (p + 1) 个线程，保证有 n 个线程处于运行 状态。 Executors JDK 的 java.util.concurrent.Executors 类提供了几个静态的方法，用于创建不同类型的线程池。 ExecutorService service = Executors.newFixedThreadPool(10);ArrayList&lt;Future&lt;Integer&gt;&gt; results = new ArrayList&lt;&gt;();for (int i = 0; i &lt; 14; i++) &#123; Future&lt;Integer&gt; r = service.submit(new Callable&lt;Integer&gt;() &#123; public Integer call() &#123; return new Random().nextInt(); &#125;); results.add(r);&#125; newFixedThreadPool 可以创建固定数目的线程，一旦创建不会自动销毁线程，即便长期没有任务。除非显式关闭线程池。如果任务队列中有任务，就取出任务执行。 另外，还可以使用 newCachedThreadPool 方法创建一个不设定固定线程数目的线程池，它有一个特性，线程完成任务后，如果一分钟之内又有新任务，就会复用这个线程执行新任务。如果超过一分钟还没有任务执行，就会自动销毁。 另外，还提供了 newSingleThreadExecutor 创建有一个工作线程的线程池。 原理JDK 中的线程池通过 HashSet 存储工作者线程，通过 BlockingQueue 来存储待处理任务。 通过核心工作者数目(corePoolSize) 和 最大工作者数目(maximumPoolSize) 来确定如何处理任务。如果当前工作者线程数目 小于核心工作者数目，则创建一个工作者线程执行这个任务。否则，将这个任务放入待处理队列。如果入队失败，再看看当前工作 者数目是不是小于最大工作者数目，如果小于，则创建工作者线程执行这个任务。否则，拒绝执行这个任务。 另外，如果待处理队列中没有任务要处理，并且工作者线程数目超过了核心工作者数目，那么，需要减少工作者线程数目。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>线程 java</tag>
      </tags>
  </entry>
</search>
