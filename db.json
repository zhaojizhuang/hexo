{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/favicon.ico","path":"favicon.ico","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"8ca07c6d7b4ff1dd8ab4f14fde521ea3ce4eb84a","modified":1588776098133},{"_id":"source/CNAME","hash":"18087ef775bb6437f0dc7255e4f799375514deae","modified":1588292587932},{"_id":"source/favicon.ico","hash":"5e13a1eda3f3c0530d04857b87e89f320476d6ba","modified":1586186368000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1588776098173},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1588776098173},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1588776098173},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1588776098176},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1588776098176},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1588776098176},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1588776098176},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1588776098176},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1588776098177},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1588776098177},{"_id":"themes/next/README.cn.md","hash":"58ffe752bc4b7f0069fcd6304bbc2d5ff7b80f89","modified":1588776098177},{"_id":"themes/next/README.md","hash":"898213e66d34a46c3cf8446bf693bd50db0d3269","modified":1588776098177},{"_id":"themes/next/_config.yml","hash":"43332828fe2bed9adfd9be5321c84460f9be6d80","modified":1594818858848},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1588776098178},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1588776098178},{"_id":"themes/next/package.json","hash":"42d4e836442a0f12330a92769429cb530547989a","modified":1588776098194},{"_id":"source/_posts/.DS_Store","hash":"c7baf33ab7c4cdaeaff2eed5f4f1df0086c7b40b","modified":1588780621857},{"_id":"source/_posts/Docker.md","hash":"a8320f23b679b059ee3286e312290cb49db2dc8b","modified":1589895023654},{"_id":"source/_posts/cisco-platform.md","hash":"ee2e2d0074ab272cbb04a065cdf2fcf700ed0f9a","modified":1589895006852},{"_id":"source/_posts/confighpa.md","hash":"3c8d2dbb9a3567c70f482f6d5aa7ec7df450ca54","modified":1589901460517},{"_id":"source/_posts/epoll.md","hash":"2680ed062b4c227b0c026dcf0210c26853e221f5","modified":1589555768756},{"_id":"source/_posts/container-network.md","hash":"81d41380d9bafeafa44981f7eabdf935fcf8ead7","modified":1589557197315},{"_id":"source/_posts/go shen ru fen xi.md","hash":"e6e24b0f663fdf361e283a64a3840fee941dfeb4","modified":1589979288547},{"_id":"source/_posts/gomap.md","hash":"85221f0635248c8f214a38facd8765d5322ee4b3","modified":1589555495511},{"_id":"source/_posts/java-create-thread.md","hash":"a43f952e8b17acc9f46cb18d963a3b47d32f3eb9","modified":1589895057269},{"_id":"source/_posts/k8s-iptables.md","hash":"1fc023917ff8b70e35ce6d823a740982a4cc0e44","modified":1589894672496},{"_id":"source/_posts/neicunguanliyufenpei.md","hash":"ffa23e3a488ba461469b72bd92e97b9c02e00723","modified":1589555147221},{"_id":"source/_posts/k8s-svc.md","hash":"73a3216e24bab34561241f2b697ff6a846e2a673","modified":1589894636312},{"_id":"source/_posts/servicetopo.md","hash":"dd7e30b7ec83872ce95923c2f2c8550055cbb10e","modified":1589977458233},{"_id":"source/about/index.md","hash":"f7afe943a2dd777f42afbc593aa620c16588d95d","modified":1589899085676},{"_id":"source/categories/index.md","hash":"879e52f6c2cd86c1d9ae2c9d8d62e646c9635446","modified":1588778882910},{"_id":"source/ebook/index.md","hash":"bc542775a8d53dffe8ec3aee3da69c7523fbd232","modified":1588776098156},{"_id":"source/tags/index.md","hash":"1cc6fa1c074fc4ba12bac269e327b01584e50db1","modified":1589033868742},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1588776098174},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1588776098174},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1588776098175},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1588776098175},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1588776098178},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1588776098178},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1588776098178},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1588776098178},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1588776098179},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1588776098179},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1588776098179},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1588776098179},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1588776098180},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1588776098179},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1588776098180},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1588776098180},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1588776098180},{"_id":"themes/next/languages/zh-Hans.yml","hash":"536569a0b3dfc69c786f1a5ef605dfb48059e88c","modified":1588776098180},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1588776098181},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1588776098181},{"_id":"themes/next/layout/_layout.swig","hash":"735660d05ca423c3474ae6d19a183cc845c3f2a2","modified":1588781904258},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1588776098192},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1588776098193},{"_id":"themes/next/layout/google_adsense.swig","hash":"5b64bb3c36aa2347cd69e30d1c5d779c42c20090","modified":1588776098193},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1588776098193},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1588776098193},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1588776098193},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1588776098193},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1588776098194},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1588776098194},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1588776098194},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1588776098245},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1588776098245},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1588776098246},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588776098215},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1588776098181},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1588776098181},{"_id":"themes/next/layout/_macro/google_adsense.swig","hash":"5b64bb3c36aa2347cd69e30d1c5d779c42c20090","modified":1588776098182},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1588776098182},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1588776098182},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1588776098182},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1588776098182},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"65fe5c71a6924180c40215962cb0ceb8ef93e79a","modified":1588776098183},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1588776098183},{"_id":"themes/next/layout/_partials/comments.swig","hash":"6bae33586db5c2ea822f15347e8149adb6816765","modified":1589033522407},{"_id":"themes/next/layout/_partials/footer.swig","hash":"2ebff63b26a15f8a3bd9a32dbeb09bf1543648e7","modified":1588782981910},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1588776098184},{"_id":"themes/next/layout/_partials/head.swig","hash":"8f819113b88ee7e2b7349366b276e73d0c41ef8b","modified":1588776098183},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1588776098184},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1588776098184},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1588776098184},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1588776098185},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1588776098185},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1588776098186},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1588776098190},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1588776098190},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1588776098190},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1588776098190},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1588776098190},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1588776098190},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1588776098191},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1588776098195},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1588776098195},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1588776098195},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1588776098195},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1588776098195},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1588776098195},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1588776098196},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1588776098196},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1588776098196},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1588776098215},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1588776098215},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1588776098216},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1588776098216},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1588776098216},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1588776098216},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1588776098216},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1588776098216},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1588776098217},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1588776098217},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1588776098217},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1588776098217},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1588776098217},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588776098217},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1588776098218},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1588776098218},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1588776098218},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1588776098218},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1588776098218},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588776098186},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588776098186},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588776098209},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588776098209},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588776098210},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588776098214},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1588776098215},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1588776098183},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1588776098184},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1588776098184},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1588776098184},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1588776098185},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1588776098185},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1588776098185},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1588776098185},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1588776098185},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1588776098186},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1588776098186},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1588776098186},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1588776098189},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1588776098189},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1588776098189},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1588776098189},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1588776098189},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1588776098189},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1588776098189},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1588776098190},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"cd47989f957b06ed778b2137478bfb344187abc0","modified":1588776098190},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1588776098187},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1588776098187},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1588776098187},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1588776098187},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1588776098187},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1588776098187},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1588776098187},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1588776098187},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1588776098188},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1588776098188},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1588776098188},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1588776098188},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1588776098188},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1588776098191},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1588776098192},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1588776098192},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1588776098192},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1588776098208},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1588776098209},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1588776098209},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1588776098210},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1588776098214},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1588776098214},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1588776098215},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1588776098215},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1588776098218},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1588776098219},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1588776098219},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1588776098219},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1588776098219},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1588776098219},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1588776098219},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1588776098220},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1588776098220},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1588776098220},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1588776098220},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1588776098223},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1588776098225},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1588776098226},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1588776098228},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1588776098228},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1588776098229},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1588776098229},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1588776098229},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1588776098230},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1588776098230},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1588776098230},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1588776098230},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1588776098235},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1588776098235},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1588776098236},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1588776098236},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1588776098236},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1588776098236},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1588776098236},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1588776098237},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1588776098237},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1588776098237},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1588776098237},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1588776098237},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1588776098237},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1588776098238},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1588776098238},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1588776098238},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1588776098238},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1588776098238},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1588776098238},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1588776098239},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1588776098239},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1588776098239},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1588776098239},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1588776098239},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1588776098239},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1588776098240},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1588776098240},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1588776098243},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1588776098243},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1588776098245},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1588776098245},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1588776098245},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1588776098235},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1588776098191},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1588776098191},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1588776098196},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1588776098197},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1588776098197},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1588776098197},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1588776098197},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1588776098199},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1588776098203},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1588776098207},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1588776098207},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1588776098207},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1588776098207},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1588776098207},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1588776098208},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1588776098208},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1588776098210},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1588776098211},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1588776098211},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1588776098211},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1588776098211},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1588776098212},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1588776098212},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1588776098212},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1588776098213},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1588776098213},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"02fb8fa6b6c252b6bed469539cd057716606a787","modified":1588776098213},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1588776098213},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1588776098213},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1588776098213},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1588776098214},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1588776098214},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"596239ffa01da1dab45f49335adbf13f5069ac2e","modified":1588776098214},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1588776098214},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1588776098214},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1588776098220},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1588776098223},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1588776098223},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1588776098229},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1588776098229},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1588776098226},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1588776098226},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1588776098226},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1588776098226},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1588776098227},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1588776098227},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1588776098228},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1588776098228},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1588776098228},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1588776098230},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1588776098231},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1588776098231},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1588776098243},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1588776098243},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1588776098222},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1588776098234},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1588776098244},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1588776098197},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1588776098197},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1588776098198},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1588776098198},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1588776098198},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1588776098198},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1588776098198},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1588776098198},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1588776098198},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1588776098199},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1588776098199},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1588776098199},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1588776098199},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1588776098199},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1588776098199},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1588776098200},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1588776098200},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1588776098200},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"46616e2c6f42652a3f4bb86d520998ed2f2546ec","modified":1588776098200},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1588776098201},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1588776098201},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1588776098200},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1588776098201},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1588776098201},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1588776098201},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1588776098201},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1588776098201},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1588776098202},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1588776098202},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1588776098202},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1588776098202},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1588776098202},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1588776098202},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1588776098203},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1588776098202},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1588776098203},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1588776098203},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1588776098203},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1588776098203},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1588776098203},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1588776098204},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1588776098204},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1588776098204},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1588776098204},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1588776098204},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1588776098205},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1588776098205},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1588776098205},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1588776098205},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1588776098205},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1588776098206},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1588776098206},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1588776098206},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1588776098206},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1588776098206},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1588776098206},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1588776098206},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1588776098207},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1588776098212},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1588776098212},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1588776098213},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1588776098221},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1588776098221},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1588776098221},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1588776098221},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1588776098222},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1588776098222},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1588776098227},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1588776098227},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1588776098227},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1588776098227},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1588776098227},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1588776098228},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1588776098234},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1588776098231},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1588776098232},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1588776098234},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1588776098225},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1588776098242},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1588776098233}],"Category":[{"name":"docker","_id":"ckcnd4l850003r5flr6a9azh5"},{"name":"数据中心","_id":"ckcnd4l8b0008r5flwxtqrtfa"},{"name":"k8s","_id":"ckcnd4l8e000er5flgs76yxya"},{"name":"Go","_id":"ckcnd4l8p000pr5flh05numnf"},{"name":"Java","_id":"ckcnd4l8t0011r5fl6g2yyv19"}],"Data":[],"Page":[{"title":"壮哥的个人博客","date":"2018-12-08T08:46:18.000Z","_content":" \n赵吉壮，目前就职于华为 Cloud BU, 主要从事 k8s 相关特性设计与开发 \n专注于 k8s Go 云原生\n\n### 联系\n\n博客：[@壮哥的学习记录](http://zhaojizhuang.github.io/)\n微博：[@chumper](https://weibo.com/zhaojizhuang)\ncsdn：[@Kernel_Momo](https://blog.csdn.net/power886)\nGitHub：[@zhaojizhuang](https://github.com/zhaojizhuang)\n\n----\n\n### 大牛博客\n\n[酷壳|左耳朵耗子](https://coolshell.cn/)\n[Go语言充电站|大彬](https://lessisbetter.site/)\n[码农桃花源](https://qcrao.com/)\n[draveness](https://draveness.me/)\n[煎鱼](https://eddycjy.com/)\n","source":"about/index.md","raw":"---\ntitle: 壮哥的个人博客\ndate: 2018-12-08 16:46:18\n---\n \n赵吉壮，目前就职于华为 Cloud BU, 主要从事 k8s 相关特性设计与开发 \n专注于 k8s Go 云原生\n\n### 联系\n\n博客：[@壮哥的学习记录](http://zhaojizhuang.github.io/)\n微博：[@chumper](https://weibo.com/zhaojizhuang)\ncsdn：[@Kernel_Momo](https://blog.csdn.net/power886)\nGitHub：[@zhaojizhuang](https://github.com/zhaojizhuang)\n\n----\n\n### 大牛博客\n\n[酷壳|左耳朵耗子](https://coolshell.cn/)\n[Go语言充电站|大彬](https://lessisbetter.site/)\n[码农桃花源](https://qcrao.com/)\n[draveness](https://draveness.me/)\n[煎鱼](https://eddycjy.com/)\n","updated":"2020-05-19T14:38:05.676Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckcnd4l830001r5flqvlrebkt","content":"<p>赵吉壮，目前就职于华为 Cloud BU, 主要从事 k8s 相关特性设计与开发<br>专注于 k8s Go 云原生</p>\n<h3 id=\"联系\"><a href=\"#联系\" class=\"headerlink\" title=\"联系\"></a>联系</h3><p>博客：<a href=\"http://zhaojizhuang.github.io/\" target=\"_blank\" rel=\"noopener\">@壮哥的学习记录</a><br>微博：<a href=\"https://weibo.com/zhaojizhuang\" target=\"_blank\" rel=\"noopener\">@chumper</a><br>csdn：<a href=\"https://blog.csdn.net/power886\" target=\"_blank\" rel=\"noopener\">@Kernel_Momo</a><br>GitHub：<a href=\"https://github.com/zhaojizhuang\" target=\"_blank\" rel=\"noopener\">@zhaojizhuang</a></p>\n<hr>\n<h3 id=\"大牛博客\"><a href=\"#大牛博客\" class=\"headerlink\" title=\"大牛博客\"></a>大牛博客</h3><p><a href=\"https://coolshell.cn/\" target=\"_blank\" rel=\"noopener\">酷壳|左耳朵耗子</a><br><a href=\"https://lessisbetter.site/\" target=\"_blank\" rel=\"noopener\">Go语言充电站|大彬</a><br><a href=\"https://qcrao.com/\" target=\"_blank\" rel=\"noopener\">码农桃花源</a><br><a href=\"https://draveness.me/\" target=\"_blank\" rel=\"noopener\">draveness</a><br><a href=\"https://eddycjy.com/\" target=\"_blank\" rel=\"noopener\">煎鱼</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>赵吉壮，目前就职于华为 Cloud BU, 主要从事 k8s 相关特性设计与开发<br>专注于 k8s Go 云原生</p>\n<h3 id=\"联系\"><a href=\"#联系\" class=\"headerlink\" title=\"联系\"></a>联系</h3><p>博客：<a href=\"http://zhaojizhuang.github.io/\" target=\"_blank\" rel=\"noopener\">@壮哥的学习记录</a><br>微博：<a href=\"https://weibo.com/zhaojizhuang\" target=\"_blank\" rel=\"noopener\">@chumper</a><br>csdn：<a href=\"https://blog.csdn.net/power886\" target=\"_blank\" rel=\"noopener\">@Kernel_Momo</a><br>GitHub：<a href=\"https://github.com/zhaojizhuang\" target=\"_blank\" rel=\"noopener\">@zhaojizhuang</a></p>\n<hr>\n<h3 id=\"大牛博客\"><a href=\"#大牛博客\" class=\"headerlink\" title=\"大牛博客\"></a>大牛博客</h3><p><a href=\"https://coolshell.cn/\" target=\"_blank\" rel=\"noopener\">酷壳|左耳朵耗子</a><br><a href=\"https://lessisbetter.site/\" target=\"_blank\" rel=\"noopener\">Go语言充电站|大彬</a><br><a href=\"https://qcrao.com/\" target=\"_blank\" rel=\"noopener\">码农桃花源</a><br><a href=\"https://draveness.me/\" target=\"_blank\" rel=\"noopener\">draveness</a><br><a href=\"https://eddycjy.com/\" target=\"_blank\" rel=\"noopener\">煎鱼</a></p>\n"},{"title":"categories","date":"2017-07-10T08:36:26.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-07-10 16:36:26\ntype: \"categories\"\ncomments: false\n---\n","updated":"2020-05-06T15:28:02.910Z","path":"categories/index.html","layout":"page","_id":"ckcnd4lc8001xr5flhb2gjm38","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"电子书","date":"2019-12-08T10:17:56.000Z","_content":"","source":"ebook/index.md","raw":"---\ntitle: 电子书\ndate: 2019-12-08 18:17:56\n---\n","updated":"2020-05-06T14:41:38.156Z","path":"ebook/index.html","comments":1,"layout":"page","_id":"ckcnd4lc9001yr5fllp15oish","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-12-08T08:46:27.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-12-08 16:46:27\ntype: \"tags\"\ncomments: false\n---\n","updated":"2020-05-09T14:17:48.742Z","path":"tags/index.html","layout":"page","_id":"ckcnd4lca001zr5fleteuasl4","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"layout":"post","title":"Docker 介绍及其应用","date":"2017-06-11T15:40:18.000Z","author":"zhaojizhuang","mathjax":true,"_content":"\n\n\n> [重要链接！！！知乎上的深入浅出](https://www.zhihu.com/question/22969309)\n\n## 1.Docker 介绍\n**便于入题，首先用 Docker 的logo解释下：**\n![Docker logo](/images/what.jpg)\n\n那个大鲸鱼（或者是货轮）就是操作系统\n\n把要交付的应用程序看成是各种货物，原本要将各种各样形状、尺寸不同的货物放到大鲸鱼上，你得为每件货物考虑怎么安放（就是应用程序配套的环境），还得考虑货物和货物是否能叠起来（应用程序依赖的环境是否会冲突）。\n\n现在使用了集装箱（容器）把每件货物都放到集装箱里，这样大鲸鱼可以用同样地方式安放、堆叠集装了，省事省力。[参考知乎](https://www.zhihu.com/question/28300645/answer/50922662)\n\n**步入正题：**\n\nDocker 是 PaaS 提供商 dotCloud 开源的一个基于 [LXC](#lxc) 的高级容器引擎，源代码托管在 Github 上, 基于go语言并遵从Apache2.0协议开源。\n\nDocker可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。\n\n![Docker的c/s架构](/images/docker_cs.png)\n\n如图所示，Docker 使用客户端-服务器 (C/S) 架构模式。\n\n- Docker 客户端会与 Docker 守护进程进行通信。\n- Docker 守护进程会处理复杂繁重的任务，例如建立、运行、发布你的 Docker 容器。\n- Docker 客户端和守护进程 Daemon 可以运行在同一个系统上，当然你也可以使用 Docker 客户端去连接一个远程的 Docker 守护进程。Docker 客户端和守护进程之间通过 socket 或者 RESTful API 进行通信，就像下图。\n\n![docker架构图](/images/docker_frame.jpg)\n\n### 1.1 Docker 守护进程\n\n如上图所示，Docker 守护进程运行在一台主机上。用户并不直接和守护进程进行交互，而是通过 Docker 客户端间接和其通信。\n\n### 1.2 Docker 客户端\nDocker 客户端，实际上是 docker 的二进制程序，是主要的用户与 Docker 交互方式。它接收用户指令并且与背后的 Docker 守护进程通信，如此来回往复。\n\n### 1.3 Docker 内部\n要理解 Docker 内部构建，需要理解以下三种部件：\n\n**_Docker 镜像_** - Docker images  \n_**Docker 仓库**_ - Docker registeries  \n**_Docker 容器_** - Docker containers\n\n1.  **Docker 镜像**:Docker 镜像是 Docker 容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成。Docker 使用 UnionFS 来将这些层联合到单独的镜像中。UnionFS 允许独立文件系统中的文件和文件夹(称之为分支)被透明覆盖，形成一个单独连贯的文件系统。正因为有了这些层的存在，Docker 是如此的轻量。当你改变了一个 Docker 镜像，比如升级到某个程序到新的版本，一个新的层会被创建。因此，不用替换整个原先的镜像或者重新建立(在使用虚拟机的时候你可能会这么做)，只是一个新 的层被添加或升级了。现在你不用重新发布整个镜像，只需要升级，层使得分发 Docker 镜像变得简单和快速。\n\n2.  **Docker 仓库**:Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。同样的，Docker 仓库也有公有和私有的概念。公有的 Docker 仓库名字是 Docker Hub。Docker Hub 提供了庞大的镜像集合供使用。这些镜像可以是自己创建，或者在别人的镜像基础上创建。Docker 仓库是 Docker 的分发部分。\n\n3.  **Docker 容器**:Docker 容器和文件夹很类似，一个Docker容器包含了所有的某个应用运行所需要的环境。每一个 Docker 容器都是从 Docker 镜像创建的。Docker 容器可以运行、开始、停止、移动和删除。每一个 Docker 容器都是独立和安全的应用平台，Docker 容器是 Docker 的运行部分。\n\n## 2. Docker 8个的应用场景\n> 本小节介绍了常用的8个Docker的真实使用场景，分别是简化配置、代码流水线管理、提高开发效率、隔离应用、整合服务器、调试能力、多租户环境、快速部署\n\n![Docker 的8个应用场景](/images/dockeruse.png)\n\n一些Docker的使用场景，它为你展示了如何借助Docker的优势，在低开销的情况下，打造一个一致性的环境。\n\n### 1.简化配置\n\n这是Docker公司宣传的Docker的主要使用场景。虚拟机的最大好处是能在你的硬件设施上运行各种配置不一样的平台（软件、系统），Docker在降低额外开销的情况下提供了同样的功能。它能让你将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度。\n\n### 2. 代码流水线（Code Pipeline）管理\n\n前一个场景对于管理代码的流水线起到了很大的帮助。代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。\n\n### 3. 提高开发效率\n\n不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。\n\n理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，之前使用虚拟的时候，我们经常需要为开发环境的机器加内存，而现在Docker可以轻易的让几十个服务在Docker中跑起来。\n\n### 4. 隔离应用\n\n有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等。\n\n我们经常需要考虑两点，一是因为要降低成本而进行服务器整合，二是将一个整体式的应用拆分成松耦合的单个服务（译者注：微服务架构）。\n\n### 5. 整合服务器\n\n正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。\n\n### 6. 调试能力\n\nDocker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。\n\n### 7. 多租户环境\n\n另外一个Docker有意思的使用场景是在多租户的应用中，它可以避免关键应用的重写。我们一个特别的关于这个场景的例子是为IoT（物联网）的应用开发一个快速、易用的多租户环境。这种多租户的基本代码非常复杂，很难处理，重新规划这样一个应用不但消耗时间，也浪费金钱。\n\n使用Docker，可以为每一个租户的应用层的多个实例创建隔离的环境，这不仅简单而且成本低廉，当然这一切得益于Docker环境的启动速度和其高效的diff命令。\n\n### 8. 快速部署\n\n在虚拟机之前，引入新的硬件资源需要消耗几天的时间。虚拟化技术（Virtualization）将这个时间缩短到了分钟级别。而Docker通过为进程仅仅创建一个容器而无需启动一个操作系统，再次将这个过程缩短到了秒级。这正是Google和Facebook都看重的特性。\n\n你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。\n\n#### 本文的几个概念\n\n- <span id=\"lxc\">**LXC**</span>: LXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。属于操作系统层次之上的虚拟化\n","source":"_posts/Docker.md","raw":"---\nlayout: post\ntitle:  \"Docker 介绍及其应用\"\ndate:   2017-06-11 23:40:18 +0800\ncategories: docker\ntags:  [\"docker\"]\nauthor: zhaojizhuang\nmathjax: true\n---\n\n\n\n> [重要链接！！！知乎上的深入浅出](https://www.zhihu.com/question/22969309)\n\n## 1.Docker 介绍\n**便于入题，首先用 Docker 的logo解释下：**\n![Docker logo](/images/what.jpg)\n\n那个大鲸鱼（或者是货轮）就是操作系统\n\n把要交付的应用程序看成是各种货物，原本要将各种各样形状、尺寸不同的货物放到大鲸鱼上，你得为每件货物考虑怎么安放（就是应用程序配套的环境），还得考虑货物和货物是否能叠起来（应用程序依赖的环境是否会冲突）。\n\n现在使用了集装箱（容器）把每件货物都放到集装箱里，这样大鲸鱼可以用同样地方式安放、堆叠集装了，省事省力。[参考知乎](https://www.zhihu.com/question/28300645/answer/50922662)\n\n**步入正题：**\n\nDocker 是 PaaS 提供商 dotCloud 开源的一个基于 [LXC](#lxc) 的高级容器引擎，源代码托管在 Github 上, 基于go语言并遵从Apache2.0协议开源。\n\nDocker可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。\n\n![Docker的c/s架构](/images/docker_cs.png)\n\n如图所示，Docker 使用客户端-服务器 (C/S) 架构模式。\n\n- Docker 客户端会与 Docker 守护进程进行通信。\n- Docker 守护进程会处理复杂繁重的任务，例如建立、运行、发布你的 Docker 容器。\n- Docker 客户端和守护进程 Daemon 可以运行在同一个系统上，当然你也可以使用 Docker 客户端去连接一个远程的 Docker 守护进程。Docker 客户端和守护进程之间通过 socket 或者 RESTful API 进行通信，就像下图。\n\n![docker架构图](/images/docker_frame.jpg)\n\n### 1.1 Docker 守护进程\n\n如上图所示，Docker 守护进程运行在一台主机上。用户并不直接和守护进程进行交互，而是通过 Docker 客户端间接和其通信。\n\n### 1.2 Docker 客户端\nDocker 客户端，实际上是 docker 的二进制程序，是主要的用户与 Docker 交互方式。它接收用户指令并且与背后的 Docker 守护进程通信，如此来回往复。\n\n### 1.3 Docker 内部\n要理解 Docker 内部构建，需要理解以下三种部件：\n\n**_Docker 镜像_** - Docker images  \n_**Docker 仓库**_ - Docker registeries  \n**_Docker 容器_** - Docker containers\n\n1.  **Docker 镜像**:Docker 镜像是 Docker 容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成。Docker 使用 UnionFS 来将这些层联合到单独的镜像中。UnionFS 允许独立文件系统中的文件和文件夹(称之为分支)被透明覆盖，形成一个单独连贯的文件系统。正因为有了这些层的存在，Docker 是如此的轻量。当你改变了一个 Docker 镜像，比如升级到某个程序到新的版本，一个新的层会被创建。因此，不用替换整个原先的镜像或者重新建立(在使用虚拟机的时候你可能会这么做)，只是一个新 的层被添加或升级了。现在你不用重新发布整个镜像，只需要升级，层使得分发 Docker 镜像变得简单和快速。\n\n2.  **Docker 仓库**:Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。同样的，Docker 仓库也有公有和私有的概念。公有的 Docker 仓库名字是 Docker Hub。Docker Hub 提供了庞大的镜像集合供使用。这些镜像可以是自己创建，或者在别人的镜像基础上创建。Docker 仓库是 Docker 的分发部分。\n\n3.  **Docker 容器**:Docker 容器和文件夹很类似，一个Docker容器包含了所有的某个应用运行所需要的环境。每一个 Docker 容器都是从 Docker 镜像创建的。Docker 容器可以运行、开始、停止、移动和删除。每一个 Docker 容器都是独立和安全的应用平台，Docker 容器是 Docker 的运行部分。\n\n## 2. Docker 8个的应用场景\n> 本小节介绍了常用的8个Docker的真实使用场景，分别是简化配置、代码流水线管理、提高开发效率、隔离应用、整合服务器、调试能力、多租户环境、快速部署\n\n![Docker 的8个应用场景](/images/dockeruse.png)\n\n一些Docker的使用场景，它为你展示了如何借助Docker的优势，在低开销的情况下，打造一个一致性的环境。\n\n### 1.简化配置\n\n这是Docker公司宣传的Docker的主要使用场景。虚拟机的最大好处是能在你的硬件设施上运行各种配置不一样的平台（软件、系统），Docker在降低额外开销的情况下提供了同样的功能。它能让你将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度。\n\n### 2. 代码流水线（Code Pipeline）管理\n\n前一个场景对于管理代码的流水线起到了很大的帮助。代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。\n\n### 3. 提高开发效率\n\n不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。\n\n理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，之前使用虚拟的时候，我们经常需要为开发环境的机器加内存，而现在Docker可以轻易的让几十个服务在Docker中跑起来。\n\n### 4. 隔离应用\n\n有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等。\n\n我们经常需要考虑两点，一是因为要降低成本而进行服务器整合，二是将一个整体式的应用拆分成松耦合的单个服务（译者注：微服务架构）。\n\n### 5. 整合服务器\n\n正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。\n\n### 6. 调试能力\n\nDocker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。\n\n### 7. 多租户环境\n\n另外一个Docker有意思的使用场景是在多租户的应用中，它可以避免关键应用的重写。我们一个特别的关于这个场景的例子是为IoT（物联网）的应用开发一个快速、易用的多租户环境。这种多租户的基本代码非常复杂，很难处理，重新规划这样一个应用不但消耗时间，也浪费金钱。\n\n使用Docker，可以为每一个租户的应用层的多个实例创建隔离的环境，这不仅简单而且成本低廉，当然这一切得益于Docker环境的启动速度和其高效的diff命令。\n\n### 8. 快速部署\n\n在虚拟机之前，引入新的硬件资源需要消耗几天的时间。虚拟化技术（Virtualization）将这个时间缩短到了分钟级别。而Docker通过为进程仅仅创建一个容器而无需启动一个操作系统，再次将这个过程缩短到了秒级。这正是Google和Facebook都看重的特性。\n\n你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。\n\n#### 本文的几个概念\n\n- <span id=\"lxc\">**LXC**</span>: LXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。属于操作系统层次之上的虚拟化\n","slug":"Docker","published":1,"updated":"2020-05-19T13:30:23.654Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l7z0000r5fl0h98ckca","content":"<blockquote>\n<p><a href=\"https://www.zhihu.com/question/22969309\" target=\"_blank\" rel=\"noopener\">重要链接！！！知乎上的深入浅出</a></p>\n</blockquote>\n<h2 id=\"1-Docker-介绍\"><a href=\"#1-Docker-介绍\" class=\"headerlink\" title=\"1.Docker 介绍\"></a>1.Docker 介绍</h2><p><strong>便于入题，首先用 Docker 的logo解释下：</strong><br><img src=\"/images/what.jpg\" alt=\"Docker logo\"></p>\n<p>那个大鲸鱼（或者是货轮）就是操作系统</p>\n<p>把要交付的应用程序看成是各种货物，原本要将各种各样形状、尺寸不同的货物放到大鲸鱼上，你得为每件货物考虑怎么安放（就是应用程序配套的环境），还得考虑货物和货物是否能叠起来（应用程序依赖的环境是否会冲突）。</p>\n<p>现在使用了集装箱（容器）把每件货物都放到集装箱里，这样大鲸鱼可以用同样地方式安放、堆叠集装了，省事省力。<a href=\"https://www.zhihu.com/question/28300645/answer/50922662\" target=\"_blank\" rel=\"noopener\">参考知乎</a></p>\n<p><strong>步入正题：</strong></p>\n<p>Docker 是 PaaS 提供商 dotCloud 开源的一个基于 <a href=\"#lxc\">LXC</a> 的高级容器引擎，源代码托管在 Github 上, 基于go语言并遵从Apache2.0协议开源。</p>\n<p>Docker可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。</p>\n<p><img src=\"/images/docker_cs.png\" alt=\"Docker的c/s架构\"></p>\n<p>如图所示，Docker 使用客户端-服务器 (C/S) 架构模式。</p>\n<ul>\n<li>Docker 客户端会与 Docker 守护进程进行通信。</li>\n<li>Docker 守护进程会处理复杂繁重的任务，例如建立、运行、发布你的 Docker 容器。</li>\n<li>Docker 客户端和守护进程 Daemon 可以运行在同一个系统上，当然你也可以使用 Docker 客户端去连接一个远程的 Docker 守护进程。Docker 客户端和守护进程之间通过 socket 或者 RESTful API 进行通信，就像下图。</li>\n</ul>\n<p><img src=\"/images/docker_frame.jpg\" alt=\"docker架构图\"></p>\n<h3 id=\"1-1-Docker-守护进程\"><a href=\"#1-1-Docker-守护进程\" class=\"headerlink\" title=\"1.1 Docker 守护进程\"></a>1.1 Docker 守护进程</h3><p>如上图所示，Docker 守护进程运行在一台主机上。用户并不直接和守护进程进行交互，而是通过 Docker 客户端间接和其通信。</p>\n<h3 id=\"1-2-Docker-客户端\"><a href=\"#1-2-Docker-客户端\" class=\"headerlink\" title=\"1.2 Docker 客户端\"></a>1.2 Docker 客户端</h3><p>Docker 客户端，实际上是 docker 的二进制程序，是主要的用户与 Docker 交互方式。它接收用户指令并且与背后的 Docker 守护进程通信，如此来回往复。</p>\n<h3 id=\"1-3-Docker-内部\"><a href=\"#1-3-Docker-内部\" class=\"headerlink\" title=\"1.3 Docker 内部\"></a>1.3 Docker 内部</h3><p>要理解 Docker 内部构建，需要理解以下三种部件：</p>\n<p><strong><em>Docker 镜像</em></strong> - Docker images<br><em><strong>Docker 仓库</strong></em> - Docker registeries<br><strong><em>Docker 容器</em></strong> - Docker containers</p>\n<ol>\n<li><p><strong>Docker 镜像</strong>:Docker 镜像是 Docker 容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成。Docker 使用 UnionFS 来将这些层联合到单独的镜像中。UnionFS 允许独立文件系统中的文件和文件夹(称之为分支)被透明覆盖，形成一个单独连贯的文件系统。正因为有了这些层的存在，Docker 是如此的轻量。当你改变了一个 Docker 镜像，比如升级到某个程序到新的版本，一个新的层会被创建。因此，不用替换整个原先的镜像或者重新建立(在使用虚拟机的时候你可能会这么做)，只是一个新 的层被添加或升级了。现在你不用重新发布整个镜像，只需要升级，层使得分发 Docker 镜像变得简单和快速。</p>\n</li>\n<li><p><strong>Docker 仓库</strong>:Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。同样的，Docker 仓库也有公有和私有的概念。公有的 Docker 仓库名字是 Docker Hub。Docker Hub 提供了庞大的镜像集合供使用。这些镜像可以是自己创建，或者在别人的镜像基础上创建。Docker 仓库是 Docker 的分发部分。</p>\n</li>\n<li><p><strong>Docker 容器</strong>:Docker 容器和文件夹很类似，一个Docker容器包含了所有的某个应用运行所需要的环境。每一个 Docker 容器都是从 Docker 镜像创建的。Docker 容器可以运行、开始、停止、移动和删除。每一个 Docker 容器都是独立和安全的应用平台，Docker 容器是 Docker 的运行部分。</p>\n</li>\n</ol>\n<h2 id=\"2-Docker-8个的应用场景\"><a href=\"#2-Docker-8个的应用场景\" class=\"headerlink\" title=\"2. Docker 8个的应用场景\"></a>2. Docker 8个的应用场景</h2><blockquote>\n<p>本小节介绍了常用的8个Docker的真实使用场景，分别是简化配置、代码流水线管理、提高开发效率、隔离应用、整合服务器、调试能力、多租户环境、快速部署</p>\n</blockquote>\n<p><img src=\"/images/dockeruse.png\" alt=\"Docker 的8个应用场景\"></p>\n<p>一些Docker的使用场景，它为你展示了如何借助Docker的优势，在低开销的情况下，打造一个一致性的环境。</p>\n<h3 id=\"1-简化配置\"><a href=\"#1-简化配置\" class=\"headerlink\" title=\"1.简化配置\"></a>1.简化配置</h3><p>这是Docker公司宣传的Docker的主要使用场景。虚拟机的最大好处是能在你的硬件设施上运行各种配置不一样的平台（软件、系统），Docker在降低额外开销的情况下提供了同样的功能。它能让你将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度。</p>\n<h3 id=\"2-代码流水线（Code-Pipeline）管理\"><a href=\"#2-代码流水线（Code-Pipeline）管理\" class=\"headerlink\" title=\"2. 代码流水线（Code Pipeline）管理\"></a>2. 代码流水线（Code Pipeline）管理</h3><p>前一个场景对于管理代码的流水线起到了很大的帮助。代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。</p>\n<h3 id=\"3-提高开发效率\"><a href=\"#3-提高开发效率\" class=\"headerlink\" title=\"3. 提高开发效率\"></a>3. 提高开发效率</h3><p>不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。</p>\n<p>理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，之前使用虚拟的时候，我们经常需要为开发环境的机器加内存，而现在Docker可以轻易的让几十个服务在Docker中跑起来。</p>\n<h3 id=\"4-隔离应用\"><a href=\"#4-隔离应用\" class=\"headerlink\" title=\"4. 隔离应用\"></a>4. 隔离应用</h3><p>有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等。</p>\n<p>我们经常需要考虑两点，一是因为要降低成本而进行服务器整合，二是将一个整体式的应用拆分成松耦合的单个服务（译者注：微服务架构）。</p>\n<h3 id=\"5-整合服务器\"><a href=\"#5-整合服务器\" class=\"headerlink\" title=\"5. 整合服务器\"></a>5. 整合服务器</h3><p>正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。</p>\n<h3 id=\"6-调试能力\"><a href=\"#6-调试能力\" class=\"headerlink\" title=\"6. 调试能力\"></a>6. 调试能力</h3><p>Docker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。</p>\n<h3 id=\"7-多租户环境\"><a href=\"#7-多租户环境\" class=\"headerlink\" title=\"7. 多租户环境\"></a>7. 多租户环境</h3><p>另外一个Docker有意思的使用场景是在多租户的应用中，它可以避免关键应用的重写。我们一个特别的关于这个场景的例子是为IoT（物联网）的应用开发一个快速、易用的多租户环境。这种多租户的基本代码非常复杂，很难处理，重新规划这样一个应用不但消耗时间，也浪费金钱。</p>\n<p>使用Docker，可以为每一个租户的应用层的多个实例创建隔离的环境，这不仅简单而且成本低廉，当然这一切得益于Docker环境的启动速度和其高效的diff命令。</p>\n<h3 id=\"8-快速部署\"><a href=\"#8-快速部署\" class=\"headerlink\" title=\"8. 快速部署\"></a>8. 快速部署</h3><p>在虚拟机之前，引入新的硬件资源需要消耗几天的时间。虚拟化技术（Virtualization）将这个时间缩短到了分钟级别。而Docker通过为进程仅仅创建一个容器而无需启动一个操作系统，再次将这个过程缩短到了秒级。这正是Google和Facebook都看重的特性。</p>\n<p>你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。</p>\n<h4 id=\"本文的几个概念\"><a href=\"#本文的几个概念\" class=\"headerlink\" title=\"本文的几个概念\"></a>本文的几个概念</h4><ul>\n<li><span id=\"lxc\"><strong>LXC</strong></span>: LXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。属于操作系统层次之上的虚拟化</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p><a href=\"https://www.zhihu.com/question/22969309\" target=\"_blank\" rel=\"noopener\">重要链接！！！知乎上的深入浅出</a></p>\n</blockquote>\n<h2 id=\"1-Docker-介绍\"><a href=\"#1-Docker-介绍\" class=\"headerlink\" title=\"1.Docker 介绍\"></a>1.Docker 介绍</h2><p><strong>便于入题，首先用 Docker 的logo解释下：</strong><br><img src=\"/images/what.jpg\" alt=\"Docker logo\"></p>\n<p>那个大鲸鱼（或者是货轮）就是操作系统</p>\n<p>把要交付的应用程序看成是各种货物，原本要将各种各样形状、尺寸不同的货物放到大鲸鱼上，你得为每件货物考虑怎么安放（就是应用程序配套的环境），还得考虑货物和货物是否能叠起来（应用程序依赖的环境是否会冲突）。</p>\n<p>现在使用了集装箱（容器）把每件货物都放到集装箱里，这样大鲸鱼可以用同样地方式安放、堆叠集装了，省事省力。<a href=\"https://www.zhihu.com/question/28300645/answer/50922662\" target=\"_blank\" rel=\"noopener\">参考知乎</a></p>\n<p><strong>步入正题：</strong></p>\n<p>Docker 是 PaaS 提供商 dotCloud 开源的一个基于 <a href=\"#lxc\">LXC</a> 的高级容器引擎，源代码托管在 Github 上, 基于go语言并遵从Apache2.0协议开源。</p>\n<p>Docker可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。</p>\n<p><img src=\"/images/docker_cs.png\" alt=\"Docker的c/s架构\"></p>\n<p>如图所示，Docker 使用客户端-服务器 (C/S) 架构模式。</p>\n<ul>\n<li>Docker 客户端会与 Docker 守护进程进行通信。</li>\n<li>Docker 守护进程会处理复杂繁重的任务，例如建立、运行、发布你的 Docker 容器。</li>\n<li>Docker 客户端和守护进程 Daemon 可以运行在同一个系统上，当然你也可以使用 Docker 客户端去连接一个远程的 Docker 守护进程。Docker 客户端和守护进程之间通过 socket 或者 RESTful API 进行通信，就像下图。</li>\n</ul>\n<p><img src=\"/images/docker_frame.jpg\" alt=\"docker架构图\"></p>\n<h3 id=\"1-1-Docker-守护进程\"><a href=\"#1-1-Docker-守护进程\" class=\"headerlink\" title=\"1.1 Docker 守护进程\"></a>1.1 Docker 守护进程</h3><p>如上图所示，Docker 守护进程运行在一台主机上。用户并不直接和守护进程进行交互，而是通过 Docker 客户端间接和其通信。</p>\n<h3 id=\"1-2-Docker-客户端\"><a href=\"#1-2-Docker-客户端\" class=\"headerlink\" title=\"1.2 Docker 客户端\"></a>1.2 Docker 客户端</h3><p>Docker 客户端，实际上是 docker 的二进制程序，是主要的用户与 Docker 交互方式。它接收用户指令并且与背后的 Docker 守护进程通信，如此来回往复。</p>\n<h3 id=\"1-3-Docker-内部\"><a href=\"#1-3-Docker-内部\" class=\"headerlink\" title=\"1.3 Docker 内部\"></a>1.3 Docker 内部</h3><p>要理解 Docker 内部构建，需要理解以下三种部件：</p>\n<p><strong><em>Docker 镜像</em></strong> - Docker images<br><em><strong>Docker 仓库</strong></em> - Docker registeries<br><strong><em>Docker 容器</em></strong> - Docker containers</p>\n<ol>\n<li><p><strong>Docker 镜像</strong>:Docker 镜像是 Docker 容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成。Docker 使用 UnionFS 来将这些层联合到单独的镜像中。UnionFS 允许独立文件系统中的文件和文件夹(称之为分支)被透明覆盖，形成一个单独连贯的文件系统。正因为有了这些层的存在，Docker 是如此的轻量。当你改变了一个 Docker 镜像，比如升级到某个程序到新的版本，一个新的层会被创建。因此，不用替换整个原先的镜像或者重新建立(在使用虚拟机的时候你可能会这么做)，只是一个新 的层被添加或升级了。现在你不用重新发布整个镜像，只需要升级，层使得分发 Docker 镜像变得简单和快速。</p>\n</li>\n<li><p><strong>Docker 仓库</strong>:Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。同样的，Docker 仓库也有公有和私有的概念。公有的 Docker 仓库名字是 Docker Hub。Docker Hub 提供了庞大的镜像集合供使用。这些镜像可以是自己创建，或者在别人的镜像基础上创建。Docker 仓库是 Docker 的分发部分。</p>\n</li>\n<li><p><strong>Docker 容器</strong>:Docker 容器和文件夹很类似，一个Docker容器包含了所有的某个应用运行所需要的环境。每一个 Docker 容器都是从 Docker 镜像创建的。Docker 容器可以运行、开始、停止、移动和删除。每一个 Docker 容器都是独立和安全的应用平台，Docker 容器是 Docker 的运行部分。</p>\n</li>\n</ol>\n<h2 id=\"2-Docker-8个的应用场景\"><a href=\"#2-Docker-8个的应用场景\" class=\"headerlink\" title=\"2. Docker 8个的应用场景\"></a>2. Docker 8个的应用场景</h2><blockquote>\n<p>本小节介绍了常用的8个Docker的真实使用场景，分别是简化配置、代码流水线管理、提高开发效率、隔离应用、整合服务器、调试能力、多租户环境、快速部署</p>\n</blockquote>\n<p><img src=\"/images/dockeruse.png\" alt=\"Docker 的8个应用场景\"></p>\n<p>一些Docker的使用场景，它为你展示了如何借助Docker的优势，在低开销的情况下，打造一个一致性的环境。</p>\n<h3 id=\"1-简化配置\"><a href=\"#1-简化配置\" class=\"headerlink\" title=\"1.简化配置\"></a>1.简化配置</h3><p>这是Docker公司宣传的Docker的主要使用场景。虚拟机的最大好处是能在你的硬件设施上运行各种配置不一样的平台（软件、系统），Docker在降低额外开销的情况下提供了同样的功能。它能让你将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度。</p>\n<h3 id=\"2-代码流水线（Code-Pipeline）管理\"><a href=\"#2-代码流水线（Code-Pipeline）管理\" class=\"headerlink\" title=\"2. 代码流水线（Code Pipeline）管理\"></a>2. 代码流水线（Code Pipeline）管理</h3><p>前一个场景对于管理代码的流水线起到了很大的帮助。代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。</p>\n<h3 id=\"3-提高开发效率\"><a href=\"#3-提高开发效率\" class=\"headerlink\" title=\"3. 提高开发效率\"></a>3. 提高开发效率</h3><p>不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。</p>\n<p>理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，之前使用虚拟的时候，我们经常需要为开发环境的机器加内存，而现在Docker可以轻易的让几十个服务在Docker中跑起来。</p>\n<h3 id=\"4-隔离应用\"><a href=\"#4-隔离应用\" class=\"headerlink\" title=\"4. 隔离应用\"></a>4. 隔离应用</h3><p>有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等。</p>\n<p>我们经常需要考虑两点，一是因为要降低成本而进行服务器整合，二是将一个整体式的应用拆分成松耦合的单个服务（译者注：微服务架构）。</p>\n<h3 id=\"5-整合服务器\"><a href=\"#5-整合服务器\" class=\"headerlink\" title=\"5. 整合服务器\"></a>5. 整合服务器</h3><p>正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。</p>\n<h3 id=\"6-调试能力\"><a href=\"#6-调试能力\" class=\"headerlink\" title=\"6. 调试能力\"></a>6. 调试能力</h3><p>Docker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。</p>\n<h3 id=\"7-多租户环境\"><a href=\"#7-多租户环境\" class=\"headerlink\" title=\"7. 多租户环境\"></a>7. 多租户环境</h3><p>另外一个Docker有意思的使用场景是在多租户的应用中，它可以避免关键应用的重写。我们一个特别的关于这个场景的例子是为IoT（物联网）的应用开发一个快速、易用的多租户环境。这种多租户的基本代码非常复杂，很难处理，重新规划这样一个应用不但消耗时间，也浪费金钱。</p>\n<p>使用Docker，可以为每一个租户的应用层的多个实例创建隔离的环境，这不仅简单而且成本低廉，当然这一切得益于Docker环境的启动速度和其高效的diff命令。</p>\n<h3 id=\"8-快速部署\"><a href=\"#8-快速部署\" class=\"headerlink\" title=\"8. 快速部署\"></a>8. 快速部署</h3><p>在虚拟机之前，引入新的硬件资源需要消耗几天的时间。虚拟化技术（Virtualization）将这个时间缩短到了分钟级别。而Docker通过为进程仅仅创建一个容器而无需启动一个操作系统，再次将这个过程缩短到了秒级。这正是Google和Facebook都看重的特性。</p>\n<p>你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。</p>\n<h4 id=\"本文的几个概念\"><a href=\"#本文的几个概念\" class=\"headerlink\" title=\"本文的几个概念\"></a>本文的几个概念</h4><ul>\n<li><span id=\"lxc\"><strong>LXC</strong></span>: LXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。属于操作系统层次之上的虚拟化</li>\n</ul>\n"},{"layout":"post","title":"了解思科 Tetration 平台","date":"2017-06-12T10:16:18.000Z","author":"zhaojizhuang","_content":"\n\n\n### 介绍\n\n> 思科推出了 Tetration Analytics平台，Tetration 平台主要是对大规模数据中心和云平台上的网络流量的实时采集、存储和分析。\n\nTetration 平台搭配基于Cloud Scale技术的硬件设备，流动在数据中心的任何一个数据包的元信息都可以被实时记录和存储下来。\n\nTetration 平台可以辅助用户在应用关系梳理、应用访问策略制定、模拟和实时验证、应用云端迁移访问策略制定、白名单安全模型等方面脱离传统手工和被动的工作方式。\n\n**Tetration Analytics平台主要由 数据采集部分、数据存储部分、和数据分析部分组成。**\n\n**数据采集部分**：包括安装在实体服务器或者虚拟机中的软件数据采集器、以太网交换机转发芯片的硬件数据采集逻辑和第三方数据接口组成。软件数据采集器通过 libpcap（一个网络数据包捕获函数库，linux抓包工具tcpdump就是基于此的）来对数据进行采集。\n\n**存储和分析部分**：由基于思科UCS计算平台的服务器集群组成。\n\n下图是思科Tetration Analytics平台架构\n\n![Tetration Analytics](http://zhuanti.cww.net.cn/UpLoadFile/2016/7/5/20167547744377.png)\n\n存储和分析部分是该平台的精髓所在，针对万亿个数据的无监督机器学习算法的采用，为网络访问行为基线设立、网络访问异常检测、应用访问关系的动态甄别、聚类动态划分等提供了方便的工具。平台为用户提供了网络数据完善的、全面的大数据来源。\n\nTetration Analytics平台提供了存储和分析的接口，用户可以根据数据进行相应的网络数据分析，提供的接口包括 开放式 API、REST、推送事件、用户应用\n\n### 平台特性\n\n思科 Tetration Analytics 能够分析应用行为，并准确地反映出应用之间的依赖关系。它采用机器学习技术构建动态分层策略模型，从而实现应用分段和自动策略实施\n\nTetration Analytics 可大幅简化零信任模式的实施。它可以针对数据中心内的任何对象实时提供可视性。它使用基于行为的应用洞察和机器学习技术来构建动态策略模型，实现自动策略实施。此外，它还通过 REST API 支持开放式访问，客户可以编写个性化应用。\n\n\n\n\n- **遥感勘测快上加快**\n\nTetration Analytics 使用无需监管的机器学习技术，以线速处理收集的遥感勘测数据。借助自然语言技术，搜索和浏览数百亿条数据流记录。只需不到一秒即可获得切实可行的见解。\n\n- **切实可行的应用见解**\n\n依据应用组件和行为分析算法获取实时数据。标识应用组及其通信模式和服务依赖性。获取自动化白名单策略建议，实现零信任安全性。\n\n- **应用分段**\n\n在本地数据中心以及公共云和私有云中实施一致的策略，实现应用分段。持续监控合规性偏差，可在几分钟内发现生产网络中的违规情况。\n\n- **开放式 API**\n\n利用全面精细的遥感勘测数据，轻松打造个性化的定制应用。生成个性化的定制通知和查询。监控应用层的延迟情况并获取通知。此平台使用 REST API。\n\n- **强大的可扩展性**\n\nTetration Analytics 从数据中心的每个数据包收集遥感勘测数据。它可以在几秒钟内分析数百万个事件并从数十亿条记录中提供切实可行的见解。它可以长期保留数据，而不会丢失细节。\n[ddd](../docs/171010-01%20_%20程序员如何用技术变现（上）.html)\n","source":"_posts/cisco-platform.md","raw":"---\nlayout: post\ntitle:  \"了解思科 Tetration 平台\"\ndate:   2017-06-12 18:16:18 +0800\ncategories: 数据中心\ntags:  [\" Analytics\"]\nauthor: zhaojizhuang\n---\n\n\n\n### 介绍\n\n> 思科推出了 Tetration Analytics平台，Tetration 平台主要是对大规模数据中心和云平台上的网络流量的实时采集、存储和分析。\n\nTetration 平台搭配基于Cloud Scale技术的硬件设备，流动在数据中心的任何一个数据包的元信息都可以被实时记录和存储下来。\n\nTetration 平台可以辅助用户在应用关系梳理、应用访问策略制定、模拟和实时验证、应用云端迁移访问策略制定、白名单安全模型等方面脱离传统手工和被动的工作方式。\n\n**Tetration Analytics平台主要由 数据采集部分、数据存储部分、和数据分析部分组成。**\n\n**数据采集部分**：包括安装在实体服务器或者虚拟机中的软件数据采集器、以太网交换机转发芯片的硬件数据采集逻辑和第三方数据接口组成。软件数据采集器通过 libpcap（一个网络数据包捕获函数库，linux抓包工具tcpdump就是基于此的）来对数据进行采集。\n\n**存储和分析部分**：由基于思科UCS计算平台的服务器集群组成。\n\n下图是思科Tetration Analytics平台架构\n\n![Tetration Analytics](http://zhuanti.cww.net.cn/UpLoadFile/2016/7/5/20167547744377.png)\n\n存储和分析部分是该平台的精髓所在，针对万亿个数据的无监督机器学习算法的采用，为网络访问行为基线设立、网络访问异常检测、应用访问关系的动态甄别、聚类动态划分等提供了方便的工具。平台为用户提供了网络数据完善的、全面的大数据来源。\n\nTetration Analytics平台提供了存储和分析的接口，用户可以根据数据进行相应的网络数据分析，提供的接口包括 开放式 API、REST、推送事件、用户应用\n\n### 平台特性\n\n思科 Tetration Analytics 能够分析应用行为，并准确地反映出应用之间的依赖关系。它采用机器学习技术构建动态分层策略模型，从而实现应用分段和自动策略实施\n\nTetration Analytics 可大幅简化零信任模式的实施。它可以针对数据中心内的任何对象实时提供可视性。它使用基于行为的应用洞察和机器学习技术来构建动态策略模型，实现自动策略实施。此外，它还通过 REST API 支持开放式访问，客户可以编写个性化应用。\n\n\n\n\n- **遥感勘测快上加快**\n\nTetration Analytics 使用无需监管的机器学习技术，以线速处理收集的遥感勘测数据。借助自然语言技术，搜索和浏览数百亿条数据流记录。只需不到一秒即可获得切实可行的见解。\n\n- **切实可行的应用见解**\n\n依据应用组件和行为分析算法获取实时数据。标识应用组及其通信模式和服务依赖性。获取自动化白名单策略建议，实现零信任安全性。\n\n- **应用分段**\n\n在本地数据中心以及公共云和私有云中实施一致的策略，实现应用分段。持续监控合规性偏差，可在几分钟内发现生产网络中的违规情况。\n\n- **开放式 API**\n\n利用全面精细的遥感勘测数据，轻松打造个性化的定制应用。生成个性化的定制通知和查询。监控应用层的延迟情况并获取通知。此平台使用 REST API。\n\n- **强大的可扩展性**\n\nTetration Analytics 从数据中心的每个数据包收集遥感勘测数据。它可以在几秒钟内分析数百万个事件并从数十亿条记录中提供切实可行的见解。它可以长期保留数据，而不会丢失细节。\n[ddd](../docs/171010-01%20_%20程序员如何用技术变现（上）.html)\n","slug":"cisco-platform","published":1,"updated":"2020-05-19T13:30:06.852Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l840002r5fltsc7y91y","content":"<h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><blockquote>\n<p>思科推出了 Tetration Analytics平台，Tetration 平台主要是对大规模数据中心和云平台上的网络流量的实时采集、存储和分析。</p>\n</blockquote>\n<p>Tetration 平台搭配基于Cloud Scale技术的硬件设备，流动在数据中心的任何一个数据包的元信息都可以被实时记录和存储下来。</p>\n<p>Tetration 平台可以辅助用户在应用关系梳理、应用访问策略制定、模拟和实时验证、应用云端迁移访问策略制定、白名单安全模型等方面脱离传统手工和被动的工作方式。</p>\n<p><strong>Tetration Analytics平台主要由 数据采集部分、数据存储部分、和数据分析部分组成。</strong></p>\n<p><strong>数据采集部分</strong>：包括安装在实体服务器或者虚拟机中的软件数据采集器、以太网交换机转发芯片的硬件数据采集逻辑和第三方数据接口组成。软件数据采集器通过 libpcap（一个网络数据包捕获函数库，linux抓包工具tcpdump就是基于此的）来对数据进行采集。</p>\n<p><strong>存储和分析部分</strong>：由基于思科UCS计算平台的服务器集群组成。</p>\n<p>下图是思科Tetration Analytics平台架构</p>\n<p><img src=\"http://zhuanti.cww.net.cn/UpLoadFile/2016/7/5/20167547744377.png\" alt=\"Tetration Analytics\"></p>\n<p>存储和分析部分是该平台的精髓所在，针对万亿个数据的无监督机器学习算法的采用，为网络访问行为基线设立、网络访问异常检测、应用访问关系的动态甄别、聚类动态划分等提供了方便的工具。平台为用户提供了网络数据完善的、全面的大数据来源。</p>\n<p>Tetration Analytics平台提供了存储和分析的接口，用户可以根据数据进行相应的网络数据分析，提供的接口包括 开放式 API、REST、推送事件、用户应用</p>\n<h3 id=\"平台特性\"><a href=\"#平台特性\" class=\"headerlink\" title=\"平台特性\"></a>平台特性</h3><p>思科 Tetration Analytics 能够分析应用行为，并准确地反映出应用之间的依赖关系。它采用机器学习技术构建动态分层策略模型，从而实现应用分段和自动策略实施</p>\n<p>Tetration Analytics 可大幅简化零信任模式的实施。它可以针对数据中心内的任何对象实时提供可视性。它使用基于行为的应用洞察和机器学习技术来构建动态策略模型，实现自动策略实施。此外，它还通过 REST API 支持开放式访问，客户可以编写个性化应用。</p>\n<ul>\n<li><strong>遥感勘测快上加快</strong></li>\n</ul>\n<p>Tetration Analytics 使用无需监管的机器学习技术，以线速处理收集的遥感勘测数据。借助自然语言技术，搜索和浏览数百亿条数据流记录。只需不到一秒即可获得切实可行的见解。</p>\n<ul>\n<li><strong>切实可行的应用见解</strong></li>\n</ul>\n<p>依据应用组件和行为分析算法获取实时数据。标识应用组及其通信模式和服务依赖性。获取自动化白名单策略建议，实现零信任安全性。</p>\n<ul>\n<li><strong>应用分段</strong></li>\n</ul>\n<p>在本地数据中心以及公共云和私有云中实施一致的策略，实现应用分段。持续监控合规性偏差，可在几分钟内发现生产网络中的违规情况。</p>\n<ul>\n<li><strong>开放式 API</strong></li>\n</ul>\n<p>利用全面精细的遥感勘测数据，轻松打造个性化的定制应用。生成个性化的定制通知和查询。监控应用层的延迟情况并获取通知。此平台使用 REST API。</p>\n<ul>\n<li><strong>强大的可扩展性</strong></li>\n</ul>\n<p>Tetration Analytics 从数据中心的每个数据包收集遥感勘测数据。它可以在几秒钟内分析数百万个事件并从数十亿条记录中提供切实可行的见解。它可以长期保留数据，而不会丢失细节。<br><a href=\"../docs/171010-01%20_%20程序员如何用技术变现（上）.html\">ddd</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><blockquote>\n<p>思科推出了 Tetration Analytics平台，Tetration 平台主要是对大规模数据中心和云平台上的网络流量的实时采集、存储和分析。</p>\n</blockquote>\n<p>Tetration 平台搭配基于Cloud Scale技术的硬件设备，流动在数据中心的任何一个数据包的元信息都可以被实时记录和存储下来。</p>\n<p>Tetration 平台可以辅助用户在应用关系梳理、应用访问策略制定、模拟和实时验证、应用云端迁移访问策略制定、白名单安全模型等方面脱离传统手工和被动的工作方式。</p>\n<p><strong>Tetration Analytics平台主要由 数据采集部分、数据存储部分、和数据分析部分组成。</strong></p>\n<p><strong>数据采集部分</strong>：包括安装在实体服务器或者虚拟机中的软件数据采集器、以太网交换机转发芯片的硬件数据采集逻辑和第三方数据接口组成。软件数据采集器通过 libpcap（一个网络数据包捕获函数库，linux抓包工具tcpdump就是基于此的）来对数据进行采集。</p>\n<p><strong>存储和分析部分</strong>：由基于思科UCS计算平台的服务器集群组成。</p>\n<p>下图是思科Tetration Analytics平台架构</p>\n<p><img src=\"http://zhuanti.cww.net.cn/UpLoadFile/2016/7/5/20167547744377.png\" alt=\"Tetration Analytics\"></p>\n<p>存储和分析部分是该平台的精髓所在，针对万亿个数据的无监督机器学习算法的采用，为网络访问行为基线设立、网络访问异常检测、应用访问关系的动态甄别、聚类动态划分等提供了方便的工具。平台为用户提供了网络数据完善的、全面的大数据来源。</p>\n<p>Tetration Analytics平台提供了存储和分析的接口，用户可以根据数据进行相应的网络数据分析，提供的接口包括 开放式 API、REST、推送事件、用户应用</p>\n<h3 id=\"平台特性\"><a href=\"#平台特性\" class=\"headerlink\" title=\"平台特性\"></a>平台特性</h3><p>思科 Tetration Analytics 能够分析应用行为，并准确地反映出应用之间的依赖关系。它采用机器学习技术构建动态分层策略模型，从而实现应用分段和自动策略实施</p>\n<p>Tetration Analytics 可大幅简化零信任模式的实施。它可以针对数据中心内的任何对象实时提供可视性。它使用基于行为的应用洞察和机器学习技术来构建动态策略模型，实现自动策略实施。此外，它还通过 REST API 支持开放式访问，客户可以编写个性化应用。</p>\n<ul>\n<li><strong>遥感勘测快上加快</strong></li>\n</ul>\n<p>Tetration Analytics 使用无需监管的机器学习技术，以线速处理收集的遥感勘测数据。借助自然语言技术，搜索和浏览数百亿条数据流记录。只需不到一秒即可获得切实可行的见解。</p>\n<ul>\n<li><strong>切实可行的应用见解</strong></li>\n</ul>\n<p>依据应用组件和行为分析算法获取实时数据。标识应用组及其通信模式和服务依赖性。获取自动化白名单策略建议，实现零信任安全性。</p>\n<ul>\n<li><strong>应用分段</strong></li>\n</ul>\n<p>在本地数据中心以及公共云和私有云中实施一致的策略，实现应用分段。持续监控合规性偏差，可在几分钟内发现生产网络中的违规情况。</p>\n<ul>\n<li><strong>开放式 API</strong></li>\n</ul>\n<p>利用全面精细的遥感勘测数据，轻松打造个性化的定制应用。生成个性化的定制通知和查询。监控应用层的延迟情况并获取通知。此平台使用 REST API。</p>\n<ul>\n<li><strong>强大的可扩展性</strong></li>\n</ul>\n<p>Tetration Analytics 从数据中心的每个数据包收集遥感勘测数据。它可以在几秒钟内分析数百万个事件并从数十亿条记录中提供切实可行的见解。它可以长期保留数据，而不会丢失细节。<br><a href=\"../docs/171010-01%20_%20程序员如何用技术变现（上）.html\">ddd</a></p>\n"},{"layout":"post","title":"自行配置你的HPA扩缩容速率","date":"2020-05-19T10:16:18.000Z","author":"zhaojizhuang","_content":"\n\n## HPA介绍\n\nHPA, Pod 水平自动伸缩（Horizontal Pod Autoscaler）特性， 可以基于CPU利用率自动伸缩 replication controller、deployment和 replica set 中的 pod 数量，（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标custom metrics。 pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets。\n\nPod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。\n\n> [官网文档](https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#pod-%e6%b0%b4%e5%b9%b3%e8%87%aa%e5%8a%a8%e4%bc%b8%e7%bc%a9%e5%b7%a5%e4%bd%9c%e6%9c%ba%e5%88%b6) \n\n**HPA 工作机制**\n\n关于 HPA 的原理可以看下 baxiaoshi的云原生学习笔记 [https://www.yuque.com/baxiaoshi/tyado3/yw9deb](https://www.yuque.com/baxiaoshi/tyado3/yw9deb),本文不做过多介绍，只介绍自行配置hpa特性\n\n`HPA` 是由 `hpacontroller` 来实现的, 通过  `--horizontal-pod-autoscaler-sync-period` 参数 指定周期（默认值为15秒）\n\n\n一个HPA的例子如下：\n\n```yaml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: php-apache\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: php-apache\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\nstatus:\n  observedGeneration: 1\n  lastScaleTime: <some-time>\n  currentReplicas: 1\n  desiredReplicas: 1\n  currentMetrics:\n  - type: Resource\n    resource:\n      name: cpu\n      current:\n        averageUtilization: 0\n        averageValue: 0\n\n```\n\n\n## 背景\n\n前面已经介绍了 `HPA` 的相关信息，相信大家都对 `HPA` 有了简单的了解, 当使用 `Horizontal Pod Autoscaler` 管理一组副本缩放时， **有可能因为指标动态的变化造成副本数量频繁的变化，有时这被称为 抖动**。为了避免这种情况，社区引入了 延迟时间的设置，*该配置是针对整个集群的配置，不能对应用粒度进行配置*\n\n`--horizontal-pod-autoscaler-downscale-stabilization: ` 这个 `kube-controller-manager` 的参数表示缩容冷却时间。 即自从上次缩容执行结束后，多久可以再次执行缩容，默认时间是5分钟(5m0s)。`\n\n注意：这个配置是集群级别的，只能配置整个集群的扩缩容速率，用户不能自行配置自己的应用扩缩容速率：\n\n考虑一下场景的应用：\n\n- 对于大流量的的web应用。需要非常快的扩容速率，缓慢的缩容速率（为了迎接下一个流量高峰）\n- 对于处理数据类的应用。需要尽快的扩容（减少处理数据的时间），并尽快缩容（降低成本）\n- 对于处理常规流量和数据的应用，按照常规的方式配置就可以了\n\n对于上述3种应用场景，1.17以前的集群是不能支持，这也导致了一些未解决的 `issue`：\n- [https://github.com/kubernetes/kubernetes/issues/39090](https://github.com/kubernetes/kubernetes/issues/39090)\n- [https://github.com/kubernetes/kubernetes/issues/65097](https://github.com/kubernetes/kubernetes/issues/65097)\n- [https://github.com/kubernetes/kubernetes/issues/69428](https://github.com/kubernetes/kubernetes/issues/69428)\n\n为此社区 在`1.18`引入了 能自行配置`HPA`速率的新特性 见 相关 [keps](https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/20190307-configurable-scale-velocity-for-hpa.md)\n\n## 原理介绍\n\n为了自定义扩缩容行为，需要给单个 `hpa` 对象添加一个 `behavior`字段，如下：\n\n```yaml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: php-apache\nspec:\n  ***\n  behavior:\n      scaleUp:\n        policies:\n        - type: percent\n          value: 900%\n      scaleDown:\n        policies:\n        - type: pods\n          value: 1\n          periodSeconds: 600 # (i.e., scale down one pod every 10 min)\n  metrics:\n    ***\n```\n\n其中 `behavior` 字段 包含 如下对象：\n\n- `scaleUp` 配置扩容时的规则。\n    - `stabilizationWindowSeconds`： -该值表示 `HPA` 控制器应参考的采样时间，以防止副本数量波动。\n    - `selectPolicy`：可以是`min`或`max`,用于选择 `policies` 中的最大值还是最小值。默认为 `max`。\n    - `policies`： 是一个数组，每个元素有如下字段：\n        - `type`： 可为`pods`或者`percent`（以Pod的绝对数量或当前副本的百分比表示）。\n        - `periodSeconds` 规则应用的时间范围（以秒为单位），即每多少秒扩缩容一次。\n        - `value`: 规则的值，设为0 表示禁止 扩容或者缩容\n- `scaleDown` 与 `scaleUp` 类似，指定的是缩容的规则。\n\n用户通过控制 HPA中指定参数，来控制HPA扩缩容的逻辑\n\n`selectPolicy` 字段指示应应用的策略。默认情况下为max，也就是：**将使用尽可能扩大副本的最大数量，而选择尽可能减少副本的最小数量。** 有点绕没关系，待会看下面的例子\n\n## 场景\n\n### 场景1：尽可能快的扩容\n\n> 这种模式适合流量增速比较快的场景\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleUp:\n    policies:\n    - type: percent\n      value: 900%\n```\n`900%` 表示可以扩容的数量为当前副本数量的9倍，也就是可以扩大到当前副本数量的10倍，其他值都是默认值\n\n如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下：\n\n`1 -> 10 -> 100 -> 1000`\n\nscaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的[扩缩容算法](https://v1-14.docs.kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82) \n\n### 场景2：尽可能快的扩容，然后慢慢缩容\n\n> 这种模式适合不想快速缩容的场景\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleUp:\n    policies:\n    - type: percent\n      value: 900%\n  scaleDown:\n    policies:\n    - type: pods\n      value: 1\n      periodSeconds: 600 # (i.e., 每隔10分钟缩容一个pod)\n```\n这种配置扩容场景同 场景1，缩容场景下却是不同的，这里配置的是每隔10分钟缩容一个pod\n\n假如说扩容之后有1000个pod，那么缩容过程中pod数量如下：\n\n`1000 -> 1000 -> 1000 -> … (7 more min) -> 999`\n\n### 场景3：慢慢扩容，按常规缩容\n\n> 这种模式适合不太激进的扩容\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleUp:\n    policies:\n    - type: pods\n      value: 1\n```\n\n如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下：\n\n`1 -> 2 -> 3 -> 4`\n\n同样，scaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的[扩缩容算法](https://v1-14.docs.kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82) \n\n### 场景4： 常规扩容，禁止缩容\n\n> 这种模式适用于 不允许应用缩容，或者你想单独控制应用的缩容的场景\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleDown:\n    policies:\n    - type: pods\n      value: 0\n```\n\n副本数量将按照常规方式扩容，不会发生缩容\n\n### 场景5：延迟缩容\n\n> 这种模式适用于 用户并不想立即缩容，而是想等待更大的负载时间到来，再计算缩容情况\n\n```yaml\nbehavior:\n  scaleDown:\n    stabilizationWindowSeconds: 600\n    policies:\n    - type: pods\n      value: 5\n```\n这种配置情况下 `hpa` 缩容策略行为如下：\n\n- 会采集最近 `600s` 的（默认300s）的缩容建议，类似于滑动窗口\n- 选择最大的那个\n- 按照不大于每秒5个pod的速率缩容\n\n假如 `CurReplicas = 10` , `HPA controller` 每 `1min` 处理一次:\n\n- 前 9 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议：\n    `recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9]`\n- 第 10 min，我们增加一个扩缩容建议，比如说是 `8`\n    `recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9，8]`\n    HPA 算法会取其中最大的一个 `10`，因此应用不会发生缩容，`repicas` 的值不变\n- 第 11 min，我们增加一个扩缩容建议，比如 `7`，维持 `600s` 的滑动窗口，因此需要把第一个 `10` 去掉，如下：\n    `recommendations = [9, 8, 9, 9, 8, 9, 8, 9, 8, 7]`\n    HPA 算法会取最大的一个 `9`, 应用副本数量变化： `10 -> 9`\n    \n### 场景6： 避免错误的扩容\n\n> 这种模式在数据处理流中比较常见，用户想要根据队列中的数据来扩容，当数据较多时快速扩容。当指标有抖动时并不想扩容\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleUp:\n    stabilizationWindowSeconds: 300\n    policies:\n    - type: pods\n      value: 20\n```\n\n这种配置情况下 `hpa` 扩容策略行为如下：\n\n- 会采集最近 `300s` 的（默认0s）的扩容建议，类似于滑动窗口\n- 选择最小的那个\n- 按照不大于每秒20个pod的速率扩容\n\n假如 `CurReplicas = 2` , `HPA controller` 每 `1min` 处理一次:\n\n- 前 5 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议：\n    `recommendations = [2, 3, 19, 10, 3]`\n- 第 6 min，我们增加一个扩缩容建议，比如说是 `4`\n    `recommendations = [2, 3, 19, 10, 3, 4]`\n    HPA 算法会取其中最小的一个 `2`，因此应用不会发生缩容，`repicas` 的值不变\n- 第 11 min，我们增加一个扩缩容建议，比如 `7`，维持 `300s` 的滑动窗口，因此需要把第一个 `2` 去掉，如下：\n    `recommendations = [3, 19, 10, 3, 4，7]`\n    HPA 算法会取最小的一个 `3`, 应用副本数量变化： `2 -> 3`\n\n## 算法原理\n\n算法的伪代码如下：\n\n```go\n//  HPA controller 中的for循环\nfor {\n    desiredReplicas = AnyAlgorithmInHPAController(...)\n    \n    // 扩容场景\n    if desiredReplicas > curReplicas {\n      replicas = []int{}\n      for _, policy := range behavior.ScaleUp.Policies {\n        if policy.type == \"pods\" {\n          replicas = append(replicas, CurReplicas + policy.Value)\n        } else if policy.type == \"percent\" {\n          replicas = append(replicas, CurReplicas * (1 + policy.Value/100))\n        }\n      }\n      if behavior.ScaleUp.selectPolicy == \"max\" {\n        scaleUpLimit = max(replicas)\n      } else {\n        scaleUpLimit = min(replicas)\n      }\n      // 这里的min可以理解为 尽量维持原状，即如果是扩容，尽量取最小的那个\n      limitedReplicas = min(max, desiredReplicas)\n    }\n    // 缩容场景\n    if desiredReplicas < curReplicas {\n      for _, policy := range behaviro.scaleDown.Policies {\n        replicas = []int{}\n        if policy.type == \"pods\" {\n          replicas = append(replicas, CurReplicas - policy.Value)\n        } else if policy.type == \"percent\" {\n          replicas = append(replicas, CurReplicas * (1 - policy.Value /100))\n        }\n        if behavior.ScaleDown.SelectPolicy == \"max\" {\n          scaleDownLimit = min(replicas)\n        } else {\n          scaleDownLimit = max(replicas)\n        }\n        // 这里的max可以理解为 尽量维持原状，即如果是缩容，尽量取最大的那个\n        limitedReplicas = max(min, desiredReplicas)\n      }\n    }\n    storeRecommend(limitedReplicas, scaleRecommendations)\n    // 选择合适的扩缩容副本\n    nextReplicas := applyRecommendationIfNeeded(scaleRecommendations)\n    // 扩缩容\n    setReplicas(nextReplicas)\n    sleep(ControllerSleepTime)\n  }\n```\n\n## 默认值\n\n为了平滑得扩速容，默认值是有必要的\n`behavior` 的默认值如下：\n\n- behavior.scaleDown.stabilizationWindowSeconds = 300, 缩容情况下默认等待 5 min中再缩容.\n- behavior.scaleUp.stabilizationWindowSeconds = 0, 扩容时立即扩容，不等待\n- behavior.scaleUp.policies 默认值如下：\n    - 百分比策略\n        - policy = `percent`\n        - periodSeconds = `60`, 扩容间隔为 1min\n        - value = `100` 每次最多扩容翻倍\n    - Pod个数策略\n        - policy = `pods`\n        - periodSeconds = `60`, 扩容间隔为 1min\n        - value = `4` 每次最多扩容4个\n- behavior.scaleDown.policies 默认值如下：\n    - 百分比策略\n        - policy = `percent`\n        - periodSeconds = `60` 缩容间隔为 1min\n        - value = `100` 一次缩容最多可以把所有的示例都干掉","source":"_posts/confighpa.md","raw":"---\nlayout: post\ntitle:  \"自行配置你的HPA扩缩容速率\"\ndate:   2020-05-19 18:16:18 +0800\ncategories: k8s\ntags:  [\"k8s\"]\nauthor: zhaojizhuang\n---\n\n\n## HPA介绍\n\nHPA, Pod 水平自动伸缩（Horizontal Pod Autoscaler）特性， 可以基于CPU利用率自动伸缩 replication controller、deployment和 replica set 中的 pod 数量，（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标custom metrics。 pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets。\n\nPod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。\n\n> [官网文档](https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#pod-%e6%b0%b4%e5%b9%b3%e8%87%aa%e5%8a%a8%e4%bc%b8%e7%bc%a9%e5%b7%a5%e4%bd%9c%e6%9c%ba%e5%88%b6) \n\n**HPA 工作机制**\n\n关于 HPA 的原理可以看下 baxiaoshi的云原生学习笔记 [https://www.yuque.com/baxiaoshi/tyado3/yw9deb](https://www.yuque.com/baxiaoshi/tyado3/yw9deb),本文不做过多介绍，只介绍自行配置hpa特性\n\n`HPA` 是由 `hpacontroller` 来实现的, 通过  `--horizontal-pod-autoscaler-sync-period` 参数 指定周期（默认值为15秒）\n\n\n一个HPA的例子如下：\n\n```yaml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: php-apache\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: php-apache\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\nstatus:\n  observedGeneration: 1\n  lastScaleTime: <some-time>\n  currentReplicas: 1\n  desiredReplicas: 1\n  currentMetrics:\n  - type: Resource\n    resource:\n      name: cpu\n      current:\n        averageUtilization: 0\n        averageValue: 0\n\n```\n\n\n## 背景\n\n前面已经介绍了 `HPA` 的相关信息，相信大家都对 `HPA` 有了简单的了解, 当使用 `Horizontal Pod Autoscaler` 管理一组副本缩放时， **有可能因为指标动态的变化造成副本数量频繁的变化，有时这被称为 抖动**。为了避免这种情况，社区引入了 延迟时间的设置，*该配置是针对整个集群的配置，不能对应用粒度进行配置*\n\n`--horizontal-pod-autoscaler-downscale-stabilization: ` 这个 `kube-controller-manager` 的参数表示缩容冷却时间。 即自从上次缩容执行结束后，多久可以再次执行缩容，默认时间是5分钟(5m0s)。`\n\n注意：这个配置是集群级别的，只能配置整个集群的扩缩容速率，用户不能自行配置自己的应用扩缩容速率：\n\n考虑一下场景的应用：\n\n- 对于大流量的的web应用。需要非常快的扩容速率，缓慢的缩容速率（为了迎接下一个流量高峰）\n- 对于处理数据类的应用。需要尽快的扩容（减少处理数据的时间），并尽快缩容（降低成本）\n- 对于处理常规流量和数据的应用，按照常规的方式配置就可以了\n\n对于上述3种应用场景，1.17以前的集群是不能支持，这也导致了一些未解决的 `issue`：\n- [https://github.com/kubernetes/kubernetes/issues/39090](https://github.com/kubernetes/kubernetes/issues/39090)\n- [https://github.com/kubernetes/kubernetes/issues/65097](https://github.com/kubernetes/kubernetes/issues/65097)\n- [https://github.com/kubernetes/kubernetes/issues/69428](https://github.com/kubernetes/kubernetes/issues/69428)\n\n为此社区 在`1.18`引入了 能自行配置`HPA`速率的新特性 见 相关 [keps](https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/20190307-configurable-scale-velocity-for-hpa.md)\n\n## 原理介绍\n\n为了自定义扩缩容行为，需要给单个 `hpa` 对象添加一个 `behavior`字段，如下：\n\n```yaml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: php-apache\nspec:\n  ***\n  behavior:\n      scaleUp:\n        policies:\n        - type: percent\n          value: 900%\n      scaleDown:\n        policies:\n        - type: pods\n          value: 1\n          periodSeconds: 600 # (i.e., scale down one pod every 10 min)\n  metrics:\n    ***\n```\n\n其中 `behavior` 字段 包含 如下对象：\n\n- `scaleUp` 配置扩容时的规则。\n    - `stabilizationWindowSeconds`： -该值表示 `HPA` 控制器应参考的采样时间，以防止副本数量波动。\n    - `selectPolicy`：可以是`min`或`max`,用于选择 `policies` 中的最大值还是最小值。默认为 `max`。\n    - `policies`： 是一个数组，每个元素有如下字段：\n        - `type`： 可为`pods`或者`percent`（以Pod的绝对数量或当前副本的百分比表示）。\n        - `periodSeconds` 规则应用的时间范围（以秒为单位），即每多少秒扩缩容一次。\n        - `value`: 规则的值，设为0 表示禁止 扩容或者缩容\n- `scaleDown` 与 `scaleUp` 类似，指定的是缩容的规则。\n\n用户通过控制 HPA中指定参数，来控制HPA扩缩容的逻辑\n\n`selectPolicy` 字段指示应应用的策略。默认情况下为max，也就是：**将使用尽可能扩大副本的最大数量，而选择尽可能减少副本的最小数量。** 有点绕没关系，待会看下面的例子\n\n## 场景\n\n### 场景1：尽可能快的扩容\n\n> 这种模式适合流量增速比较快的场景\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleUp:\n    policies:\n    - type: percent\n      value: 900%\n```\n`900%` 表示可以扩容的数量为当前副本数量的9倍，也就是可以扩大到当前副本数量的10倍，其他值都是默认值\n\n如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下：\n\n`1 -> 10 -> 100 -> 1000`\n\nscaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的[扩缩容算法](https://v1-14.docs.kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82) \n\n### 场景2：尽可能快的扩容，然后慢慢缩容\n\n> 这种模式适合不想快速缩容的场景\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleUp:\n    policies:\n    - type: percent\n      value: 900%\n  scaleDown:\n    policies:\n    - type: pods\n      value: 1\n      periodSeconds: 600 # (i.e., 每隔10分钟缩容一个pod)\n```\n这种配置扩容场景同 场景1，缩容场景下却是不同的，这里配置的是每隔10分钟缩容一个pod\n\n假如说扩容之后有1000个pod，那么缩容过程中pod数量如下：\n\n`1000 -> 1000 -> 1000 -> … (7 more min) -> 999`\n\n### 场景3：慢慢扩容，按常规缩容\n\n> 这种模式适合不太激进的扩容\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleUp:\n    policies:\n    - type: pods\n      value: 1\n```\n\n如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下：\n\n`1 -> 2 -> 3 -> 4`\n\n同样，scaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的[扩缩容算法](https://v1-14.docs.kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82) \n\n### 场景4： 常规扩容，禁止缩容\n\n> 这种模式适用于 不允许应用缩容，或者你想单独控制应用的缩容的场景\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleDown:\n    policies:\n    - type: pods\n      value: 0\n```\n\n副本数量将按照常规方式扩容，不会发生缩容\n\n### 场景5：延迟缩容\n\n> 这种模式适用于 用户并不想立即缩容，而是想等待更大的负载时间到来，再计算缩容情况\n\n```yaml\nbehavior:\n  scaleDown:\n    stabilizationWindowSeconds: 600\n    policies:\n    - type: pods\n      value: 5\n```\n这种配置情况下 `hpa` 缩容策略行为如下：\n\n- 会采集最近 `600s` 的（默认300s）的缩容建议，类似于滑动窗口\n- 选择最大的那个\n- 按照不大于每秒5个pod的速率缩容\n\n假如 `CurReplicas = 10` , `HPA controller` 每 `1min` 处理一次:\n\n- 前 9 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议：\n    `recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9]`\n- 第 10 min，我们增加一个扩缩容建议，比如说是 `8`\n    `recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9，8]`\n    HPA 算法会取其中最大的一个 `10`，因此应用不会发生缩容，`repicas` 的值不变\n- 第 11 min，我们增加一个扩缩容建议，比如 `7`，维持 `600s` 的滑动窗口，因此需要把第一个 `10` 去掉，如下：\n    `recommendations = [9, 8, 9, 9, 8, 9, 8, 9, 8, 7]`\n    HPA 算法会取最大的一个 `9`, 应用副本数量变化： `10 -> 9`\n    \n### 场景6： 避免错误的扩容\n\n> 这种模式在数据处理流中比较常见，用户想要根据队列中的数据来扩容，当数据较多时快速扩容。当指标有抖动时并不想扩容\n\nHPA的配置如下：\n\n```yaml\nbehavior:\n  scaleUp:\n    stabilizationWindowSeconds: 300\n    policies:\n    - type: pods\n      value: 20\n```\n\n这种配置情况下 `hpa` 扩容策略行为如下：\n\n- 会采集最近 `300s` 的（默认0s）的扩容建议，类似于滑动窗口\n- 选择最小的那个\n- 按照不大于每秒20个pod的速率扩容\n\n假如 `CurReplicas = 2` , `HPA controller` 每 `1min` 处理一次:\n\n- 前 5 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议：\n    `recommendations = [2, 3, 19, 10, 3]`\n- 第 6 min，我们增加一个扩缩容建议，比如说是 `4`\n    `recommendations = [2, 3, 19, 10, 3, 4]`\n    HPA 算法会取其中最小的一个 `2`，因此应用不会发生缩容，`repicas` 的值不变\n- 第 11 min，我们增加一个扩缩容建议，比如 `7`，维持 `300s` 的滑动窗口，因此需要把第一个 `2` 去掉，如下：\n    `recommendations = [3, 19, 10, 3, 4，7]`\n    HPA 算法会取最小的一个 `3`, 应用副本数量变化： `2 -> 3`\n\n## 算法原理\n\n算法的伪代码如下：\n\n```go\n//  HPA controller 中的for循环\nfor {\n    desiredReplicas = AnyAlgorithmInHPAController(...)\n    \n    // 扩容场景\n    if desiredReplicas > curReplicas {\n      replicas = []int{}\n      for _, policy := range behavior.ScaleUp.Policies {\n        if policy.type == \"pods\" {\n          replicas = append(replicas, CurReplicas + policy.Value)\n        } else if policy.type == \"percent\" {\n          replicas = append(replicas, CurReplicas * (1 + policy.Value/100))\n        }\n      }\n      if behavior.ScaleUp.selectPolicy == \"max\" {\n        scaleUpLimit = max(replicas)\n      } else {\n        scaleUpLimit = min(replicas)\n      }\n      // 这里的min可以理解为 尽量维持原状，即如果是扩容，尽量取最小的那个\n      limitedReplicas = min(max, desiredReplicas)\n    }\n    // 缩容场景\n    if desiredReplicas < curReplicas {\n      for _, policy := range behaviro.scaleDown.Policies {\n        replicas = []int{}\n        if policy.type == \"pods\" {\n          replicas = append(replicas, CurReplicas - policy.Value)\n        } else if policy.type == \"percent\" {\n          replicas = append(replicas, CurReplicas * (1 - policy.Value /100))\n        }\n        if behavior.ScaleDown.SelectPolicy == \"max\" {\n          scaleDownLimit = min(replicas)\n        } else {\n          scaleDownLimit = max(replicas)\n        }\n        // 这里的max可以理解为 尽量维持原状，即如果是缩容，尽量取最大的那个\n        limitedReplicas = max(min, desiredReplicas)\n      }\n    }\n    storeRecommend(limitedReplicas, scaleRecommendations)\n    // 选择合适的扩缩容副本\n    nextReplicas := applyRecommendationIfNeeded(scaleRecommendations)\n    // 扩缩容\n    setReplicas(nextReplicas)\n    sleep(ControllerSleepTime)\n  }\n```\n\n## 默认值\n\n为了平滑得扩速容，默认值是有必要的\n`behavior` 的默认值如下：\n\n- behavior.scaleDown.stabilizationWindowSeconds = 300, 缩容情况下默认等待 5 min中再缩容.\n- behavior.scaleUp.stabilizationWindowSeconds = 0, 扩容时立即扩容，不等待\n- behavior.scaleUp.policies 默认值如下：\n    - 百分比策略\n        - policy = `percent`\n        - periodSeconds = `60`, 扩容间隔为 1min\n        - value = `100` 每次最多扩容翻倍\n    - Pod个数策略\n        - policy = `pods`\n        - periodSeconds = `60`, 扩容间隔为 1min\n        - value = `4` 每次最多扩容4个\n- behavior.scaleDown.policies 默认值如下：\n    - 百分比策略\n        - policy = `percent`\n        - periodSeconds = `60` 缩容间隔为 1min\n        - value = `100` 一次缩容最多可以把所有的示例都干掉","slug":"confighpa","published":1,"updated":"2020-05-19T15:17:40.517Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l870005r5flbwdf4jdm","content":"<h2 id=\"HPA介绍\"><a href=\"#HPA介绍\" class=\"headerlink\" title=\"HPA介绍\"></a>HPA介绍</h2><p>HPA, Pod 水平自动伸缩（Horizontal Pod Autoscaler）特性， 可以基于CPU利用率自动伸缩 replication controller、deployment和 replica set 中的 pod 数量，（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标custom metrics。 pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets。</p>\n<p>Pod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。</p>\n<blockquote>\n<p><a href=\"https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#pod-%e6%b0%b4%e5%b9%b3%e8%87%aa%e5%8a%a8%e4%bc%b8%e7%bc%a9%e5%b7%a5%e4%bd%9c%e6%9c%ba%e5%88%b6\" target=\"_blank\" rel=\"noopener\">官网文档</a> </p>\n</blockquote>\n<p><strong>HPA 工作机制</strong></p>\n<p>关于 HPA 的原理可以看下 baxiaoshi的云原生学习笔记 <a href=\"https://www.yuque.com/baxiaoshi/tyado3/yw9deb\" target=\"_blank\" rel=\"noopener\">https://www.yuque.com/baxiaoshi/tyado3/yw9deb</a>,本文不做过多介绍，只介绍自行配置hpa特性</p>\n<p><code>HPA</code> 是由 <code>hpacontroller</code> 来实现的, 通过  <code>--horizontal-pod-autoscaler-sync-period</code> 参数 指定周期（默认值为15秒）</p>\n<p>一个HPA的例子如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">autoscaling/v2beta2</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">HorizontalPodAutoscaler</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">php-apache</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleTargetRef:</span></span><br><span class=\"line\"><span class=\"attr\">    apiVersion:</span> <span class=\"string\">apps/v1</span></span><br><span class=\"line\"><span class=\"attr\">    kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">    name:</span> <span class=\"string\">php-apache</span></span><br><span class=\"line\"><span class=\"attr\">  minReplicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">  maxReplicas:</span> <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"attr\">  metrics:</span></span><br><span class=\"line\"><span class=\"attr\">  - type:</span> <span class=\"string\">Resource</span></span><br><span class=\"line\"><span class=\"attr\">    resource:</span></span><br><span class=\"line\"><span class=\"attr\">      name:</span> <span class=\"string\">cpu</span></span><br><span class=\"line\"><span class=\"attr\">      target:</span></span><br><span class=\"line\"><span class=\"attr\">        type:</span> <span class=\"string\">Utilization</span></span><br><span class=\"line\"><span class=\"attr\">        averageUtilization:</span> <span class=\"number\">50</span></span><br><span class=\"line\"><span class=\"attr\">status:</span></span><br><span class=\"line\"><span class=\"attr\">  observedGeneration:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">  lastScaleTime:</span> <span class=\"string\">&lt;some-time&gt;</span></span><br><span class=\"line\"><span class=\"attr\">  currentReplicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">  desiredReplicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">  currentMetrics:</span></span><br><span class=\"line\"><span class=\"attr\">  - type:</span> <span class=\"string\">Resource</span></span><br><span class=\"line\"><span class=\"attr\">    resource:</span></span><br><span class=\"line\"><span class=\"attr\">      name:</span> <span class=\"string\">cpu</span></span><br><span class=\"line\"><span class=\"attr\">      current:</span></span><br><span class=\"line\"><span class=\"attr\">        averageUtilization:</span> <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"attr\">        averageValue:</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>前面已经介绍了 <code>HPA</code> 的相关信息，相信大家都对 <code>HPA</code> 有了简单的了解, 当使用 <code>Horizontal Pod Autoscaler</code> 管理一组副本缩放时， <strong>有可能因为指标动态的变化造成副本数量频繁的变化，有时这被称为 抖动</strong>。为了避免这种情况，社区引入了 延迟时间的设置，<em>该配置是针对整个集群的配置，不能对应用粒度进行配置</em></p>\n<p><code>--horizontal-pod-autoscaler-downscale-stabilization:</code> 这个 <code>kube-controller-manager</code> 的参数表示缩容冷却时间。 即自从上次缩容执行结束后，多久可以再次执行缩容，默认时间是5分钟(5m0s)。`</p>\n<p>注意：这个配置是集群级别的，只能配置整个集群的扩缩容速率，用户不能自行配置自己的应用扩缩容速率：</p>\n<p>考虑一下场景的应用：</p>\n<ul>\n<li>对于大流量的的web应用。需要非常快的扩容速率，缓慢的缩容速率（为了迎接下一个流量高峰）</li>\n<li>对于处理数据类的应用。需要尽快的扩容（减少处理数据的时间），并尽快缩容（降低成本）</li>\n<li>对于处理常规流量和数据的应用，按照常规的方式配置就可以了</li>\n</ul>\n<p>对于上述3种应用场景，1.17以前的集群是不能支持，这也导致了一些未解决的 <code>issue</code>：</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/39090\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/issues/39090</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/65097\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/issues/65097</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/69428\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/issues/69428</a></li>\n</ul>\n<p>为此社区 在<code>1.18</code>引入了 能自行配置<code>HPA</code>速率的新特性 见 相关 <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/20190307-configurable-scale-velocity-for-hpa.md\" target=\"_blank\" rel=\"noopener\">keps</a></p>\n<h2 id=\"原理介绍\"><a href=\"#原理介绍\" class=\"headerlink\" title=\"原理介绍\"></a>原理介绍</h2><p>为了自定义扩缩容行为，需要给单个 <code>hpa</code> 对象添加一个 <code>behavior</code>字段，如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">autoscaling/v2beta2</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">HorizontalPodAutoscaler</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">php-apache</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"string\">***</span></span><br><span class=\"line\"><span class=\"attr\">  behavior:</span></span><br><span class=\"line\"><span class=\"attr\">      scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">        policies:</span></span><br><span class=\"line\"><span class=\"attr\">        - type:</span> <span class=\"string\">percent</span></span><br><span class=\"line\"><span class=\"attr\">          value:</span> <span class=\"number\">900</span><span class=\"string\">%</span></span><br><span class=\"line\"><span class=\"attr\">      scaleDown:</span></span><br><span class=\"line\"><span class=\"attr\">        policies:</span></span><br><span class=\"line\"><span class=\"attr\">        - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">          value:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">          periodSeconds:</span> <span class=\"number\">600</span> <span class=\"comment\"># (i.e., scale down one pod every 10 min)</span></span><br><span class=\"line\"><span class=\"attr\">  metrics:</span></span><br><span class=\"line\">    <span class=\"string\">***</span></span><br></pre></td></tr></table></figure>\n<p>其中 <code>behavior</code> 字段 包含 如下对象：</p>\n<ul>\n<li><code>scaleUp</code> 配置扩容时的规则。<ul>\n<li><code>stabilizationWindowSeconds</code>： -该值表示 <code>HPA</code> 控制器应参考的采样时间，以防止副本数量波动。</li>\n<li><code>selectPolicy</code>：可以是<code>min</code>或<code>max</code>,用于选择 <code>policies</code> 中的最大值还是最小值。默认为 <code>max</code>。</li>\n<li><code>policies</code>： 是一个数组，每个元素有如下字段：<ul>\n<li><code>type</code>： 可为<code>pods</code>或者<code>percent</code>（以Pod的绝对数量或当前副本的百分比表示）。</li>\n<li><code>periodSeconds</code> 规则应用的时间范围（以秒为单位），即每多少秒扩缩容一次。</li>\n<li><code>value</code>: 规则的值，设为0 表示禁止 扩容或者缩容</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><code>scaleDown</code> 与 <code>scaleUp</code> 类似，指定的是缩容的规则。</li>\n</ul>\n<p>用户通过控制 HPA中指定参数，来控制HPA扩缩容的逻辑</p>\n<p><code>selectPolicy</code> 字段指示应应用的策略。默认情况下为max，也就是：<strong>将使用尽可能扩大副本的最大数量，而选择尽可能减少副本的最小数量。</strong> 有点绕没关系，待会看下面的例子</p>\n<h2 id=\"场景\"><a href=\"#场景\" class=\"headerlink\" title=\"场景\"></a>场景</h2><h3 id=\"场景1：尽可能快的扩容\"><a href=\"#场景1：尽可能快的扩容\" class=\"headerlink\" title=\"场景1：尽可能快的扩容\"></a>场景1：尽可能快的扩容</h3><blockquote>\n<p>这种模式适合流量增速比较快的场景</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">percent</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">900</span><span class=\"string\">%</span></span><br></pre></td></tr></table></figure>\n<p><code>900%</code> 表示可以扩容的数量为当前副本数量的9倍，也就是可以扩大到当前副本数量的10倍，其他值都是默认值</p>\n<p>如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下：</p>\n<p><code>1 -&gt; 10 -&gt; 100 -&gt; 1000</code></p>\n<p>scaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的<a href=\"https://v1-14.docs.kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82\" target=\"_blank\" rel=\"noopener\">扩缩容算法</a> </p>\n<h3 id=\"场景2：尽可能快的扩容，然后慢慢缩容\"><a href=\"#场景2：尽可能快的扩容，然后慢慢缩容\" class=\"headerlink\" title=\"场景2：尽可能快的扩容，然后慢慢缩容\"></a>场景2：尽可能快的扩容，然后慢慢缩容</h3><blockquote>\n<p>这种模式适合不想快速缩容的场景</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">percent</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">900</span><span class=\"string\">%</span></span><br><span class=\"line\"><span class=\"attr\">  scaleDown:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">      periodSeconds:</span> <span class=\"number\">600</span> <span class=\"comment\"># (i.e., 每隔10分钟缩容一个pod)</span></span><br></pre></td></tr></table></figure>\n<p>这种配置扩容场景同 场景1，缩容场景下却是不同的，这里配置的是每隔10分钟缩容一个pod</p>\n<p>假如说扩容之后有1000个pod，那么缩容过程中pod数量如下：</p>\n<p><code>1000 -&gt; 1000 -&gt; 1000 -&gt; … (7 more min) -&gt; 999</code></p>\n<h3 id=\"场景3：慢慢扩容，按常规缩容\"><a href=\"#场景3：慢慢扩容，按常规缩容\" class=\"headerlink\" title=\"场景3：慢慢扩容，按常规缩容\"></a>场景3：慢慢扩容，按常规缩容</h3><blockquote>\n<p>这种模式适合不太激进的扩容</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下：</p>\n<p><code>1 -&gt; 2 -&gt; 3 -&gt; 4</code></p>\n<p>同样，scaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的<a href=\"https://v1-14.docs.kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82\" target=\"_blank\" rel=\"noopener\">扩缩容算法</a> </p>\n<h3 id=\"场景4：-常规扩容，禁止缩容\"><a href=\"#场景4：-常规扩容，禁止缩容\" class=\"headerlink\" title=\"场景4： 常规扩容，禁止缩容\"></a>场景4： 常规扩容，禁止缩容</h3><blockquote>\n<p>这种模式适用于 不允许应用缩容，或者你想单独控制应用的缩容的场景</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleDown:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<p>副本数量将按照常规方式扩容，不会发生缩容</p>\n<h3 id=\"场景5：延迟缩容\"><a href=\"#场景5：延迟缩容\" class=\"headerlink\" title=\"场景5：延迟缩容\"></a>场景5：延迟缩容</h3><blockquote>\n<p>这种模式适用于 用户并不想立即缩容，而是想等待更大的负载时间到来，再计算缩容情况</p>\n</blockquote>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleDown:</span></span><br><span class=\"line\"><span class=\"attr\">    stabilizationWindowSeconds:</span> <span class=\"number\">600</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">5</span></span><br></pre></td></tr></table></figure>\n<p>这种配置情况下 <code>hpa</code> 缩容策略行为如下：</p>\n<ul>\n<li>会采集最近 <code>600s</code> 的（默认300s）的缩容建议，类似于滑动窗口</li>\n<li>选择最大的那个</li>\n<li>按照不大于每秒5个pod的速率缩容</li>\n</ul>\n<p>假如 <code>CurReplicas = 10</code> , <code>HPA controller</code> 每 <code>1min</code> 处理一次:</p>\n<ul>\n<li>前 9 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议：<br>  <code>recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9]</code></li>\n<li>第 10 min，我们增加一个扩缩容建议，比如说是 <code>8</code><br>  <code>recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9，8]</code><br>  HPA 算法会取其中最大的一个 <code>10</code>，因此应用不会发生缩容，<code>repicas</code> 的值不变</li>\n<li>第 11 min，我们增加一个扩缩容建议，比如 <code>7</code>，维持 <code>600s</code> 的滑动窗口，因此需要把第一个 <code>10</code> 去掉，如下：<br>  <code>recommendations = [9, 8, 9, 9, 8, 9, 8, 9, 8, 7]</code><br>  HPA 算法会取最大的一个 <code>9</code>, 应用副本数量变化： <code>10 -&gt; 9</code></li>\n</ul>\n<h3 id=\"场景6：-避免错误的扩容\"><a href=\"#场景6：-避免错误的扩容\" class=\"headerlink\" title=\"场景6： 避免错误的扩容\"></a>场景6： 避免错误的扩容</h3><blockquote>\n<p>这种模式在数据处理流中比较常见，用户想要根据队列中的数据来扩容，当数据较多时快速扩容。当指标有抖动时并不想扩容</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">    stabilizationWindowSeconds:</span> <span class=\"number\">300</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">20</span></span><br></pre></td></tr></table></figure>\n<p>这种配置情况下 <code>hpa</code> 扩容策略行为如下：</p>\n<ul>\n<li>会采集最近 <code>300s</code> 的（默认0s）的扩容建议，类似于滑动窗口</li>\n<li>选择最小的那个</li>\n<li>按照不大于每秒20个pod的速率扩容</li>\n</ul>\n<p>假如 <code>CurReplicas = 2</code> , <code>HPA controller</code> 每 <code>1min</code> 处理一次:</p>\n<ul>\n<li>前 5 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议：<br>  <code>recommendations = [2, 3, 19, 10, 3]</code></li>\n<li>第 6 min，我们增加一个扩缩容建议，比如说是 <code>4</code><br>  <code>recommendations = [2, 3, 19, 10, 3, 4]</code><br>  HPA 算法会取其中最小的一个 <code>2</code>，因此应用不会发生缩容，<code>repicas</code> 的值不变</li>\n<li>第 11 min，我们增加一个扩缩容建议，比如 <code>7</code>，维持 <code>300s</code> 的滑动窗口，因此需要把第一个 <code>2</code> 去掉，如下：<br>  <code>recommendations = [3, 19, 10, 3, 4，7]</code><br>  HPA 算法会取最小的一个 <code>3</code>, 应用副本数量变化： <code>2 -&gt; 3</code></li>\n</ul>\n<h2 id=\"算法原理\"><a href=\"#算法原理\" class=\"headerlink\" title=\"算法原理\"></a>算法原理</h2><p>算法的伪代码如下：</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  HPA controller 中的for循环</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">    desiredReplicas = AnyAlgorithmInHPAController(...)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 扩容场景</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> desiredReplicas &gt; curReplicas &#123;</span><br><span class=\"line\">      replicas = []<span class=\"keyword\">int</span>&#123;&#125;</span><br><span class=\"line\">      <span class=\"keyword\">for</span> _, policy := <span class=\"keyword\">range</span> behavior.ScaleUp.Policies &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> policy.<span class=\"keyword\">type</span> == <span class=\"string\">\"pods\"</span> &#123;</span><br><span class=\"line\">          replicas = <span class=\"built_in\">append</span>(replicas, CurReplicas + policy.Value)</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> policy.<span class=\"keyword\">type</span> == <span class=\"string\">\"percent\"</span> &#123;</span><br><span class=\"line\">          replicas = <span class=\"built_in\">append</span>(replicas, CurReplicas * (<span class=\"number\">1</span> + policy.Value/<span class=\"number\">100</span>))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> behavior.ScaleUp.selectPolicy == <span class=\"string\">\"max\"</span> &#123;</span><br><span class=\"line\">        scaleUpLimit = max(replicas)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        scaleUpLimit = min(replicas)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"comment\">// 这里的min可以理解为 尽量维持原状，即如果是扩容，尽量取最小的那个</span></span><br><span class=\"line\">      limitedReplicas = min(max, desiredReplicas)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 缩容场景</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> desiredReplicas &lt; curReplicas &#123;</span><br><span class=\"line\">      <span class=\"keyword\">for</span> _, policy := <span class=\"keyword\">range</span> behaviro.scaleDown.Policies &#123;</span><br><span class=\"line\">        replicas = []<span class=\"keyword\">int</span>&#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> policy.<span class=\"keyword\">type</span> == <span class=\"string\">\"pods\"</span> &#123;</span><br><span class=\"line\">          replicas = <span class=\"built_in\">append</span>(replicas, CurReplicas - policy.Value)</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> policy.<span class=\"keyword\">type</span> == <span class=\"string\">\"percent\"</span> &#123;</span><br><span class=\"line\">          replicas = <span class=\"built_in\">append</span>(replicas, CurReplicas * (<span class=\"number\">1</span> - policy.Value /<span class=\"number\">100</span>))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> behavior.ScaleDown.SelectPolicy == <span class=\"string\">\"max\"</span> &#123;</span><br><span class=\"line\">          scaleDownLimit = min(replicas)</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">          scaleDownLimit = max(replicas)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 这里的max可以理解为 尽量维持原状，即如果是缩容，尽量取最大的那个</span></span><br><span class=\"line\">        limitedReplicas = max(min, desiredReplicas)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    storeRecommend(limitedReplicas, scaleRecommendations)</span><br><span class=\"line\">    <span class=\"comment\">// 选择合适的扩缩容副本</span></span><br><span class=\"line\">    nextReplicas := applyRecommendationIfNeeded(scaleRecommendations)</span><br><span class=\"line\">    <span class=\"comment\">// 扩缩容</span></span><br><span class=\"line\">    setReplicas(nextReplicas)</span><br><span class=\"line\">    sleep(ControllerSleepTime)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"默认值\"><a href=\"#默认值\" class=\"headerlink\" title=\"默认值\"></a>默认值</h2><p>为了平滑得扩速容，默认值是有必要的<br><code>behavior</code> 的默认值如下：</p>\n<ul>\n<li>behavior.scaleDown.stabilizationWindowSeconds = 300, 缩容情况下默认等待 5 min中再缩容.</li>\n<li>behavior.scaleUp.stabilizationWindowSeconds = 0, 扩容时立即扩容，不等待</li>\n<li>behavior.scaleUp.policies 默认值如下：<ul>\n<li>百分比策略<ul>\n<li>policy = <code>percent</code></li>\n<li>periodSeconds = <code>60</code>, 扩容间隔为 1min</li>\n<li>value = <code>100</code> 每次最多扩容翻倍</li>\n</ul>\n</li>\n<li>Pod个数策略<ul>\n<li>policy = <code>pods</code></li>\n<li>periodSeconds = <code>60</code>, 扩容间隔为 1min</li>\n<li>value = <code>4</code> 每次最多扩容4个</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>behavior.scaleDown.policies 默认值如下：<ul>\n<li>百分比策略<ul>\n<li>policy = <code>percent</code></li>\n<li>periodSeconds = <code>60</code> 缩容间隔为 1min</li>\n<li>value = <code>100</code> 一次缩容最多可以把所有的示例都干掉</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"HPA介绍\"><a href=\"#HPA介绍\" class=\"headerlink\" title=\"HPA介绍\"></a>HPA介绍</h2><p>HPA, Pod 水平自动伸缩（Horizontal Pod Autoscaler）特性， 可以基于CPU利用率自动伸缩 replication controller、deployment和 replica set 中的 pod 数量，（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标custom metrics。 pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets。</p>\n<p>Pod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。</p>\n<blockquote>\n<p><a href=\"https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#pod-%e6%b0%b4%e5%b9%b3%e8%87%aa%e5%8a%a8%e4%bc%b8%e7%bc%a9%e5%b7%a5%e4%bd%9c%e6%9c%ba%e5%88%b6\" target=\"_blank\" rel=\"noopener\">官网文档</a> </p>\n</blockquote>\n<p><strong>HPA 工作机制</strong></p>\n<p>关于 HPA 的原理可以看下 baxiaoshi的云原生学习笔记 <a href=\"https://www.yuque.com/baxiaoshi/tyado3/yw9deb\" target=\"_blank\" rel=\"noopener\">https://www.yuque.com/baxiaoshi/tyado3/yw9deb</a>,本文不做过多介绍，只介绍自行配置hpa特性</p>\n<p><code>HPA</code> 是由 <code>hpacontroller</code> 来实现的, 通过  <code>--horizontal-pod-autoscaler-sync-period</code> 参数 指定周期（默认值为15秒）</p>\n<p>一个HPA的例子如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">autoscaling/v2beta2</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">HorizontalPodAutoscaler</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">php-apache</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleTargetRef:</span></span><br><span class=\"line\"><span class=\"attr\">    apiVersion:</span> <span class=\"string\">apps/v1</span></span><br><span class=\"line\"><span class=\"attr\">    kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">    name:</span> <span class=\"string\">php-apache</span></span><br><span class=\"line\"><span class=\"attr\">  minReplicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">  maxReplicas:</span> <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"attr\">  metrics:</span></span><br><span class=\"line\"><span class=\"attr\">  - type:</span> <span class=\"string\">Resource</span></span><br><span class=\"line\"><span class=\"attr\">    resource:</span></span><br><span class=\"line\"><span class=\"attr\">      name:</span> <span class=\"string\">cpu</span></span><br><span class=\"line\"><span class=\"attr\">      target:</span></span><br><span class=\"line\"><span class=\"attr\">        type:</span> <span class=\"string\">Utilization</span></span><br><span class=\"line\"><span class=\"attr\">        averageUtilization:</span> <span class=\"number\">50</span></span><br><span class=\"line\"><span class=\"attr\">status:</span></span><br><span class=\"line\"><span class=\"attr\">  observedGeneration:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">  lastScaleTime:</span> <span class=\"string\">&lt;some-time&gt;</span></span><br><span class=\"line\"><span class=\"attr\">  currentReplicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">  desiredReplicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">  currentMetrics:</span></span><br><span class=\"line\"><span class=\"attr\">  - type:</span> <span class=\"string\">Resource</span></span><br><span class=\"line\"><span class=\"attr\">    resource:</span></span><br><span class=\"line\"><span class=\"attr\">      name:</span> <span class=\"string\">cpu</span></span><br><span class=\"line\"><span class=\"attr\">      current:</span></span><br><span class=\"line\"><span class=\"attr\">        averageUtilization:</span> <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"attr\">        averageValue:</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h2><p>前面已经介绍了 <code>HPA</code> 的相关信息，相信大家都对 <code>HPA</code> 有了简单的了解, 当使用 <code>Horizontal Pod Autoscaler</code> 管理一组副本缩放时， <strong>有可能因为指标动态的变化造成副本数量频繁的变化，有时这被称为 抖动</strong>。为了避免这种情况，社区引入了 延迟时间的设置，<em>该配置是针对整个集群的配置，不能对应用粒度进行配置</em></p>\n<p><code>--horizontal-pod-autoscaler-downscale-stabilization:</code> 这个 <code>kube-controller-manager</code> 的参数表示缩容冷却时间。 即自从上次缩容执行结束后，多久可以再次执行缩容，默认时间是5分钟(5m0s)。`</p>\n<p>注意：这个配置是集群级别的，只能配置整个集群的扩缩容速率，用户不能自行配置自己的应用扩缩容速率：</p>\n<p>考虑一下场景的应用：</p>\n<ul>\n<li>对于大流量的的web应用。需要非常快的扩容速率，缓慢的缩容速率（为了迎接下一个流量高峰）</li>\n<li>对于处理数据类的应用。需要尽快的扩容（减少处理数据的时间），并尽快缩容（降低成本）</li>\n<li>对于处理常规流量和数据的应用，按照常规的方式配置就可以了</li>\n</ul>\n<p>对于上述3种应用场景，1.17以前的集群是不能支持，这也导致了一些未解决的 <code>issue</code>：</p>\n<ul>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/39090\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/issues/39090</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/65097\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/issues/65097</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/69428\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/issues/69428</a></li>\n</ul>\n<p>为此社区 在<code>1.18</code>引入了 能自行配置<code>HPA</code>速率的新特性 见 相关 <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/20190307-configurable-scale-velocity-for-hpa.md\" target=\"_blank\" rel=\"noopener\">keps</a></p>\n<h2 id=\"原理介绍\"><a href=\"#原理介绍\" class=\"headerlink\" title=\"原理介绍\"></a>原理介绍</h2><p>为了自定义扩缩容行为，需要给单个 <code>hpa</code> 对象添加一个 <code>behavior</code>字段，如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">autoscaling/v2beta2</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">HorizontalPodAutoscaler</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\"><span class=\"attr\">  name:</span> <span class=\"string\">php-apache</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"string\">***</span></span><br><span class=\"line\"><span class=\"attr\">  behavior:</span></span><br><span class=\"line\"><span class=\"attr\">      scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">        policies:</span></span><br><span class=\"line\"><span class=\"attr\">        - type:</span> <span class=\"string\">percent</span></span><br><span class=\"line\"><span class=\"attr\">          value:</span> <span class=\"number\">900</span><span class=\"string\">%</span></span><br><span class=\"line\"><span class=\"attr\">      scaleDown:</span></span><br><span class=\"line\"><span class=\"attr\">        policies:</span></span><br><span class=\"line\"><span class=\"attr\">        - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">          value:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">          periodSeconds:</span> <span class=\"number\">600</span> <span class=\"comment\"># (i.e., scale down one pod every 10 min)</span></span><br><span class=\"line\"><span class=\"attr\">  metrics:</span></span><br><span class=\"line\">    <span class=\"string\">***</span></span><br></pre></td></tr></table></figure>\n<p>其中 <code>behavior</code> 字段 包含 如下对象：</p>\n<ul>\n<li><code>scaleUp</code> 配置扩容时的规则。<ul>\n<li><code>stabilizationWindowSeconds</code>： -该值表示 <code>HPA</code> 控制器应参考的采样时间，以防止副本数量波动。</li>\n<li><code>selectPolicy</code>：可以是<code>min</code>或<code>max</code>,用于选择 <code>policies</code> 中的最大值还是最小值。默认为 <code>max</code>。</li>\n<li><code>policies</code>： 是一个数组，每个元素有如下字段：<ul>\n<li><code>type</code>： 可为<code>pods</code>或者<code>percent</code>（以Pod的绝对数量或当前副本的百分比表示）。</li>\n<li><code>periodSeconds</code> 规则应用的时间范围（以秒为单位），即每多少秒扩缩容一次。</li>\n<li><code>value</code>: 规则的值，设为0 表示禁止 扩容或者缩容</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><code>scaleDown</code> 与 <code>scaleUp</code> 类似，指定的是缩容的规则。</li>\n</ul>\n<p>用户通过控制 HPA中指定参数，来控制HPA扩缩容的逻辑</p>\n<p><code>selectPolicy</code> 字段指示应应用的策略。默认情况下为max，也就是：<strong>将使用尽可能扩大副本的最大数量，而选择尽可能减少副本的最小数量。</strong> 有点绕没关系，待会看下面的例子</p>\n<h2 id=\"场景\"><a href=\"#场景\" class=\"headerlink\" title=\"场景\"></a>场景</h2><h3 id=\"场景1：尽可能快的扩容\"><a href=\"#场景1：尽可能快的扩容\" class=\"headerlink\" title=\"场景1：尽可能快的扩容\"></a>场景1：尽可能快的扩容</h3><blockquote>\n<p>这种模式适合流量增速比较快的场景</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">percent</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">900</span><span class=\"string\">%</span></span><br></pre></td></tr></table></figure>\n<p><code>900%</code> 表示可以扩容的数量为当前副本数量的9倍，也就是可以扩大到当前副本数量的10倍，其他值都是默认值</p>\n<p>如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下：</p>\n<p><code>1 -&gt; 10 -&gt; 100 -&gt; 1000</code></p>\n<p>scaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的<a href=\"https://v1-14.docs.kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82\" target=\"_blank\" rel=\"noopener\">扩缩容算法</a> </p>\n<h3 id=\"场景2：尽可能快的扩容，然后慢慢缩容\"><a href=\"#场景2：尽可能快的扩容，然后慢慢缩容\" class=\"headerlink\" title=\"场景2：尽可能快的扩容，然后慢慢缩容\"></a>场景2：尽可能快的扩容，然后慢慢缩容</h3><blockquote>\n<p>这种模式适合不想快速缩容的场景</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">percent</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">900</span><span class=\"string\">%</span></span><br><span class=\"line\"><span class=\"attr\">  scaleDown:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"attr\">      periodSeconds:</span> <span class=\"number\">600</span> <span class=\"comment\"># (i.e., 每隔10分钟缩容一个pod)</span></span><br></pre></td></tr></table></figure>\n<p>这种配置扩容场景同 场景1，缩容场景下却是不同的，这里配置的是每隔10分钟缩容一个pod</p>\n<p>假如说扩容之后有1000个pod，那么缩容过程中pod数量如下：</p>\n<p><code>1000 -&gt; 1000 -&gt; 1000 -&gt; … (7 more min) -&gt; 999</code></p>\n<h3 id=\"场景3：慢慢扩容，按常规缩容\"><a href=\"#场景3：慢慢扩容，按常规缩容\" class=\"headerlink\" title=\"场景3：慢慢扩容，按常规缩容\"></a>场景3：慢慢扩容，按常规缩容</h3><blockquote>\n<p>这种模式适合不太激进的扩容</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<p>如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下：</p>\n<p><code>1 -&gt; 2 -&gt; 3 -&gt; 4</code></p>\n<p>同样，scaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的<a href=\"https://v1-14.docs.kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/#%E7%AE%97%E6%B3%95%E7%BB%86%E8%8A%82\" target=\"_blank\" rel=\"noopener\">扩缩容算法</a> </p>\n<h3 id=\"场景4：-常规扩容，禁止缩容\"><a href=\"#场景4：-常规扩容，禁止缩容\" class=\"headerlink\" title=\"场景4： 常规扩容，禁止缩容\"></a>场景4： 常规扩容，禁止缩容</h3><blockquote>\n<p>这种模式适用于 不允许应用缩容，或者你想单独控制应用的缩容的场景</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleDown:</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n<p>副本数量将按照常规方式扩容，不会发生缩容</p>\n<h3 id=\"场景5：延迟缩容\"><a href=\"#场景5：延迟缩容\" class=\"headerlink\" title=\"场景5：延迟缩容\"></a>场景5：延迟缩容</h3><blockquote>\n<p>这种模式适用于 用户并不想立即缩容，而是想等待更大的负载时间到来，再计算缩容情况</p>\n</blockquote>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleDown:</span></span><br><span class=\"line\"><span class=\"attr\">    stabilizationWindowSeconds:</span> <span class=\"number\">600</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">5</span></span><br></pre></td></tr></table></figure>\n<p>这种配置情况下 <code>hpa</code> 缩容策略行为如下：</p>\n<ul>\n<li>会采集最近 <code>600s</code> 的（默认300s）的缩容建议，类似于滑动窗口</li>\n<li>选择最大的那个</li>\n<li>按照不大于每秒5个pod的速率缩容</li>\n</ul>\n<p>假如 <code>CurReplicas = 10</code> , <code>HPA controller</code> 每 <code>1min</code> 处理一次:</p>\n<ul>\n<li>前 9 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议：<br>  <code>recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9]</code></li>\n<li>第 10 min，我们增加一个扩缩容建议，比如说是 <code>8</code><br>  <code>recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9，8]</code><br>  HPA 算法会取其中最大的一个 <code>10</code>，因此应用不会发生缩容，<code>repicas</code> 的值不变</li>\n<li>第 11 min，我们增加一个扩缩容建议，比如 <code>7</code>，维持 <code>600s</code> 的滑动窗口，因此需要把第一个 <code>10</code> 去掉，如下：<br>  <code>recommendations = [9, 8, 9, 9, 8, 9, 8, 9, 8, 7]</code><br>  HPA 算法会取最大的一个 <code>9</code>, 应用副本数量变化： <code>10 -&gt; 9</code></li>\n</ul>\n<h3 id=\"场景6：-避免错误的扩容\"><a href=\"#场景6：-避免错误的扩容\" class=\"headerlink\" title=\"场景6： 避免错误的扩容\"></a>场景6： 避免错误的扩容</h3><blockquote>\n<p>这种模式在数据处理流中比较常见，用户想要根据队列中的数据来扩容，当数据较多时快速扩容。当指标有抖动时并不想扩容</p>\n</blockquote>\n<p>HPA的配置如下：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">behavior:</span></span><br><span class=\"line\"><span class=\"attr\">  scaleUp:</span></span><br><span class=\"line\"><span class=\"attr\">    stabilizationWindowSeconds:</span> <span class=\"number\">300</span></span><br><span class=\"line\"><span class=\"attr\">    policies:</span></span><br><span class=\"line\"><span class=\"attr\">    - type:</span> <span class=\"string\">pods</span></span><br><span class=\"line\"><span class=\"attr\">      value:</span> <span class=\"number\">20</span></span><br></pre></td></tr></table></figure>\n<p>这种配置情况下 <code>hpa</code> 扩容策略行为如下：</p>\n<ul>\n<li>会采集最近 <code>300s</code> 的（默认0s）的扩容建议，类似于滑动窗口</li>\n<li>选择最小的那个</li>\n<li>按照不大于每秒20个pod的速率扩容</li>\n</ul>\n<p>假如 <code>CurReplicas = 2</code> , <code>HPA controller</code> 每 <code>1min</code> 处理一次:</p>\n<ul>\n<li>前 5 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议：<br>  <code>recommendations = [2, 3, 19, 10, 3]</code></li>\n<li>第 6 min，我们增加一个扩缩容建议，比如说是 <code>4</code><br>  <code>recommendations = [2, 3, 19, 10, 3, 4]</code><br>  HPA 算法会取其中最小的一个 <code>2</code>，因此应用不会发生缩容，<code>repicas</code> 的值不变</li>\n<li>第 11 min，我们增加一个扩缩容建议，比如 <code>7</code>，维持 <code>300s</code> 的滑动窗口，因此需要把第一个 <code>2</code> 去掉，如下：<br>  <code>recommendations = [3, 19, 10, 3, 4，7]</code><br>  HPA 算法会取最小的一个 <code>3</code>, 应用副本数量变化： <code>2 -&gt; 3</code></li>\n</ul>\n<h2 id=\"算法原理\"><a href=\"#算法原理\" class=\"headerlink\" title=\"算法原理\"></a>算法原理</h2><p>算法的伪代码如下：</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  HPA controller 中的for循环</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> &#123;</span><br><span class=\"line\">    desiredReplicas = AnyAlgorithmInHPAController(...)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 扩容场景</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> desiredReplicas &gt; curReplicas &#123;</span><br><span class=\"line\">      replicas = []<span class=\"keyword\">int</span>&#123;&#125;</span><br><span class=\"line\">      <span class=\"keyword\">for</span> _, policy := <span class=\"keyword\">range</span> behavior.ScaleUp.Policies &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> policy.<span class=\"keyword\">type</span> == <span class=\"string\">\"pods\"</span> &#123;</span><br><span class=\"line\">          replicas = <span class=\"built_in\">append</span>(replicas, CurReplicas + policy.Value)</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> policy.<span class=\"keyword\">type</span> == <span class=\"string\">\"percent\"</span> &#123;</span><br><span class=\"line\">          replicas = <span class=\"built_in\">append</span>(replicas, CurReplicas * (<span class=\"number\">1</span> + policy.Value/<span class=\"number\">100</span>))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> behavior.ScaleUp.selectPolicy == <span class=\"string\">\"max\"</span> &#123;</span><br><span class=\"line\">        scaleUpLimit = max(replicas)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        scaleUpLimit = min(replicas)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"comment\">// 这里的min可以理解为 尽量维持原状，即如果是扩容，尽量取最小的那个</span></span><br><span class=\"line\">      limitedReplicas = min(max, desiredReplicas)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 缩容场景</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> desiredReplicas &lt; curReplicas &#123;</span><br><span class=\"line\">      <span class=\"keyword\">for</span> _, policy := <span class=\"keyword\">range</span> behaviro.scaleDown.Policies &#123;</span><br><span class=\"line\">        replicas = []<span class=\"keyword\">int</span>&#123;&#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> policy.<span class=\"keyword\">type</span> == <span class=\"string\">\"pods\"</span> &#123;</span><br><span class=\"line\">          replicas = <span class=\"built_in\">append</span>(replicas, CurReplicas - policy.Value)</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> policy.<span class=\"keyword\">type</span> == <span class=\"string\">\"percent\"</span> &#123;</span><br><span class=\"line\">          replicas = <span class=\"built_in\">append</span>(replicas, CurReplicas * (<span class=\"number\">1</span> - policy.Value /<span class=\"number\">100</span>))</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> behavior.ScaleDown.SelectPolicy == <span class=\"string\">\"max\"</span> &#123;</span><br><span class=\"line\">          scaleDownLimit = min(replicas)</span><br><span class=\"line\">        &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">          scaleDownLimit = max(replicas)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"comment\">// 这里的max可以理解为 尽量维持原状，即如果是缩容，尽量取最大的那个</span></span><br><span class=\"line\">        limitedReplicas = max(min, desiredReplicas)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    storeRecommend(limitedReplicas, scaleRecommendations)</span><br><span class=\"line\">    <span class=\"comment\">// 选择合适的扩缩容副本</span></span><br><span class=\"line\">    nextReplicas := applyRecommendationIfNeeded(scaleRecommendations)</span><br><span class=\"line\">    <span class=\"comment\">// 扩缩容</span></span><br><span class=\"line\">    setReplicas(nextReplicas)</span><br><span class=\"line\">    sleep(ControllerSleepTime)</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"默认值\"><a href=\"#默认值\" class=\"headerlink\" title=\"默认值\"></a>默认值</h2><p>为了平滑得扩速容，默认值是有必要的<br><code>behavior</code> 的默认值如下：</p>\n<ul>\n<li>behavior.scaleDown.stabilizationWindowSeconds = 300, 缩容情况下默认等待 5 min中再缩容.</li>\n<li>behavior.scaleUp.stabilizationWindowSeconds = 0, 扩容时立即扩容，不等待</li>\n<li>behavior.scaleUp.policies 默认值如下：<ul>\n<li>百分比策略<ul>\n<li>policy = <code>percent</code></li>\n<li>periodSeconds = <code>60</code>, 扩容间隔为 1min</li>\n<li>value = <code>100</code> 每次最多扩容翻倍</li>\n</ul>\n</li>\n<li>Pod个数策略<ul>\n<li>policy = <code>pods</code></li>\n<li>periodSeconds = <code>60</code>, 扩容间隔为 1min</li>\n<li>value = <code>4</code> 每次最多扩容4个</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>behavior.scaleDown.policies 默认值如下：<ul>\n<li>百分比策略<ul>\n<li>policy = <code>percent</code></li>\n<li>periodSeconds = <code>60</code> 缩容间隔为 1min</li>\n<li>value = <code>100</code> 一次缩容最多可以把所有的示例都干掉</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n"},{"layout":"post","title":" 谈谈 epoll ","date":"2019-05-10T03:40:18.000Z","author":"zhaojizhuang","_content":"\n\n## epoll\n\n## IO 多路复用\n\n目前支持I/O多路复用的系统调用有 `select，pselect，poll，epoll` ，I/O多路复用就是通过一种机制，`一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作`\n\n### select\n\n调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写），或者超时，函数返回。当select函数返回后，**可以通过遍历fdset，来找到就绪的描述符**。\n\nselect的流程\n\n假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，**操作系统把进程A分别加入这三个socket的等待队列中**\n\n![](https://pic4.zhimg.com/80/v2-0cccb4976f8f2c2f8107f2b3a5bc46b3_720w.jpg)\n\n当任何一个socket收到数据后，中断程序将唤起进程,将进程从所有**fd（socket）的等待队列中**移除，再将进程加入到工作队列里面\n\n进程A被唤醒后，它知道至少有一个socket接收了数据。**程序需遍历一遍socket列表，可以得到就绪的socket**\n\n缺点：\n\n- 其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，**默认只能监视1024个socket**。\n\n- 其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。\n\n### poll与select一样，只是去掉了 1024的限制\n\n### epoll\n\n`epoll` 事先通过 `epoll_ctl()` 来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 `callback` 的回调机制，迅速激活这个文件描述符，当进程调用 `epoll_wait()` 时便得到通知。(**此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在**。)\n\nepoll使**用一个文件描述符(`eventpoll`)管理多个描述符**，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次\n\n```c\nint s = socket(AF_INET, SOCK_STREAM, 0);   \nbind(s, ...)\nlisten(s, ...)\n\nint epfd = epoll_create(...);\nepoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中\n\nwhile(1){\n    int n = epoll_wait(...)\n    for(接收到数据的socket){\n        //处理\n    }\n}\n```\n流程：\n首先创建 epoll对象\n创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket\n\n假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程\n\n![](https://pic1.zhimg.com/80/v2-90632d0dc3ded7f91379b848ab53974c_720w.jpg)\n\n当socket接收到数据，**中断程序一方面修改rdlist**，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，**进程A可以知道哪些socket发生了变化**。\n\n![](https://pic4.zhimg.com/80/v2-40bd5825e27cf49b7fd9a59dfcbe4d6f_720w.jpg)\n\n\n\n参考文章 \n1. https://www.jianshu.com/p/dfd940e7fca2\n\n[2 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(1)](https://zhuanlan.zhihu.com/p/63179839)\n\n[3 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(2)](https://zhuanlan.zhihu.com/p/64138532)\n\n[4 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(3)](https://zhuanlan.zhihu.com/p/64746509)\n","source":"_posts/epoll.md","raw":"---\nlayout: post\ntitle:  \" 谈谈 epoll \"\ndate:   2019-05-10 11:40:18 +0800\ncategories: \ntags:  [\"linux\", \"epoll\"]\nauthor: zhaojizhuang\n---\n\n\n## epoll\n\n## IO 多路复用\n\n目前支持I/O多路复用的系统调用有 `select，pselect，poll，epoll` ，I/O多路复用就是通过一种机制，`一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作`\n\n### select\n\n调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写），或者超时，函数返回。当select函数返回后，**可以通过遍历fdset，来找到就绪的描述符**。\n\nselect的流程\n\n假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，**操作系统把进程A分别加入这三个socket的等待队列中**\n\n![](https://pic4.zhimg.com/80/v2-0cccb4976f8f2c2f8107f2b3a5bc46b3_720w.jpg)\n\n当任何一个socket收到数据后，中断程序将唤起进程,将进程从所有**fd（socket）的等待队列中**移除，再将进程加入到工作队列里面\n\n进程A被唤醒后，它知道至少有一个socket接收了数据。**程序需遍历一遍socket列表，可以得到就绪的socket**\n\n缺点：\n\n- 其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，**默认只能监视1024个socket**。\n\n- 其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。\n\n### poll与select一样，只是去掉了 1024的限制\n\n### epoll\n\n`epoll` 事先通过 `epoll_ctl()` 来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 `callback` 的回调机制，迅速激活这个文件描述符，当进程调用 `epoll_wait()` 时便得到通知。(**此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在**。)\n\nepoll使**用一个文件描述符(`eventpoll`)管理多个描述符**，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次\n\n```c\nint s = socket(AF_INET, SOCK_STREAM, 0);   \nbind(s, ...)\nlisten(s, ...)\n\nint epfd = epoll_create(...);\nepoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中\n\nwhile(1){\n    int n = epoll_wait(...)\n    for(接收到数据的socket){\n        //处理\n    }\n}\n```\n流程：\n首先创建 epoll对象\n创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket\n\n假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程\n\n![](https://pic1.zhimg.com/80/v2-90632d0dc3ded7f91379b848ab53974c_720w.jpg)\n\n当socket接收到数据，**中断程序一方面修改rdlist**，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，**进程A可以知道哪些socket发生了变化**。\n\n![](https://pic4.zhimg.com/80/v2-40bd5825e27cf49b7fd9a59dfcbe4d6f_720w.jpg)\n\n\n\n参考文章 \n1. https://www.jianshu.com/p/dfd940e7fca2\n\n[2 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(1)](https://zhuanlan.zhihu.com/p/63179839)\n\n[3 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(2)](https://zhuanlan.zhihu.com/p/64138532)\n\n[4 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(3)](https://zhuanlan.zhihu.com/p/64746509)\n","slug":"epoll","published":1,"updated":"2020-05-15T15:16:08.756Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l890006r5flxgkuv528","content":"<h2 id=\"epoll\"><a href=\"#epoll\" class=\"headerlink\" title=\"epoll\"></a>epoll</h2><h2 id=\"IO-多路复用\"><a href=\"#IO-多路复用\" class=\"headerlink\" title=\"IO 多路复用\"></a>IO 多路复用</h2><p>目前支持I/O多路复用的系统调用有 <code>select，pselect，poll，epoll</code> ，I/O多路复用就是通过一种机制，<code>一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作</code></p>\n<h3 id=\"select\"><a href=\"#select\" class=\"headerlink\" title=\"select\"></a>select</h3><p>调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写），或者超时，函数返回。当select函数返回后，<strong>可以通过遍历fdset，来找到就绪的描述符</strong>。</p>\n<p>select的流程</p>\n<p>假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，<strong>操作系统把进程A分别加入这三个socket的等待队列中</strong></p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-0cccb4976f8f2c2f8107f2b3a5bc46b3_720w.jpg\" alt=\"\"></p>\n<p>当任何一个socket收到数据后，中断程序将唤起进程,将进程从所有<strong>fd（socket）的等待队列中</strong>移除，再将进程加入到工作队列里面</p>\n<p>进程A被唤醒后，它知道至少有一个socket接收了数据。<strong>程序需遍历一遍socket列表，可以得到就绪的socket</strong></p>\n<p>缺点：</p>\n<ul>\n<li><p>其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，<strong>默认只能监视1024个socket</strong>。</p>\n</li>\n<li><p>其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。</p>\n</li>\n</ul>\n<h3 id=\"poll与select一样，只是去掉了-1024的限制\"><a href=\"#poll与select一样，只是去掉了-1024的限制\" class=\"headerlink\" title=\"poll与select一样，只是去掉了 1024的限制\"></a>poll与select一样，只是去掉了 1024的限制</h3><h3 id=\"epoll-1\"><a href=\"#epoll-1\" class=\"headerlink\" title=\"epoll\"></a>epoll</h3><p><code>epoll</code> 事先通过 <code>epoll_ctl()</code> 来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 <code>callback</code> 的回调机制，迅速激活这个文件描述符，当进程调用 <code>epoll_wait()</code> 时便得到通知。(<strong>此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在</strong>。)</p>\n<p>epoll使<strong>用一个文件描述符(<code>eventpoll</code>)管理多个描述符</strong>，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> s = socket(AF_INET, SOCK_STREAM, <span class=\"number\">0</span>);   </span><br><span class=\"line\">bind(s, ...)</span><br><span class=\"line\">listen(s, ...)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> epfd = epoll_create(...);</span><br><span class=\"line\">epoll_ctl(epfd, ...); <span class=\"comment\">//将所有需要监听的socket添加到epfd中</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span>(<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = epoll_wait(...)</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(接收到数据的socket)&#123;</span><br><span class=\"line\">        <span class=\"comment\">//处理</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>流程：<br>首先创建 epoll对象<br>创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket</p>\n<p>假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-90632d0dc3ded7f91379b848ab53974c_720w.jpg\" alt=\"\"></p>\n<p>当socket接收到数据，<strong>中断程序一方面修改rdlist</strong>，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，<strong>进程A可以知道哪些socket发生了变化</strong>。</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-40bd5825e27cf49b7fd9a59dfcbe4d6f_720w.jpg\" alt=\"\"></p>\n<p>参考文章 </p>\n<ol>\n<li><a href=\"https://www.jianshu.com/p/dfd940e7fca2\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/dfd940e7fca2</a></li>\n</ol>\n<p><a href=\"https://zhuanlan.zhihu.com/p/63179839\" target=\"_blank\" rel=\"noopener\">2 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(1)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/64138532\" target=\"_blank\" rel=\"noopener\">3 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(2)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/64746509\" target=\"_blank\" rel=\"noopener\">4 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(3)</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"epoll\"><a href=\"#epoll\" class=\"headerlink\" title=\"epoll\"></a>epoll</h2><h2 id=\"IO-多路复用\"><a href=\"#IO-多路复用\" class=\"headerlink\" title=\"IO 多路复用\"></a>IO 多路复用</h2><p>目前支持I/O多路复用的系统调用有 <code>select，pselect，poll，epoll</code> ，I/O多路复用就是通过一种机制，<code>一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作</code></p>\n<h3 id=\"select\"><a href=\"#select\" class=\"headerlink\" title=\"select\"></a>select</h3><p>调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写），或者超时，函数返回。当select函数返回后，<strong>可以通过遍历fdset，来找到就绪的描述符</strong>。</p>\n<p>select的流程</p>\n<p>假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，<strong>操作系统把进程A分别加入这三个socket的等待队列中</strong></p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-0cccb4976f8f2c2f8107f2b3a5bc46b3_720w.jpg\" alt=\"\"></p>\n<p>当任何一个socket收到数据后，中断程序将唤起进程,将进程从所有<strong>fd（socket）的等待队列中</strong>移除，再将进程加入到工作队列里面</p>\n<p>进程A被唤醒后，它知道至少有一个socket接收了数据。<strong>程序需遍历一遍socket列表，可以得到就绪的socket</strong></p>\n<p>缺点：</p>\n<ul>\n<li><p>其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，<strong>默认只能监视1024个socket</strong>。</p>\n</li>\n<li><p>其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。</p>\n</li>\n</ul>\n<h3 id=\"poll与select一样，只是去掉了-1024的限制\"><a href=\"#poll与select一样，只是去掉了-1024的限制\" class=\"headerlink\" title=\"poll与select一样，只是去掉了 1024的限制\"></a>poll与select一样，只是去掉了 1024的限制</h3><h3 id=\"epoll-1\"><a href=\"#epoll-1\" class=\"headerlink\" title=\"epoll\"></a>epoll</h3><p><code>epoll</code> 事先通过 <code>epoll_ctl()</code> 来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 <code>callback</code> 的回调机制，迅速激活这个文件描述符，当进程调用 <code>epoll_wait()</code> 时便得到通知。(<strong>此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在</strong>。)</p>\n<p>epoll使<strong>用一个文件描述符(<code>eventpoll</code>)管理多个描述符</strong>，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">int</span> s = socket(AF_INET, SOCK_STREAM, <span class=\"number\">0</span>);   </span><br><span class=\"line\">bind(s, ...)</span><br><span class=\"line\">listen(s, ...)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">int</span> epfd = epoll_create(...);</span><br><span class=\"line\">epoll_ctl(epfd, ...); <span class=\"comment\">//将所有需要监听的socket添加到epfd中</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span>(<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> n = epoll_wait(...)</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(接收到数据的socket)&#123;</span><br><span class=\"line\">        <span class=\"comment\">//处理</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>流程：<br>首先创建 epoll对象<br>创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket</p>\n<p>假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程</p>\n<p><img src=\"https://pic1.zhimg.com/80/v2-90632d0dc3ded7f91379b848ab53974c_720w.jpg\" alt=\"\"></p>\n<p>当socket接收到数据，<strong>中断程序一方面修改rdlist</strong>，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，<strong>进程A可以知道哪些socket发生了变化</strong>。</p>\n<p><img src=\"https://pic4.zhimg.com/80/v2-40bd5825e27cf49b7fd9a59dfcbe4d6f_720w.jpg\" alt=\"\"></p>\n<p>参考文章 </p>\n<ol>\n<li><a href=\"https://www.jianshu.com/p/dfd940e7fca2\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/dfd940e7fca2</a></li>\n</ol>\n<p><a href=\"https://zhuanlan.zhihu.com/p/63179839\" target=\"_blank\" rel=\"noopener\">2 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(1)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/64138532\" target=\"_blank\" rel=\"noopener\">3 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(2)</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/64746509\" target=\"_blank\" rel=\"noopener\">4 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(3)</a></p>\n"},{"layout":"post","title":"容器网络","date":"2018-10-25T09:29:18.000Z","author":"zhaojizhuang","_content":"\n# 容器网络\n\n\n##  vxlan \n\nvxlan原理: VXLAN通过MAC-in-UDP的报文封装，实现了二层报文在三层网络上的透传,属于overlay网络\n\n## Flannel\n\n首先，flannel利用**`Kubernetes-API(这里就是取node.spec.podCIDR)或者etcd`**用于存储整个集群的网络配置，其中最主要的内容为设置集群的网络地址空间。例如，设定整个集群内所有容器的IP都取自网段“10.1.0.0/16”。\n\n接着，flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配。\n\n`flannel` 的 `UDP` 模式和 `Vxlan` 模式 `host-gw` 模式\n\n- `UDP` 模式是 三层 `overlay`,即，将原始数据包的三层包（IP包）装在 `UDP` 包里,通过 ip+端口 传到目的地，ip为目标node ip 端口为目标节点上flanneld进程监听的8285端口，解析后传入flannel0设备进入内核网络协议栈，\nUDP模式下 封包解包是在 flanneld里进行的也就是用户态下\n\n![](https://static001.geekbang.org/resource/image/84/8d/84caa6dc3f9dcdf8b88b56bd2e22138d.png)\n\n![](https://static001.geekbang.org/resource/image/e6/f0/e6827cecb75641d3c8838f2213543cf0.png)\n\n**重要！！！ 《深入解析kubernetes》** 33章  https://time.geekbang.org/column/article/65287\n\n- VxLan 模式 是二层 `overlay`,即将原始Ethernet包（MAC包）封装起来，通过vtep设备发到目的vtep，vxlan是内核模块，vtep是flannneld创建的，vxlan封包解封完全是在内核态完成的\n- \n\n![](https://static001.geekbang.org/resource/image/43/41/43f5ebb001145ecd896fd10fb27c5c41.png)\n\n - 注意点 \n  - inner mac 为 目的vtep的mac\n  - outer ip为目的node的ip **这一点和UDP有区别**\n下一跳ip对应的mac地址是ARP表里记录的，inner mac对应的arp记录是 flanneld维护的，outer mac arp表是node自学习的\n\n![](https://static001.geekbang.org/resource/image/ce/38/cefe6b99422fba768c53f0093947cd38.png)\n\n- `host-gw` 模式的工作原理,是在 节点上加路由表，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。\n这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。\n\n```shell\n$ ip route\n...\n<目的容器IP地址段> via <网关的IP地址> dev eth0# 网关的 IP 地址，正是目的容器所在宿主机的 IP 地址\n```\n\n**Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。如果分布在不同的子网里是不行的，只是三层可达**\n\n### POD IP的分配\n#### 使用CNI后，即配置了 `kubelet` 的 `--network-plugin=cni`，容器的IP分配：\nkubelet 先创建pause容器生成network namespace\n调用 网络driver CNI driver\nCNI driver 根据配置调用具体的cni 插件\ncni 插件给pause 容器配置网络\npod 中其他的容器都使用 pause 容器的网络\n\n#### CNM模式\nPod IP是docker engine分配的，Pod也是以docker0为网关，通过veth连接network namespace\n#### flannel的两种方式 CNI CNM总结\nCNI中，docker0的ip与Pod无关，Pod总是生成的时候才去动态的申请自己的IP，而CNM模式下，Pod的网段在docker engine启动时就已经决定。\nCNI只是一个网络接口规范，各种功能都由插件实现，flannel只是插件的一种，而且docker也只是容器载体的一种选择，Kubernetes还可以使用其他的，\n\n## cluster IP的分配\n    是在kube-apiserver中 `pkg/registry/core/service/ipallocator`中分配的\n    \n    \n## network policy \n\n```yaml\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          user: alice\n    - podSelector:\n        matchLabels:\n          role: client\n```\n\n像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系，表示的是yaml数组里的两个元素\n\n```yaml\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          user: alice\n      podSelector:\n        matchLabels:\n          role: client\n```\n\n像上面这样定义的 namespaceSelector 和 podSelector，是“与”（AND）的关系，yaml里表示的是一个数组元素的两个字段\n\nKubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。\n\n\n## 通过NodePort来访问service的话，client的源ip会被做SNAT\n\n```yaml\n          client\n             \\ ^\n              \\ \\\n               v \\\n   node 1 <--- node 2\n    | ^   SNAT\n    | |   --->\n    v |\n endpoint\n```\n\n流程：\n\n- 客户端发送数据包到 node2:nodePort\n- node2 使用它自己的 IP 地址替换数据包的源 IP 地址（SNAT）\n- node2 使用 pod IP 地址替换数据包的目的 IP 地址\n- 数据包被路由到 node 1，然后交给 endpoint\n- Pod 的回复被路由回 node2\n- Pod 的回复被发送回给客户端\n\n可以将 `service.spec.externalTrafficPolicy` 的值为 Local，请求就只会被代理到本地 endpoints 而不会被转发到其它节点。这样就保留了最初的源 IP 地址 **不会对访问NodePort的client ip做 SNAT了**。如果没有本地 endpoints，发送到这个节点的数据包将会被丢弃。","source":"_posts/container-network.md","raw":"---\nlayout: post\ntitle:  \"容器网络\"\ndate:   2018-10-25 17:29:18 +0800\ncategories: k8s\ntags:  [\"k8s\", \"容器网络\"]\nauthor: zhaojizhuang\n\n---\n\n# 容器网络\n\n\n##  vxlan \n\nvxlan原理: VXLAN通过MAC-in-UDP的报文封装，实现了二层报文在三层网络上的透传,属于overlay网络\n\n## Flannel\n\n首先，flannel利用**`Kubernetes-API(这里就是取node.spec.podCIDR)或者etcd`**用于存储整个集群的网络配置，其中最主要的内容为设置集群的网络地址空间。例如，设定整个集群内所有容器的IP都取自网段“10.1.0.0/16”。\n\n接着，flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配。\n\n`flannel` 的 `UDP` 模式和 `Vxlan` 模式 `host-gw` 模式\n\n- `UDP` 模式是 三层 `overlay`,即，将原始数据包的三层包（IP包）装在 `UDP` 包里,通过 ip+端口 传到目的地，ip为目标node ip 端口为目标节点上flanneld进程监听的8285端口，解析后传入flannel0设备进入内核网络协议栈，\nUDP模式下 封包解包是在 flanneld里进行的也就是用户态下\n\n![](https://static001.geekbang.org/resource/image/84/8d/84caa6dc3f9dcdf8b88b56bd2e22138d.png)\n\n![](https://static001.geekbang.org/resource/image/e6/f0/e6827cecb75641d3c8838f2213543cf0.png)\n\n**重要！！！ 《深入解析kubernetes》** 33章  https://time.geekbang.org/column/article/65287\n\n- VxLan 模式 是二层 `overlay`,即将原始Ethernet包（MAC包）封装起来，通过vtep设备发到目的vtep，vxlan是内核模块，vtep是flannneld创建的，vxlan封包解封完全是在内核态完成的\n- \n\n![](https://static001.geekbang.org/resource/image/43/41/43f5ebb001145ecd896fd10fb27c5c41.png)\n\n - 注意点 \n  - inner mac 为 目的vtep的mac\n  - outer ip为目的node的ip **这一点和UDP有区别**\n下一跳ip对应的mac地址是ARP表里记录的，inner mac对应的arp记录是 flanneld维护的，outer mac arp表是node自学习的\n\n![](https://static001.geekbang.org/resource/image/ce/38/cefe6b99422fba768c53f0093947cd38.png)\n\n- `host-gw` 模式的工作原理,是在 节点上加路由表，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。\n这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。\n\n```shell\n$ ip route\n...\n<目的容器IP地址段> via <网关的IP地址> dev eth0# 网关的 IP 地址，正是目的容器所在宿主机的 IP 地址\n```\n\n**Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。如果分布在不同的子网里是不行的，只是三层可达**\n\n### POD IP的分配\n#### 使用CNI后，即配置了 `kubelet` 的 `--network-plugin=cni`，容器的IP分配：\nkubelet 先创建pause容器生成network namespace\n调用 网络driver CNI driver\nCNI driver 根据配置调用具体的cni 插件\ncni 插件给pause 容器配置网络\npod 中其他的容器都使用 pause 容器的网络\n\n#### CNM模式\nPod IP是docker engine分配的，Pod也是以docker0为网关，通过veth连接network namespace\n#### flannel的两种方式 CNI CNM总结\nCNI中，docker0的ip与Pod无关，Pod总是生成的时候才去动态的申请自己的IP，而CNM模式下，Pod的网段在docker engine启动时就已经决定。\nCNI只是一个网络接口规范，各种功能都由插件实现，flannel只是插件的一种，而且docker也只是容器载体的一种选择，Kubernetes还可以使用其他的，\n\n## cluster IP的分配\n    是在kube-apiserver中 `pkg/registry/core/service/ipallocator`中分配的\n    \n    \n## network policy \n\n```yaml\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          user: alice\n    - podSelector:\n        matchLabels:\n          role: client\n```\n\n像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系，表示的是yaml数组里的两个元素\n\n```yaml\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          user: alice\n      podSelector:\n        matchLabels:\n          role: client\n```\n\n像上面这样定义的 namespaceSelector 和 podSelector，是“与”（AND）的关系，yaml里表示的是一个数组元素的两个字段\n\nKubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。\n\n\n## 通过NodePort来访问service的话，client的源ip会被做SNAT\n\n```yaml\n          client\n             \\ ^\n              \\ \\\n               v \\\n   node 1 <--- node 2\n    | ^   SNAT\n    | |   --->\n    v |\n endpoint\n```\n\n流程：\n\n- 客户端发送数据包到 node2:nodePort\n- node2 使用它自己的 IP 地址替换数据包的源 IP 地址（SNAT）\n- node2 使用 pod IP 地址替换数据包的目的 IP 地址\n- 数据包被路由到 node 1，然后交给 endpoint\n- Pod 的回复被路由回 node2\n- Pod 的回复被发送回给客户端\n\n可以将 `service.spec.externalTrafficPolicy` 的值为 Local，请求就只会被代理到本地 endpoints 而不会被转发到其它节点。这样就保留了最初的源 IP 地址 **不会对访问NodePort的client ip做 SNAT了**。如果没有本地 endpoints，发送到这个节点的数据包将会被丢弃。","slug":"container-network","published":1,"updated":"2020-05-15T15:39:57.315Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l8a0007r5flbysp2k1q","content":"<h1 id=\"容器网络\"><a href=\"#容器网络\" class=\"headerlink\" title=\"容器网络\"></a>容器网络</h1><h2 id=\"vxlan\"><a href=\"#vxlan\" class=\"headerlink\" title=\"vxlan\"></a>vxlan</h2><p>vxlan原理: VXLAN通过MAC-in-UDP的报文封装，实现了二层报文在三层网络上的透传,属于overlay网络</p>\n<h2 id=\"Flannel\"><a href=\"#Flannel\" class=\"headerlink\" title=\"Flannel\"></a>Flannel</h2><p>首先，flannel利用<strong><code>Kubernetes-API(这里就是取node.spec.podCIDR)或者etcd</code></strong>用于存储整个集群的网络配置，其中最主要的内容为设置集群的网络地址空间。例如，设定整个集群内所有容器的IP都取自网段“10.1.0.0/16”。</p>\n<p>接着，flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配。</p>\n<p><code>flannel</code> 的 <code>UDP</code> 模式和 <code>Vxlan</code> 模式 <code>host-gw</code> 模式</p>\n<ul>\n<li><code>UDP</code> 模式是 三层 <code>overlay</code>,即，将原始数据包的三层包（IP包）装在 <code>UDP</code> 包里,通过 ip+端口 传到目的地，ip为目标node ip 端口为目标节点上flanneld进程监听的8285端口，解析后传入flannel0设备进入内核网络协议栈，<br>UDP模式下 封包解包是在 flanneld里进行的也就是用户态下</li>\n</ul>\n<p><img src=\"https://static001.geekbang.org/resource/image/84/8d/84caa6dc3f9dcdf8b88b56bd2e22138d.png\" alt=\"\"></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/e6/f0/e6827cecb75641d3c8838f2213543cf0.png\" alt=\"\"></p>\n<p><strong>重要！！！ 《深入解析kubernetes》</strong> 33章  <a href=\"https://time.geekbang.org/column/article/65287\" target=\"_blank\" rel=\"noopener\">https://time.geekbang.org/column/article/65287</a></p>\n<ul>\n<li>VxLan 模式 是二层 <code>overlay</code>,即将原始Ethernet包（MAC包）封装起来，通过vtep设备发到目的vtep，vxlan是内核模块，vtep是flannneld创建的，vxlan封包解封完全是在内核态完成的</li>\n<li></li>\n</ul>\n<p><img src=\"https://static001.geekbang.org/resource/image/43/41/43f5ebb001145ecd896fd10fb27c5c41.png\" alt=\"\"></p>\n<ul>\n<li>注意点 <ul>\n<li>inner mac 为 目的vtep的mac</li>\n<li>outer ip为目的node的ip <strong>这一点和UDP有区别</strong><br>下一跳ip对应的mac地址是ARP表里记录的，inner mac对应的arp记录是 flanneld维护的，outer mac arp表是node自学习的</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://static001.geekbang.org/resource/image/ce/38/cefe6b99422fba768c53f0093947cd38.png\" alt=\"\"></p>\n<ul>\n<li><code>host-gw</code> 模式的工作原理,是在 节点上加路由表，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。<br>这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> ip route</span></span><br><span class=\"line\">...</span><br><span class=\"line\">&lt;目的容器IP地址段&gt; via &lt;网关的IP地址&gt; dev eth0# 网关的 IP 地址，正是目的容器所在宿主机的 IP 地址</span><br></pre></td></tr></table></figure>\n<p><strong>Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。如果分布在不同的子网里是不行的，只是三层可达</strong></p>\n<h3 id=\"POD-IP的分配\"><a href=\"#POD-IP的分配\" class=\"headerlink\" title=\"POD IP的分配\"></a>POD IP的分配</h3><h4 id=\"使用CNI后，即配置了-kubelet-的-network-plugin-cni，容器的IP分配：\"><a href=\"#使用CNI后，即配置了-kubelet-的-network-plugin-cni，容器的IP分配：\" class=\"headerlink\" title=\"使用CNI后，即配置了 kubelet 的 --network-plugin=cni，容器的IP分配：\"></a>使用CNI后，即配置了 <code>kubelet</code> 的 <code>--network-plugin=cni</code>，容器的IP分配：</h4><p>kubelet 先创建pause容器生成network namespace<br>调用 网络driver CNI driver<br>CNI driver 根据配置调用具体的cni 插件<br>cni 插件给pause 容器配置网络<br>pod 中其他的容器都使用 pause 容器的网络</p>\n<h4 id=\"CNM模式\"><a href=\"#CNM模式\" class=\"headerlink\" title=\"CNM模式\"></a>CNM模式</h4><p>Pod IP是docker engine分配的，Pod也是以docker0为网关，通过veth连接network namespace</p>\n<h4 id=\"flannel的两种方式-CNI-CNM总结\"><a href=\"#flannel的两种方式-CNI-CNM总结\" class=\"headerlink\" title=\"flannel的两种方式 CNI CNM总结\"></a>flannel的两种方式 CNI CNM总结</h4><p>CNI中，docker0的ip与Pod无关，Pod总是生成的时候才去动态的申请自己的IP，而CNM模式下，Pod的网段在docker engine启动时就已经决定。<br>CNI只是一个网络接口规范，各种功能都由插件实现，flannel只是插件的一种，而且docker也只是容器载体的一种选择，Kubernetes还可以使用其他的，</p>\n<h2 id=\"cluster-IP的分配\"><a href=\"#cluster-IP的分配\" class=\"headerlink\" title=\"cluster IP的分配\"></a>cluster IP的分配</h2><pre><code>是在kube-apiserver中 `pkg/registry/core/service/ipallocator`中分配的\n</code></pre><h2 id=\"network-policy\"><a href=\"#network-policy\" class=\"headerlink\" title=\"network policy\"></a>network policy</h2><figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">ingress:</span></span><br><span class=\"line\"><span class=\"attr\">- from:</span></span><br><span class=\"line\"><span class=\"attr\">  - namespaceSelector:</span></span><br><span class=\"line\"><span class=\"attr\">      matchLabels:</span></span><br><span class=\"line\"><span class=\"attr\">        user:</span> <span class=\"string\">alice</span></span><br><span class=\"line\"><span class=\"attr\">  - podSelector:</span></span><br><span class=\"line\"><span class=\"attr\">      matchLabels:</span></span><br><span class=\"line\"><span class=\"attr\">        role:</span> <span class=\"string\">client</span></span><br></pre></td></tr></table></figure>\n<p>像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系，表示的是yaml数组里的两个元素</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">ingress:</span></span><br><span class=\"line\"><span class=\"attr\">- from:</span></span><br><span class=\"line\"><span class=\"attr\">  - namespaceSelector:</span></span><br><span class=\"line\"><span class=\"attr\">      matchLabels:</span></span><br><span class=\"line\"><span class=\"attr\">        user:</span> <span class=\"string\">alice</span></span><br><span class=\"line\"><span class=\"attr\">    podSelector:</span></span><br><span class=\"line\"><span class=\"attr\">      matchLabels:</span></span><br><span class=\"line\"><span class=\"attr\">        role:</span> <span class=\"string\">client</span></span><br></pre></td></tr></table></figure>\n<p>像上面这样定义的 namespaceSelector 和 podSelector，是“与”（AND）的关系，yaml里表示的是一个数组元素的两个字段</p>\n<p>Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。</p>\n<h2 id=\"通过NodePort来访问service的话，client的源ip会被做SNAT\"><a href=\"#通过NodePort来访问service的话，client的源ip会被做SNAT\" class=\"headerlink\" title=\"通过NodePort来访问service的话，client的源ip会被做SNAT\"></a>通过NodePort来访问service的话，client的源ip会被做SNAT</h2><figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\">         <span class=\"string\">client</span></span><br><span class=\"line\">            <span class=\"string\">\\</span> <span class=\"string\">^</span></span><br><span class=\"line\">             <span class=\"string\">\\</span> <span class=\"string\">\\</span></span><br><span class=\"line\">              <span class=\"string\">v</span> <span class=\"string\">\\</span></span><br><span class=\"line\">  <span class=\"string\">node</span> <span class=\"number\">1</span> <span class=\"string\">&lt;---</span> <span class=\"string\">node</span> <span class=\"number\">2</span></span><br><span class=\"line\">   <span class=\"string\">| ^   SNAT</span></span><br><span class=\"line\"><span class=\"string\">   | |   ---&gt;</span></span><br><span class=\"line\"><span class=\"string\">   v |</span></span><br><span class=\"line\"><span class=\"string\">endpoint</span></span><br></pre></td></tr></table></figure>\n<p>流程：</p>\n<ul>\n<li>客户端发送数据包到 node2:nodePort</li>\n<li>node2 使用它自己的 IP 地址替换数据包的源 IP 地址（SNAT）</li>\n<li>node2 使用 pod IP 地址替换数据包的目的 IP 地址</li>\n<li>数据包被路由到 node 1，然后交给 endpoint</li>\n<li>Pod 的回复被路由回 node2</li>\n<li>Pod 的回复被发送回给客户端</li>\n</ul>\n<p>可以将 <code>service.spec.externalTrafficPolicy</code> 的值为 Local，请求就只会被代理到本地 endpoints 而不会被转发到其它节点。这样就保留了最初的源 IP 地址 <strong>不会对访问NodePort的client ip做 SNAT了</strong>。如果没有本地 endpoints，发送到这个节点的数据包将会被丢弃。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"容器网络\"><a href=\"#容器网络\" class=\"headerlink\" title=\"容器网络\"></a>容器网络</h1><h2 id=\"vxlan\"><a href=\"#vxlan\" class=\"headerlink\" title=\"vxlan\"></a>vxlan</h2><p>vxlan原理: VXLAN通过MAC-in-UDP的报文封装，实现了二层报文在三层网络上的透传,属于overlay网络</p>\n<h2 id=\"Flannel\"><a href=\"#Flannel\" class=\"headerlink\" title=\"Flannel\"></a>Flannel</h2><p>首先，flannel利用<strong><code>Kubernetes-API(这里就是取node.spec.podCIDR)或者etcd</code></strong>用于存储整个集群的网络配置，其中最主要的内容为设置集群的网络地址空间。例如，设定整个集群内所有容器的IP都取自网段“10.1.0.0/16”。</p>\n<p>接着，flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配。</p>\n<p><code>flannel</code> 的 <code>UDP</code> 模式和 <code>Vxlan</code> 模式 <code>host-gw</code> 模式</p>\n<ul>\n<li><code>UDP</code> 模式是 三层 <code>overlay</code>,即，将原始数据包的三层包（IP包）装在 <code>UDP</code> 包里,通过 ip+端口 传到目的地，ip为目标node ip 端口为目标节点上flanneld进程监听的8285端口，解析后传入flannel0设备进入内核网络协议栈，<br>UDP模式下 封包解包是在 flanneld里进行的也就是用户态下</li>\n</ul>\n<p><img src=\"https://static001.geekbang.org/resource/image/84/8d/84caa6dc3f9dcdf8b88b56bd2e22138d.png\" alt=\"\"></p>\n<p><img src=\"https://static001.geekbang.org/resource/image/e6/f0/e6827cecb75641d3c8838f2213543cf0.png\" alt=\"\"></p>\n<p><strong>重要！！！ 《深入解析kubernetes》</strong> 33章  <a href=\"https://time.geekbang.org/column/article/65287\" target=\"_blank\" rel=\"noopener\">https://time.geekbang.org/column/article/65287</a></p>\n<ul>\n<li>VxLan 模式 是二层 <code>overlay</code>,即将原始Ethernet包（MAC包）封装起来，通过vtep设备发到目的vtep，vxlan是内核模块，vtep是flannneld创建的，vxlan封包解封完全是在内核态完成的</li>\n<li></li>\n</ul>\n<p><img src=\"https://static001.geekbang.org/resource/image/43/41/43f5ebb001145ecd896fd10fb27c5c41.png\" alt=\"\"></p>\n<ul>\n<li>注意点 <ul>\n<li>inner mac 为 目的vtep的mac</li>\n<li>outer ip为目的node的ip <strong>这一点和UDP有区别</strong><br>下一跳ip对应的mac地址是ARP表里记录的，inner mac对应的arp记录是 flanneld维护的，outer mac arp表是node自学习的</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://static001.geekbang.org/resource/image/ce/38/cefe6b99422fba768c53f0093947cd38.png\" alt=\"\"></p>\n<ul>\n<li><code>host-gw</code> 模式的工作原理,是在 节点上加路由表，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。<br>这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> ip route</span></span><br><span class=\"line\">...</span><br><span class=\"line\">&lt;目的容器IP地址段&gt; via &lt;网关的IP地址&gt; dev eth0# 网关的 IP 地址，正是目的容器所在宿主机的 IP 地址</span><br></pre></td></tr></table></figure>\n<p><strong>Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。如果分布在不同的子网里是不行的，只是三层可达</strong></p>\n<h3 id=\"POD-IP的分配\"><a href=\"#POD-IP的分配\" class=\"headerlink\" title=\"POD IP的分配\"></a>POD IP的分配</h3><h4 id=\"使用CNI后，即配置了-kubelet-的-network-plugin-cni，容器的IP分配：\"><a href=\"#使用CNI后，即配置了-kubelet-的-network-plugin-cni，容器的IP分配：\" class=\"headerlink\" title=\"使用CNI后，即配置了 kubelet 的 --network-plugin=cni，容器的IP分配：\"></a>使用CNI后，即配置了 <code>kubelet</code> 的 <code>--network-plugin=cni</code>，容器的IP分配：</h4><p>kubelet 先创建pause容器生成network namespace<br>调用 网络driver CNI driver<br>CNI driver 根据配置调用具体的cni 插件<br>cni 插件给pause 容器配置网络<br>pod 中其他的容器都使用 pause 容器的网络</p>\n<h4 id=\"CNM模式\"><a href=\"#CNM模式\" class=\"headerlink\" title=\"CNM模式\"></a>CNM模式</h4><p>Pod IP是docker engine分配的，Pod也是以docker0为网关，通过veth连接network namespace</p>\n<h4 id=\"flannel的两种方式-CNI-CNM总结\"><a href=\"#flannel的两种方式-CNI-CNM总结\" class=\"headerlink\" title=\"flannel的两种方式 CNI CNM总结\"></a>flannel的两种方式 CNI CNM总结</h4><p>CNI中，docker0的ip与Pod无关，Pod总是生成的时候才去动态的申请自己的IP，而CNM模式下，Pod的网段在docker engine启动时就已经决定。<br>CNI只是一个网络接口规范，各种功能都由插件实现，flannel只是插件的一种，而且docker也只是容器载体的一种选择，Kubernetes还可以使用其他的，</p>\n<h2 id=\"cluster-IP的分配\"><a href=\"#cluster-IP的分配\" class=\"headerlink\" title=\"cluster IP的分配\"></a>cluster IP的分配</h2><pre><code>是在kube-apiserver中 `pkg/registry/core/service/ipallocator`中分配的\n</code></pre><h2 id=\"network-policy\"><a href=\"#network-policy\" class=\"headerlink\" title=\"network policy\"></a>network policy</h2><figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">ingress:</span></span><br><span class=\"line\"><span class=\"attr\">- from:</span></span><br><span class=\"line\"><span class=\"attr\">  - namespaceSelector:</span></span><br><span class=\"line\"><span class=\"attr\">      matchLabels:</span></span><br><span class=\"line\"><span class=\"attr\">        user:</span> <span class=\"string\">alice</span></span><br><span class=\"line\"><span class=\"attr\">  - podSelector:</span></span><br><span class=\"line\"><span class=\"attr\">      matchLabels:</span></span><br><span class=\"line\"><span class=\"attr\">        role:</span> <span class=\"string\">client</span></span><br></pre></td></tr></table></figure>\n<p>像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系，表示的是yaml数组里的两个元素</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">ingress:</span></span><br><span class=\"line\"><span class=\"attr\">- from:</span></span><br><span class=\"line\"><span class=\"attr\">  - namespaceSelector:</span></span><br><span class=\"line\"><span class=\"attr\">      matchLabels:</span></span><br><span class=\"line\"><span class=\"attr\">        user:</span> <span class=\"string\">alice</span></span><br><span class=\"line\"><span class=\"attr\">    podSelector:</span></span><br><span class=\"line\"><span class=\"attr\">      matchLabels:</span></span><br><span class=\"line\"><span class=\"attr\">        role:</span> <span class=\"string\">client</span></span><br></pre></td></tr></table></figure>\n<p>像上面这样定义的 namespaceSelector 和 podSelector，是“与”（AND）的关系，yaml里表示的是一个数组元素的两个字段</p>\n<p>Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。</p>\n<h2 id=\"通过NodePort来访问service的话，client的源ip会被做SNAT\"><a href=\"#通过NodePort来访问service的话，client的源ip会被做SNAT\" class=\"headerlink\" title=\"通过NodePort来访问service的话，client的源ip会被做SNAT\"></a>通过NodePort来访问service的话，client的源ip会被做SNAT</h2><figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\">         <span class=\"string\">client</span></span><br><span class=\"line\">            <span class=\"string\">\\</span> <span class=\"string\">^</span></span><br><span class=\"line\">             <span class=\"string\">\\</span> <span class=\"string\">\\</span></span><br><span class=\"line\">              <span class=\"string\">v</span> <span class=\"string\">\\</span></span><br><span class=\"line\">  <span class=\"string\">node</span> <span class=\"number\">1</span> <span class=\"string\">&lt;---</span> <span class=\"string\">node</span> <span class=\"number\">2</span></span><br><span class=\"line\">   <span class=\"string\">| ^   SNAT</span></span><br><span class=\"line\"><span class=\"string\">   | |   ---&gt;</span></span><br><span class=\"line\"><span class=\"string\">   v |</span></span><br><span class=\"line\"><span class=\"string\">endpoint</span></span><br></pre></td></tr></table></figure>\n<p>流程：</p>\n<ul>\n<li>客户端发送数据包到 node2:nodePort</li>\n<li>node2 使用它自己的 IP 地址替换数据包的源 IP 地址（SNAT）</li>\n<li>node2 使用 pod IP 地址替换数据包的目的 IP 地址</li>\n<li>数据包被路由到 node 1，然后交给 endpoint</li>\n<li>Pod 的回复被路由回 node2</li>\n<li>Pod 的回复被发送回给客户端</li>\n</ul>\n<p>可以将 <code>service.spec.externalTrafficPolicy</code> 的值为 Local，请求就只会被代理到本地 endpoints 而不会被转发到其它节点。这样就保留了最初的源 IP 地址 <strong>不会对访问NodePort的client ip做 SNAT了</strong>。如果没有本地 endpoints，发送到这个节点的数据包将会被丢弃。</p>\n"},{"title":"Go 学习笔记","date":"2019-01-05T06:19:10.000Z","author":"zhaojizhuang","mathjax":true,"_content":"\n\n# Go 学习笔记\n\n\n## go程序是如何运行的\n\n[参考链接1](https://juejin.im/post/5d1c087af265da1bb5651356)\n\n## defer 源码分析\n\n[参考链接](https://eddycjy.com/posts/go/defer/2019-05-27-defer/)\n\ndefer、return、返回值三者的执行逻辑应该是：return最先执行，return负责将结果写入返回值中；接着defer开始执行一些收尾工作；最后函数携带当前返回值退出\n\n## 逃逸分析 堆栈分配\n\n[参考链接](https://eddycjy.com/posts/go/talk/2019-05-20-stack-heap/)\n\n`go build -gcflags '-m -l' xxx.go` 就可以看到逃逸分析的过程和结果\n\n## go性能大杀器 pprof\n\n[参考链接1](https://zhuanlan.zhihu.com/p/71529062)\n[参考链接2](https://github.com/eddycjy/blog/blob/master/content/posts/go/tools/2018-09-15-go-tool-pprof.md)\n\n### \n\n- pprof中自带 web 火焰图，需要安装graphviz\n`go tool pprof -http=:8181 xxx,pprof`\n\n- 下面的语句 可以**结合代码查看哪个函数**用时最多\n`go tool pprof main.go xxxx.prof  进入pprof后执行 list  <函数名> `\n\n### 对于web开放的pprof （在http的go程序中 添加 `_ \"net/http/pprof\"`的import,会增加 debug/pprof 的endpoint),结束后将默认进入 pprof 的交互式命令模式\n\n    ```shell\n    go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60\n    go tool pprof http://localhost:6060/debug/pprof/heap\n    ```\n    \n## go性能大杀器 trace\n\n同pprof\n\n- 对于web开放的pprof （在http的go程序中 添加 `_ \"net/http/pprof\"`的import\n\n```shell    \ncurl http://127.0.0.1:6060/debug/pprof/trace\\?seconds\\=20 > trace.out\ngo tool trace trace.out # 此处和pprof不同，不用加 -http=:8181 这里他会自动选择端口\n```\n\n- 对于后台应用,后台程序main启动时添加 trace.Start(os.Stderr)直接运行下面的命令即可 \n\n`go run main.go 2> trace.out`\n\n它能够跟踪捕获各种执行中的事件，例如 Goroutine 的创建/阻塞/解除阻塞，Syscall 的进入/退出/阻止，GC 事件，Heap 的大小改变，Processor 启动/停止等等\n\n\n## interface \n\n1. 不含有任何方法的 `interface`\n```go\ntype eface struct { // 16 bytes\n\t_type *_type\n\tdata  unsafe.Pointer\n}\n```\n2. 含有 方法的 `interface`\n\n```go\ntype iface struct { // 16 bytes\n\ttab  *itab\n\tdata unsafe.Pointer\n}\n```\n\t\t\n\n\n|变量类型|结构体实现接口|结构体指针实现接口|\n|----|----|----|\n|结构体初始化变量|\t通过|\t**不通过**|\n|结构体指针初始化变量|\t通过|\t通过|\n\n不通过的如下\n\n```go\ntype Duck interface {\n\tQuack()\n}\n\ntype Cat struct{}\n\nfunc (c *Cat) Quack() {\n\tfmt.Println(\"meow\")\n}\n\nfunc main() {\n\tvar c Duck = Cat{}  // 将结构体变量传到指针类型接受的函数是不行的，反过来可行\n\tc.Quack()\n}\n\n$ go build interface.go\n./interface.go:20:6: cannot use Cat literal (type Cat) as type Duck in assignment:\n\tCat does not implement Duck (Quack method has pointer receiver)\n```\n\nGo中函数调用都是值拷贝，使用 c.Quack() 调用方法时都会发生**值拷贝**：\n\n- 对于 &Cat{} 来说，这意味着拷贝一个新的 &Cat{} 指针，这个指针与原来的指针指向一个相同并且唯一的结构体，所以编译器可以隐式的对变量解引用（dereference）获取指针指向的结构体；\n- 对于 Cat{} 来说，这意味着 Quack 方法会接受一个全新的 Cat{}，因为方法的参数是*Cat，编译器不会无中生有创建一个新的指针；即使编译器可以创建新指针，这个指针指向的也不是最初调用该方法的结构体；\n\n## panic\n\npanic只会调用当前Goroutine的defer（）\n\n```go\nfunc main() {\n\tdefer println(\"in main\")\n\tgo func() {\n\t\tdefer println(\"in goroutine\")\n\t\tpanic(\"\")\n\t}()\n\n\ttime.Sleep(1 * time.Second)\n}\n\n$ go run main.go\nin goroutine\npanic:\n```\n\n![](https://img.draveness.me/2020-01-19-15794253176199-golang-panic-and-defers.png)\n\n## make  new\n\nnew 返回的是指针，指向一个type类型内存空间的指针\nnew等价于   \n\n```go\nvar  a  typeA  // tpyeA的零值\nb:=&a\n\n```\n但是 new不能对 chanel map  slice进行初始化 ，这几个必须经过make进行结构体的初始化才能用\n\n\n","source":"_posts/go shen ru fen xi.md","raw":"---\ntitle:  \"Go 学习笔记\"\ndate:   2019-01-05 14:19:10 +0800\ncategories: Go\ntags:  [\"Go\",\"epoll\",\"linux\"]\nauthor: zhaojizhuang\nmathjax: true\n---\n\n\n# Go 学习笔记\n\n\n## go程序是如何运行的\n\n[参考链接1](https://juejin.im/post/5d1c087af265da1bb5651356)\n\n## defer 源码分析\n\n[参考链接](https://eddycjy.com/posts/go/defer/2019-05-27-defer/)\n\ndefer、return、返回值三者的执行逻辑应该是：return最先执行，return负责将结果写入返回值中；接着defer开始执行一些收尾工作；最后函数携带当前返回值退出\n\n## 逃逸分析 堆栈分配\n\n[参考链接](https://eddycjy.com/posts/go/talk/2019-05-20-stack-heap/)\n\n`go build -gcflags '-m -l' xxx.go` 就可以看到逃逸分析的过程和结果\n\n## go性能大杀器 pprof\n\n[参考链接1](https://zhuanlan.zhihu.com/p/71529062)\n[参考链接2](https://github.com/eddycjy/blog/blob/master/content/posts/go/tools/2018-09-15-go-tool-pprof.md)\n\n### \n\n- pprof中自带 web 火焰图，需要安装graphviz\n`go tool pprof -http=:8181 xxx,pprof`\n\n- 下面的语句 可以**结合代码查看哪个函数**用时最多\n`go tool pprof main.go xxxx.prof  进入pprof后执行 list  <函数名> `\n\n### 对于web开放的pprof （在http的go程序中 添加 `_ \"net/http/pprof\"`的import,会增加 debug/pprof 的endpoint),结束后将默认进入 pprof 的交互式命令模式\n\n    ```shell\n    go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60\n    go tool pprof http://localhost:6060/debug/pprof/heap\n    ```\n    \n## go性能大杀器 trace\n\n同pprof\n\n- 对于web开放的pprof （在http的go程序中 添加 `_ \"net/http/pprof\"`的import\n\n```shell    \ncurl http://127.0.0.1:6060/debug/pprof/trace\\?seconds\\=20 > trace.out\ngo tool trace trace.out # 此处和pprof不同，不用加 -http=:8181 这里他会自动选择端口\n```\n\n- 对于后台应用,后台程序main启动时添加 trace.Start(os.Stderr)直接运行下面的命令即可 \n\n`go run main.go 2> trace.out`\n\n它能够跟踪捕获各种执行中的事件，例如 Goroutine 的创建/阻塞/解除阻塞，Syscall 的进入/退出/阻止，GC 事件，Heap 的大小改变，Processor 启动/停止等等\n\n\n## interface \n\n1. 不含有任何方法的 `interface`\n```go\ntype eface struct { // 16 bytes\n\t_type *_type\n\tdata  unsafe.Pointer\n}\n```\n2. 含有 方法的 `interface`\n\n```go\ntype iface struct { // 16 bytes\n\ttab  *itab\n\tdata unsafe.Pointer\n}\n```\n\t\t\n\n\n|变量类型|结构体实现接口|结构体指针实现接口|\n|----|----|----|\n|结构体初始化变量|\t通过|\t**不通过**|\n|结构体指针初始化变量|\t通过|\t通过|\n\n不通过的如下\n\n```go\ntype Duck interface {\n\tQuack()\n}\n\ntype Cat struct{}\n\nfunc (c *Cat) Quack() {\n\tfmt.Println(\"meow\")\n}\n\nfunc main() {\n\tvar c Duck = Cat{}  // 将结构体变量传到指针类型接受的函数是不行的，反过来可行\n\tc.Quack()\n}\n\n$ go build interface.go\n./interface.go:20:6: cannot use Cat literal (type Cat) as type Duck in assignment:\n\tCat does not implement Duck (Quack method has pointer receiver)\n```\n\nGo中函数调用都是值拷贝，使用 c.Quack() 调用方法时都会发生**值拷贝**：\n\n- 对于 &Cat{} 来说，这意味着拷贝一个新的 &Cat{} 指针，这个指针与原来的指针指向一个相同并且唯一的结构体，所以编译器可以隐式的对变量解引用（dereference）获取指针指向的结构体；\n- 对于 Cat{} 来说，这意味着 Quack 方法会接受一个全新的 Cat{}，因为方法的参数是*Cat，编译器不会无中生有创建一个新的指针；即使编译器可以创建新指针，这个指针指向的也不是最初调用该方法的结构体；\n\n## panic\n\npanic只会调用当前Goroutine的defer（）\n\n```go\nfunc main() {\n\tdefer println(\"in main\")\n\tgo func() {\n\t\tdefer println(\"in goroutine\")\n\t\tpanic(\"\")\n\t}()\n\n\ttime.Sleep(1 * time.Second)\n}\n\n$ go run main.go\nin goroutine\npanic:\n```\n\n![](https://img.draveness.me/2020-01-19-15794253176199-golang-panic-and-defers.png)\n\n## make  new\n\nnew 返回的是指针，指向一个type类型内存空间的指针\nnew等价于   \n\n```go\nvar  a  typeA  // tpyeA的零值\nb:=&a\n\n```\n但是 new不能对 chanel map  slice进行初始化 ，这几个必须经过make进行结构体的初始化才能用\n\n\n","slug":"go shen ru fen xi","published":1,"updated":"2020-05-20T12:54:48.547Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcnd4l8b000br5flo8nomy9e","content":"<h1 id=\"Go-学习笔记\"><a href=\"#Go-学习笔记\" class=\"headerlink\" title=\"Go 学习笔记\"></a>Go 学习笔记</h1><h2 id=\"go程序是如何运行的\"><a href=\"#go程序是如何运行的\" class=\"headerlink\" title=\"go程序是如何运行的\"></a>go程序是如何运行的</h2><p><a href=\"https://juejin.im/post/5d1c087af265da1bb5651356\" target=\"_blank\" rel=\"noopener\">参考链接1</a></p>\n<h2 id=\"defer-源码分析\"><a href=\"#defer-源码分析\" class=\"headerlink\" title=\"defer 源码分析\"></a>defer 源码分析</h2><p><a href=\"https://eddycjy.com/posts/go/defer/2019-05-27-defer/\" target=\"_blank\" rel=\"noopener\">参考链接</a></p>\n<p>defer、return、返回值三者的执行逻辑应该是：return最先执行，return负责将结果写入返回值中；接着defer开始执行一些收尾工作；最后函数携带当前返回值退出</p>\n<h2 id=\"逃逸分析-堆栈分配\"><a href=\"#逃逸分析-堆栈分配\" class=\"headerlink\" title=\"逃逸分析 堆栈分配\"></a>逃逸分析 堆栈分配</h2><p><a href=\"https://eddycjy.com/posts/go/talk/2019-05-20-stack-heap/\" target=\"_blank\" rel=\"noopener\">参考链接</a></p>\n<p><code>go build -gcflags &#39;-m -l&#39; xxx.go</code> 就可以看到逃逸分析的过程和结果</p>\n<h2 id=\"go性能大杀器-pprof\"><a href=\"#go性能大杀器-pprof\" class=\"headerlink\" title=\"go性能大杀器 pprof\"></a>go性能大杀器 pprof</h2><p><a href=\"https://zhuanlan.zhihu.com/p/71529062\" target=\"_blank\" rel=\"noopener\">参考链接1</a><br><a href=\"https://github.com/eddycjy/blog/blob/master/content/posts/go/tools/2018-09-15-go-tool-pprof.md\" target=\"_blank\" rel=\"noopener\">参考链接2</a></p>\n<p>### </p>\n<ul>\n<li><p>pprof中自带 web 火焰图，需要安装graphviz<br><code>go tool pprof -http=:8181 xxx,pprof</code></p>\n</li>\n<li><p>下面的语句 可以<strong>结合代码查看哪个函数</strong>用时最多<br><code>go tool pprof main.go xxxx.prof  进入pprof后执行 list  &lt;函数名&gt;</code></p>\n</li>\n</ul>\n<h3 id=\"对于web开放的pprof-（在http的go程序中-添加-quot-net-http-pprof-quot-的import-会增加-debug-pprof-的endpoint-结束后将默认进入-pprof-的交互式命令模式\"><a href=\"#对于web开放的pprof-（在http的go程序中-添加-quot-net-http-pprof-quot-的import-会增加-debug-pprof-的endpoint-结束后将默认进入-pprof-的交互式命令模式\" class=\"headerlink\" title=\"对于web开放的pprof （在http的go程序中 添加 _ &quot;net/http/pprof&quot;的import,会增加 debug/pprof 的endpoint),结束后将默认进入 pprof 的交互式命令模式\"></a>对于web开放的pprof （在http的go程序中 添加 <code>_ &quot;net/http/pprof&quot;</code>的import,会增加 debug/pprof 的endpoint),结束后将默认进入 pprof 的交互式命令模式</h3><pre><code><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60</span><br><span class=\"line\">go tool pprof http://localhost:6060/debug/pprof/heap</span><br></pre></td></tr></table></figure>\n</code></pre><h2 id=\"go性能大杀器-trace\"><a href=\"#go性能大杀器-trace\" class=\"headerlink\" title=\"go性能大杀器 trace\"></a>go性能大杀器 trace</h2><p>同pprof</p>\n<ul>\n<li>对于web开放的pprof （在http的go程序中 添加 <code>_ &quot;net/http/pprof&quot;</code>的import</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">curl http://127.0.0.1:6060/debug/pprof/trace\\?seconds\\=20 &gt; trace.out</span><br><span class=\"line\">go tool trace trace.out # 此处和pprof不同，不用加 -http=:8181 这里他会自动选择端口</span><br></pre></td></tr></table></figure>\n<ul>\n<li>对于后台应用,后台程序main启动时添加 trace.Start(os.Stderr)直接运行下面的命令即可 </li>\n</ul>\n<p><code>go run main.go 2&gt; trace.out</code></p>\n<p>它能够跟踪捕获各种执行中的事件，例如 Goroutine 的创建/阻塞/解除阻塞，Syscall 的进入/退出/阻止，GC 事件，Heap 的大小改变，Processor 启动/停止等等</p>\n<h2 id=\"interface\"><a href=\"#interface\" class=\"headerlink\" title=\"interface\"></a>interface</h2><ol>\n<li><p>不含有任何方法的 <code>interface</code></p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> eface <span class=\"keyword\">struct</span> &#123; <span class=\"comment\">// 16 bytes</span></span><br><span class=\"line\">\t_type *_type</span><br><span class=\"line\">\tdata  unsafe.Pointer</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>含有 方法的 <code>interface</code></p>\n</li>\n</ol>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> iface <span class=\"keyword\">struct</span> &#123; <span class=\"comment\">// 16 bytes</span></span><br><span class=\"line\">\ttab  *itab</span><br><span class=\"line\">\tdata unsafe.Pointer</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>变量类型</th>\n<th>结构体实现接口</th>\n<th>结构体指针实现接口</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>结构体初始化变量</td>\n<td>通过</td>\n<td><strong>不通过</strong></td>\n</tr>\n<tr>\n<td>结构体指针初始化变量</td>\n<td>通过</td>\n<td>通过</td>\n</tr>\n</tbody>\n</table>\n<p>不通过的如下</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> Duck <span class=\"keyword\">interface</span> &#123;</span><br><span class=\"line\">\tQuack()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">type</span> Cat <span class=\"keyword\">struct</span>&#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(c *Cat)</span> <span class=\"title\">Quack</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\tfmt.Println(<span class=\"string\">\"meow\"</span>)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> c Duck = Cat&#123;&#125;  <span class=\"comment\">// 将结构体变量传到指针类型接受的函数是不行的，反过来可行</span></span><br><span class=\"line\">\tc.Quack()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ <span class=\"keyword\">go</span> build <span class=\"keyword\">interface</span>.<span class=\"keyword\">go</span></span><br><span class=\"line\">./<span class=\"keyword\">interface</span>.<span class=\"keyword\">go</span>:<span class=\"number\">20</span>:<span class=\"number\">6</span>: cannot use Cat literal (<span class=\"keyword\">type</span> Cat) as <span class=\"keyword\">type</span> Duck in assignment:</span><br><span class=\"line\">\tCat does not implement Duck (Quack method has pointer receiver)</span><br></pre></td></tr></table></figure>\n<p>Go中函数调用都是值拷贝，使用 c.Quack() 调用方法时都会发生<strong>值拷贝</strong>：</p>\n<ul>\n<li>对于 &amp;Cat{} 来说，这意味着拷贝一个新的 &amp;Cat{} 指针，这个指针与原来的指针指向一个相同并且唯一的结构体，所以编译器可以隐式的对变量解引用（dereference）获取指针指向的结构体；</li>\n<li>对于 Cat{} 来说，这意味着 Quack 方法会接受一个全新的 Cat{}，因为方法的参数是*Cat，编译器不会无中生有创建一个新的指针；即使编译器可以创建新指针，这个指针指向的也不是最初调用该方法的结构体；</li>\n</ul>\n<h2 id=\"panic\"><a href=\"#panic\" class=\"headerlink\" title=\"panic\"></a>panic</h2><p>panic只会调用当前Goroutine的defer（）</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">defer</span> <span class=\"built_in\">println</span>(<span class=\"string\">\"in main\"</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">go</span> <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">defer</span> <span class=\"built_in\">println</span>(<span class=\"string\">\"in goroutine\"</span>)</span><br><span class=\"line\">\t\t<span class=\"built_in\">panic</span>(<span class=\"string\">\"\"</span>)</span><br><span class=\"line\">\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\ttime.Sleep(<span class=\"number\">1</span> * time.Second)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ <span class=\"keyword\">go</span> run main.<span class=\"keyword\">go</span></span><br><span class=\"line\">in goroutine</span><br><span class=\"line\"><span class=\"built_in\">panic</span>:</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img.draveness.me/2020-01-19-15794253176199-golang-panic-and-defers.png\" alt=\"\"></p>\n<h2 id=\"make-new\"><a href=\"#make-new\" class=\"headerlink\" title=\"make  new\"></a>make  new</h2><p>new 返回的是指针，指向一个type类型内存空间的指针<br>new等价于   </p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span>  a  typeA  <span class=\"comment\">// tpyeA的零值</span></span><br><span class=\"line\">b:=&amp;a</span><br></pre></td></tr></table></figure>\n<p>但是 new不能对 chanel map  slice进行初始化 ，这几个必须经过make进行结构体的初始化才能用</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Go-学习笔记\"><a href=\"#Go-学习笔记\" class=\"headerlink\" title=\"Go 学习笔记\"></a>Go 学习笔记</h1><h2 id=\"go程序是如何运行的\"><a href=\"#go程序是如何运行的\" class=\"headerlink\" title=\"go程序是如何运行的\"></a>go程序是如何运行的</h2><p><a href=\"https://juejin.im/post/5d1c087af265da1bb5651356\" target=\"_blank\" rel=\"noopener\">参考链接1</a></p>\n<h2 id=\"defer-源码分析\"><a href=\"#defer-源码分析\" class=\"headerlink\" title=\"defer 源码分析\"></a>defer 源码分析</h2><p><a href=\"https://eddycjy.com/posts/go/defer/2019-05-27-defer/\" target=\"_blank\" rel=\"noopener\">参考链接</a></p>\n<p>defer、return、返回值三者的执行逻辑应该是：return最先执行，return负责将结果写入返回值中；接着defer开始执行一些收尾工作；最后函数携带当前返回值退出</p>\n<h2 id=\"逃逸分析-堆栈分配\"><a href=\"#逃逸分析-堆栈分配\" class=\"headerlink\" title=\"逃逸分析 堆栈分配\"></a>逃逸分析 堆栈分配</h2><p><a href=\"https://eddycjy.com/posts/go/talk/2019-05-20-stack-heap/\" target=\"_blank\" rel=\"noopener\">参考链接</a></p>\n<p><code>go build -gcflags &#39;-m -l&#39; xxx.go</code> 就可以看到逃逸分析的过程和结果</p>\n<h2 id=\"go性能大杀器-pprof\"><a href=\"#go性能大杀器-pprof\" class=\"headerlink\" title=\"go性能大杀器 pprof\"></a>go性能大杀器 pprof</h2><p><a href=\"https://zhuanlan.zhihu.com/p/71529062\" target=\"_blank\" rel=\"noopener\">参考链接1</a><br><a href=\"https://github.com/eddycjy/blog/blob/master/content/posts/go/tools/2018-09-15-go-tool-pprof.md\" target=\"_blank\" rel=\"noopener\">参考链接2</a></p>\n<p>### </p>\n<ul>\n<li><p>pprof中自带 web 火焰图，需要安装graphviz<br><code>go tool pprof -http=:8181 xxx,pprof</code></p>\n</li>\n<li><p>下面的语句 可以<strong>结合代码查看哪个函数</strong>用时最多<br><code>go tool pprof main.go xxxx.prof  进入pprof后执行 list  &lt;函数名&gt;</code></p>\n</li>\n</ul>\n<h3 id=\"对于web开放的pprof-（在http的go程序中-添加-quot-net-http-pprof-quot-的import-会增加-debug-pprof-的endpoint-结束后将默认进入-pprof-的交互式命令模式\"><a href=\"#对于web开放的pprof-（在http的go程序中-添加-quot-net-http-pprof-quot-的import-会增加-debug-pprof-的endpoint-结束后将默认进入-pprof-的交互式命令模式\" class=\"headerlink\" title=\"对于web开放的pprof （在http的go程序中 添加 _ &quot;net/http/pprof&quot;的import,会增加 debug/pprof 的endpoint),结束后将默认进入 pprof 的交互式命令模式\"></a>对于web开放的pprof （在http的go程序中 添加 <code>_ &quot;net/http/pprof&quot;</code>的import,会增加 debug/pprof 的endpoint),结束后将默认进入 pprof 的交互式命令模式</h3><pre><code><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60</span><br><span class=\"line\">go tool pprof http://localhost:6060/debug/pprof/heap</span><br></pre></td></tr></table></figure>\n</code></pre><h2 id=\"go性能大杀器-trace\"><a href=\"#go性能大杀器-trace\" class=\"headerlink\" title=\"go性能大杀器 trace\"></a>go性能大杀器 trace</h2><p>同pprof</p>\n<ul>\n<li>对于web开放的pprof （在http的go程序中 添加 <code>_ &quot;net/http/pprof&quot;</code>的import</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">curl http://127.0.0.1:6060/debug/pprof/trace\\?seconds\\=20 &gt; trace.out</span><br><span class=\"line\">go tool trace trace.out # 此处和pprof不同，不用加 -http=:8181 这里他会自动选择端口</span><br></pre></td></tr></table></figure>\n<ul>\n<li>对于后台应用,后台程序main启动时添加 trace.Start(os.Stderr)直接运行下面的命令即可 </li>\n</ul>\n<p><code>go run main.go 2&gt; trace.out</code></p>\n<p>它能够跟踪捕获各种执行中的事件，例如 Goroutine 的创建/阻塞/解除阻塞，Syscall 的进入/退出/阻止，GC 事件，Heap 的大小改变，Processor 启动/停止等等</p>\n<h2 id=\"interface\"><a href=\"#interface\" class=\"headerlink\" title=\"interface\"></a>interface</h2><ol>\n<li><p>不含有任何方法的 <code>interface</code></p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> eface <span class=\"keyword\">struct</span> &#123; <span class=\"comment\">// 16 bytes</span></span><br><span class=\"line\">\t_type *_type</span><br><span class=\"line\">\tdata  unsafe.Pointer</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>含有 方法的 <code>interface</code></p>\n</li>\n</ol>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> iface <span class=\"keyword\">struct</span> &#123; <span class=\"comment\">// 16 bytes</span></span><br><span class=\"line\">\ttab  *itab</span><br><span class=\"line\">\tdata unsafe.Pointer</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>变量类型</th>\n<th>结构体实现接口</th>\n<th>结构体指针实现接口</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>结构体初始化变量</td>\n<td>通过</td>\n<td><strong>不通过</strong></td>\n</tr>\n<tr>\n<td>结构体指针初始化变量</td>\n<td>通过</td>\n<td>通过</td>\n</tr>\n</tbody>\n</table>\n<p>不通过的如下</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> Duck <span class=\"keyword\">interface</span> &#123;</span><br><span class=\"line\">\tQuack()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">type</span> Cat <span class=\"keyword\">struct</span>&#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(c *Cat)</span> <span class=\"title\">Quack</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\tfmt.Println(<span class=\"string\">\"meow\"</span>)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">var</span> c Duck = Cat&#123;&#125;  <span class=\"comment\">// 将结构体变量传到指针类型接受的函数是不行的，反过来可行</span></span><br><span class=\"line\">\tc.Quack()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ <span class=\"keyword\">go</span> build <span class=\"keyword\">interface</span>.<span class=\"keyword\">go</span></span><br><span class=\"line\">./<span class=\"keyword\">interface</span>.<span class=\"keyword\">go</span>:<span class=\"number\">20</span>:<span class=\"number\">6</span>: cannot use Cat literal (<span class=\"keyword\">type</span> Cat) as <span class=\"keyword\">type</span> Duck in assignment:</span><br><span class=\"line\">\tCat does not implement Duck (Quack method has pointer receiver)</span><br></pre></td></tr></table></figure>\n<p>Go中函数调用都是值拷贝，使用 c.Quack() 调用方法时都会发生<strong>值拷贝</strong>：</p>\n<ul>\n<li>对于 &amp;Cat{} 来说，这意味着拷贝一个新的 &amp;Cat{} 指针，这个指针与原来的指针指向一个相同并且唯一的结构体，所以编译器可以隐式的对变量解引用（dereference）获取指针指向的结构体；</li>\n<li>对于 Cat{} 来说，这意味着 Quack 方法会接受一个全新的 Cat{}，因为方法的参数是*Cat，编译器不会无中生有创建一个新的指针；即使编译器可以创建新指针，这个指针指向的也不是最初调用该方法的结构体；</li>\n</ul>\n<h2 id=\"panic\"><a href=\"#panic\" class=\"headerlink\" title=\"panic\"></a>panic</h2><p>panic只会调用当前Goroutine的defer（）</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">defer</span> <span class=\"built_in\">println</span>(<span class=\"string\">\"in main\"</span>)</span><br><span class=\"line\">\t<span class=\"keyword\">go</span> <span class=\"function\"><span class=\"keyword\">func</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">defer</span> <span class=\"built_in\">println</span>(<span class=\"string\">\"in goroutine\"</span>)</span><br><span class=\"line\">\t\t<span class=\"built_in\">panic</span>(<span class=\"string\">\"\"</span>)</span><br><span class=\"line\">\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\ttime.Sleep(<span class=\"number\">1</span> * time.Second)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ <span class=\"keyword\">go</span> run main.<span class=\"keyword\">go</span></span><br><span class=\"line\">in goroutine</span><br><span class=\"line\"><span class=\"built_in\">panic</span>:</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://img.draveness.me/2020-01-19-15794253176199-golang-panic-and-defers.png\" alt=\"\"></p>\n<h2 id=\"make-new\"><a href=\"#make-new\" class=\"headerlink\" title=\"make  new\"></a>make  new</h2><p>new 返回的是指针，指向一个type类型内存空间的指针<br>new等价于   </p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span>  a  typeA  <span class=\"comment\">// tpyeA的零值</span></span><br><span class=\"line\">b:=&amp;a</span><br></pre></td></tr></table></figure>\n<p>但是 new不能对 chanel map  slice进行初始化 ，这几个必须经过make进行结构体的初始化才能用</p>\n"},{"title":"浅谈 Go map ","date":"2019-04-02T06:19:10.000Z","author":"zhaojizhuang","mathjax":true,"_content":"\n## map原理分析\n\n### map 结构体\n\n```go\ntype hmap struct {\n    count     int    // 元素的个数\n    flags     uint8  // 状态标志\n    B         uint8  // 可以最多容纳 6.5 * 2 ^ B 个元素，6.5为装载因子\n    noverflow uint16 // 溢出的个数\n    hash0     uint32 // 哈希种子\n\n    buckets    unsafe.Pointer // 桶的地址\n    oldbuckets unsafe.Pointer // 旧桶的地址，用于扩容\n    nevacuate  uintptr        // 搬迁进度，小于nevacuate的已经搬迁\n    overflow *[2]*[]*bmap \n}\n// A bucket for a Go map.\ntype bmap struct {\n    // 每个元素hash值的高8位，如果tophash[0] < minTopHash，表示这个桶的搬迁状态\n    tophash [bucketCnt]uint8\n    // bucketCnt是常量8,接下来是8个key、8个value，但是我们不能直接看到；为了优化对齐，go采用了key放在一起，value放在一起的存储方式，\n8个k，8个v得内存地址\n    // 再接下来是hash冲突发生时，下一个溢出桶的地址\n}\n```\nbmap不只tophash还有两个方法 overflow 和setoverflow\n\n```go\nfunc (b *bmap) overflow(t *maptype) *bmap {\n\treturn *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize))\n}\n\nfunc (b *bmap) setoverflow(t *maptype, ovf *bmap) {\n\t*(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize)) = ovf\n}\n```\n\nhmap中的buckets中的原色bucket就是bmap，即 buckets[0],bucket[1],... bucket[2^B-1]如下图\n\n![](/images/hmap.png)\n\n**bucket就是bmap**\n\nbmap 是存放 k-v 的地方，我们把视角拉近，仔细看 bmap 的内部组成。\n\n![](/images/bmap.png)\n\nkey 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到**最后 B 个 bit 位**。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32\n\n例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是：\n\n `10010111 | 000011110110110010001111001010100010010110010101010 │ 01010`\n \n用最后的 5 个 bit 位，也就是 `01010`，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。\n\n再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。\n\nbuckets 编号就是桶编号，当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key\n\n**hash冲突的两种表示方式**：\n\n- 开放寻址法（hash冲突时，在当前index往后查找第一个空的位置即可）\n- 拉链法\n\nmap在写入过程会发生扩容，`runtime.mapassign` **函数会在以下两种情况发生时触发哈希的扩容**：\n\n- 装载因子已经超过 6.5；装载因子=总数量/桶的数量\n- 哈希使用了太多溢出桶；溢出捅的数量 超过正常桶的数量 即 noverflow 大于 1<<B buckets\n\n**每次都会将桶的数量翻倍**\n\n\n### 扩容机制：\n- **翻倍扩容**：哈希在存储元素过多时状态会触发扩容操作，每次都会将桶的数量翻倍，整个扩容过程并不是原子的，而是通过 runtime.growWork 增量触发的，在扩容期间访问哈希表时会使用旧桶，向哈希表**写入数据时**会触发旧桶元素的分流；\n- **等量扩容**，**为了解决大量写入、删除造成的内存泄漏问题**，哈希引入了 sameSizeGrow这一机制，在出现较多溢出桶时会对哈希进行『内存整理』减少对空间的占用。\n\n### 参考链接 \n- [https://www.jianshu.com/p/aa0d4808cbb8](https://www.jianshu.com/p/aa0d4808cbb8)\n\n- [https://segmentfault.com/a/1190000018387055](https://segmentfault.com/a/1190000018387055)\n\n\n","source":"_posts/gomap.md","raw":"---\ntitle:  \"浅谈 Go map \"\ndate:   2019-04-02 14:19:10 +0800\ncategories: Go\ntags:  [\"Go\"]\nauthor: zhaojizhuang\nmathjax: true\n---\n\n## map原理分析\n\n### map 结构体\n\n```go\ntype hmap struct {\n    count     int    // 元素的个数\n    flags     uint8  // 状态标志\n    B         uint8  // 可以最多容纳 6.5 * 2 ^ B 个元素，6.5为装载因子\n    noverflow uint16 // 溢出的个数\n    hash0     uint32 // 哈希种子\n\n    buckets    unsafe.Pointer // 桶的地址\n    oldbuckets unsafe.Pointer // 旧桶的地址，用于扩容\n    nevacuate  uintptr        // 搬迁进度，小于nevacuate的已经搬迁\n    overflow *[2]*[]*bmap \n}\n// A bucket for a Go map.\ntype bmap struct {\n    // 每个元素hash值的高8位，如果tophash[0] < minTopHash，表示这个桶的搬迁状态\n    tophash [bucketCnt]uint8\n    // bucketCnt是常量8,接下来是8个key、8个value，但是我们不能直接看到；为了优化对齐，go采用了key放在一起，value放在一起的存储方式，\n8个k，8个v得内存地址\n    // 再接下来是hash冲突发生时，下一个溢出桶的地址\n}\n```\nbmap不只tophash还有两个方法 overflow 和setoverflow\n\n```go\nfunc (b *bmap) overflow(t *maptype) *bmap {\n\treturn *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize))\n}\n\nfunc (b *bmap) setoverflow(t *maptype, ovf *bmap) {\n\t*(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize)) = ovf\n}\n```\n\nhmap中的buckets中的原色bucket就是bmap，即 buckets[0],bucket[1],... bucket[2^B-1]如下图\n\n![](/images/hmap.png)\n\n**bucket就是bmap**\n\nbmap 是存放 k-v 的地方，我们把视角拉近，仔细看 bmap 的内部组成。\n\n![](/images/bmap.png)\n\nkey 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到**最后 B 个 bit 位**。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32\n\n例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是：\n\n `10010111 | 000011110110110010001111001010100010010110010101010 │ 01010`\n \n用最后的 5 个 bit 位，也就是 `01010`，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。\n\n再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。\n\nbuckets 编号就是桶编号，当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key\n\n**hash冲突的两种表示方式**：\n\n- 开放寻址法（hash冲突时，在当前index往后查找第一个空的位置即可）\n- 拉链法\n\nmap在写入过程会发生扩容，`runtime.mapassign` **函数会在以下两种情况发生时触发哈希的扩容**：\n\n- 装载因子已经超过 6.5；装载因子=总数量/桶的数量\n- 哈希使用了太多溢出桶；溢出捅的数量 超过正常桶的数量 即 noverflow 大于 1<<B buckets\n\n**每次都会将桶的数量翻倍**\n\n\n### 扩容机制：\n- **翻倍扩容**：哈希在存储元素过多时状态会触发扩容操作，每次都会将桶的数量翻倍，整个扩容过程并不是原子的，而是通过 runtime.growWork 增量触发的，在扩容期间访问哈希表时会使用旧桶，向哈希表**写入数据时**会触发旧桶元素的分流；\n- **等量扩容**，**为了解决大量写入、删除造成的内存泄漏问题**，哈希引入了 sameSizeGrow这一机制，在出现较多溢出桶时会对哈希进行『内存整理』减少对空间的占用。\n\n### 参考链接 \n- [https://www.jianshu.com/p/aa0d4808cbb8](https://www.jianshu.com/p/aa0d4808cbb8)\n\n- [https://segmentfault.com/a/1190000018387055](https://segmentfault.com/a/1190000018387055)\n\n\n","slug":"gomap","published":1,"updated":"2020-05-15T15:11:35.511Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckcnd4l8d000cr5flsi6tmwun","content":"<h2 id=\"map原理分析\"><a href=\"#map原理分析\" class=\"headerlink\" title=\"map原理分析\"></a>map原理分析</h2><h3 id=\"map-结构体\"><a href=\"#map-结构体\" class=\"headerlink\" title=\"map 结构体\"></a>map 结构体</h3><figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> hmap <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    count     <span class=\"keyword\">int</span>    <span class=\"comment\">// 元素的个数</span></span><br><span class=\"line\">    flags     <span class=\"keyword\">uint8</span>  <span class=\"comment\">// 状态标志</span></span><br><span class=\"line\">    B         <span class=\"keyword\">uint8</span>  <span class=\"comment\">// 可以最多容纳 6.5 * 2 ^ B 个元素，6.5为装载因子</span></span><br><span class=\"line\">    noverflow <span class=\"keyword\">uint16</span> <span class=\"comment\">// 溢出的个数</span></span><br><span class=\"line\">    hash0     <span class=\"keyword\">uint32</span> <span class=\"comment\">// 哈希种子</span></span><br><span class=\"line\"></span><br><span class=\"line\">    buckets    unsafe.Pointer <span class=\"comment\">// 桶的地址</span></span><br><span class=\"line\">    oldbuckets unsafe.Pointer <span class=\"comment\">// 旧桶的地址，用于扩容</span></span><br><span class=\"line\">    nevacuate  <span class=\"keyword\">uintptr</span>        <span class=\"comment\">// 搬迁进度，小于nevacuate的已经搬迁</span></span><br><span class=\"line\">    overflow *[<span class=\"number\">2</span>]*[]*bmap </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// A bucket for a Go map.</span></span><br><span class=\"line\"><span class=\"keyword\">type</span> bmap <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 每个元素hash值的高8位，如果tophash[0] &lt; minTopHash，表示这个桶的搬迁状态</span></span><br><span class=\"line\">    tophash [bucketCnt]<span class=\"keyword\">uint8</span></span><br><span class=\"line\">    <span class=\"comment\">// bucketCnt是常量8,接下来是8个key、8个value，但是我们不能直接看到；为了优化对齐，go采用了key放在一起，value放在一起的存储方式，</span></span><br><span class=\"line\"><span class=\"number\">8</span>个k，<span class=\"number\">8</span>个v得内存地址</span><br><span class=\"line\">    <span class=\"comment\">// 再接下来是hash冲突发生时，下一个溢出桶的地址</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>bmap不只tophash还有两个方法 overflow 和setoverflow</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(b *bmap)</span> <span class=\"title\">overflow</span><span class=\"params\">(t *maptype)</span> *<span class=\"title\">bmap</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> *(**bmap)(add(unsafe.Pointer(b), <span class=\"keyword\">uintptr</span>(t.bucketsize)-sys.PtrSize))</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(b *bmap)</span> <span class=\"title\">setoverflow</span><span class=\"params\">(t *maptype, ovf *bmap)</span></span> &#123;</span><br><span class=\"line\">\t*(**bmap)(add(unsafe.Pointer(b), <span class=\"keyword\">uintptr</span>(t.bucketsize)-sys.PtrSize)) = ovf</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>hmap中的buckets中的原色bucket就是bmap，即 buckets[0],bucket[1],… bucket[2^B-1]如下图</p>\n<p><img src=\"/images/hmap.png\" alt=\"\"></p>\n<p><strong>bucket就是bmap</strong></p>\n<p>bmap 是存放 k-v 的地方，我们把视角拉近，仔细看 bmap 的内部组成。</p>\n<p><img src=\"/images/bmap.png\" alt=\"\"></p>\n<p>key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到<strong>最后 B 个 bit 位</strong>。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32</p>\n<p>例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是：</p>\n<p> <code>10010111 | 000011110110110010001111001010100010010110010101010 │ 01010</code></p>\n<p>用最后的 5 个 bit 位，也就是 <code>01010</code>，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。</p>\n<p>再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。</p>\n<p>buckets 编号就是桶编号，当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key</p>\n<p><strong>hash冲突的两种表示方式</strong>：</p>\n<ul>\n<li>开放寻址法（hash冲突时，在当前index往后查找第一个空的位置即可）</li>\n<li>拉链法</li>\n</ul>\n<p>map在写入过程会发生扩容，<code>runtime.mapassign</code> <strong>函数会在以下两种情况发生时触发哈希的扩容</strong>：</p>\n<ul>\n<li>装载因子已经超过 6.5；装载因子=总数量/桶的数量</li>\n<li>哈希使用了太多溢出桶；溢出捅的数量 超过正常桶的数量 即 noverflow 大于 1&lt;&lt;B buckets</li>\n</ul>\n<p><strong>每次都会将桶的数量翻倍</strong></p>\n<h3 id=\"扩容机制：\"><a href=\"#扩容机制：\" class=\"headerlink\" title=\"扩容机制：\"></a>扩容机制：</h3><ul>\n<li><strong>翻倍扩容</strong>：哈希在存储元素过多时状态会触发扩容操作，每次都会将桶的数量翻倍，整个扩容过程并不是原子的，而是通过 runtime.growWork 增量触发的，在扩容期间访问哈希表时会使用旧桶，向哈希表<strong>写入数据时</strong>会触发旧桶元素的分流；</li>\n<li><strong>等量扩容</strong>，<strong>为了解决大量写入、删除造成的内存泄漏问题</strong>，哈希引入了 sameSizeGrow这一机制，在出现较多溢出桶时会对哈希进行『内存整理』减少对空间的占用。</li>\n</ul>\n<h3 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h3><ul>\n<li><p><a href=\"https://www.jianshu.com/p/aa0d4808cbb8\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/aa0d4808cbb8</a></p>\n</li>\n<li><p><a href=\"https://segmentfault.com/a/1190000018387055\" target=\"_blank\" rel=\"noopener\">https://segmentfault.com/a/1190000018387055</a></p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"map原理分析\"><a href=\"#map原理分析\" class=\"headerlink\" title=\"map原理分析\"></a>map原理分析</h2><h3 id=\"map-结构体\"><a href=\"#map-结构体\" class=\"headerlink\" title=\"map 结构体\"></a>map 结构体</h3><figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">type</span> hmap <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    count     <span class=\"keyword\">int</span>    <span class=\"comment\">// 元素的个数</span></span><br><span class=\"line\">    flags     <span class=\"keyword\">uint8</span>  <span class=\"comment\">// 状态标志</span></span><br><span class=\"line\">    B         <span class=\"keyword\">uint8</span>  <span class=\"comment\">// 可以最多容纳 6.5 * 2 ^ B 个元素，6.5为装载因子</span></span><br><span class=\"line\">    noverflow <span class=\"keyword\">uint16</span> <span class=\"comment\">// 溢出的个数</span></span><br><span class=\"line\">    hash0     <span class=\"keyword\">uint32</span> <span class=\"comment\">// 哈希种子</span></span><br><span class=\"line\"></span><br><span class=\"line\">    buckets    unsafe.Pointer <span class=\"comment\">// 桶的地址</span></span><br><span class=\"line\">    oldbuckets unsafe.Pointer <span class=\"comment\">// 旧桶的地址，用于扩容</span></span><br><span class=\"line\">    nevacuate  <span class=\"keyword\">uintptr</span>        <span class=\"comment\">// 搬迁进度，小于nevacuate的已经搬迁</span></span><br><span class=\"line\">    overflow *[<span class=\"number\">2</span>]*[]*bmap </span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// A bucket for a Go map.</span></span><br><span class=\"line\"><span class=\"keyword\">type</span> bmap <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 每个元素hash值的高8位，如果tophash[0] &lt; minTopHash，表示这个桶的搬迁状态</span></span><br><span class=\"line\">    tophash [bucketCnt]<span class=\"keyword\">uint8</span></span><br><span class=\"line\">    <span class=\"comment\">// bucketCnt是常量8,接下来是8个key、8个value，但是我们不能直接看到；为了优化对齐，go采用了key放在一起，value放在一起的存储方式，</span></span><br><span class=\"line\"><span class=\"number\">8</span>个k，<span class=\"number\">8</span>个v得内存地址</span><br><span class=\"line\">    <span class=\"comment\">// 再接下来是hash冲突发生时，下一个溢出桶的地址</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>bmap不只tophash还有两个方法 overflow 和setoverflow</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(b *bmap)</span> <span class=\"title\">overflow</span><span class=\"params\">(t *maptype)</span> *<span class=\"title\">bmap</span></span> &#123;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> *(**bmap)(add(unsafe.Pointer(b), <span class=\"keyword\">uintptr</span>(t.bucketsize)-sys.PtrSize))</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"params\">(b *bmap)</span> <span class=\"title\">setoverflow</span><span class=\"params\">(t *maptype, ovf *bmap)</span></span> &#123;</span><br><span class=\"line\">\t*(**bmap)(add(unsafe.Pointer(b), <span class=\"keyword\">uintptr</span>(t.bucketsize)-sys.PtrSize)) = ovf</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>hmap中的buckets中的原色bucket就是bmap，即 buckets[0],bucket[1],… bucket[2^B-1]如下图</p>\n<p><img src=\"/images/hmap.png\" alt=\"\"></p>\n<p><strong>bucket就是bmap</strong></p>\n<p>bmap 是存放 k-v 的地方，我们把视角拉近，仔细看 bmap 的内部组成。</p>\n<p><img src=\"/images/bmap.png\" alt=\"\"></p>\n<p>key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到<strong>最后 B 个 bit 位</strong>。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32</p>\n<p>例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是：</p>\n<p> <code>10010111 | 000011110110110010001111001010100010010110010101010 │ 01010</code></p>\n<p>用最后的 5 个 bit 位，也就是 <code>01010</code>，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。</p>\n<p>再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。</p>\n<p>buckets 编号就是桶编号，当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key</p>\n<p><strong>hash冲突的两种表示方式</strong>：</p>\n<ul>\n<li>开放寻址法（hash冲突时，在当前index往后查找第一个空的位置即可）</li>\n<li>拉链法</li>\n</ul>\n<p>map在写入过程会发生扩容，<code>runtime.mapassign</code> <strong>函数会在以下两种情况发生时触发哈希的扩容</strong>：</p>\n<ul>\n<li>装载因子已经超过 6.5；装载因子=总数量/桶的数量</li>\n<li>哈希使用了太多溢出桶；溢出捅的数量 超过正常桶的数量 即 noverflow 大于 1&lt;&lt;B buckets</li>\n</ul>\n<p><strong>每次都会将桶的数量翻倍</strong></p>\n<h3 id=\"扩容机制：\"><a href=\"#扩容机制：\" class=\"headerlink\" title=\"扩容机制：\"></a>扩容机制：</h3><ul>\n<li><strong>翻倍扩容</strong>：哈希在存储元素过多时状态会触发扩容操作，每次都会将桶的数量翻倍，整个扩容过程并不是原子的，而是通过 runtime.growWork 增量触发的，在扩容期间访问哈希表时会使用旧桶，向哈希表<strong>写入数据时</strong>会触发旧桶元素的分流；</li>\n<li><strong>等量扩容</strong>，<strong>为了解决大量写入、删除造成的内存泄漏问题</strong>，哈希引入了 sameSizeGrow这一机制，在出现较多溢出桶时会对哈希进行『内存整理』减少对空间的占用。</li>\n</ul>\n<h3 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h3><ul>\n<li><p><a href=\"https://www.jianshu.com/p/aa0d4808cbb8\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/aa0d4808cbb8</a></p>\n</li>\n<li><p><a href=\"https://segmentfault.com/a/1190000018387055\" target=\"_blank\" rel=\"noopener\">https://segmentfault.com/a/1190000018387055</a></p>\n</li>\n</ul>\n"},{"layout":"post","title":"Java 中的并发","date":"2017-06-10T14:40:18.000Z","author":"zhaojizhuang","mathjax":true,"_content":"\n\n## Java 中的并发\n### 如何创建一个线程\n\n按 Java 语言规范中的说法，创建线程只有一种方式，就是创建一个 Thread 对象。而从 HotSpot 虚拟机的角度看，创建一个虚拟机线程 有两种方式，一种是创建 Thread 对象，另一种是创建 一个本地线程，加入到虚拟机线程中。\n\n如果从 Java 语法的角度。有两种方法。\n\n第一是继承 Thread 类，实现 run 方法，并创建子类对象。\n\n```java\n\n    public void startThreadUseSubClass() {\n        class MyThread extends Thread {\n            public void run() {\n                System.out.println(\"start thread using Subclass of Thread\");\n            }\n        }\n        MyThread thread = new MyThread();\n        thread.start();\n    }\n```\n\n另一种是传递给 Thread 构造函数一个 Runnable 对象。\n\n```java\n\n    public void startThreadUseRunnalbe() {\n        Thread thread = new Thread(new Runnable() {\n            public void run() {\n                System.out.println(\"start thread using runnable\");\n            }\n        });\n        thread.start();\n    }\n```\n\n当然， Runnalbe 对象，也不是只有这一种形式，例如如果我们想要线程执行时返回一个值，就需要用到另一种 Runnalbe 对象，它 对原来的 Runnalbe 对象进行了包装。\n\n```java\n    public void startFutureTask() {\n        FutureTask<Integer> task = new FutureTask<>(new Callable<Integer>() {\n            public Integer call() {\n                return 1;\n            }\n        });\n\n        new Thread(task).start();\n\n        try {\n            Integer result = task.get();\n            System.out.println(\"future result \" + result);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        } catch (ExecutionException e) {\n            e.printStackTrace();\n        }\n    }\n ```\n\n### 结束线程\n\n#### wait 与 sleep\n\nsleep 会使得当前线程休眠一段时间，但并不会释放已经得到的锁。\n\nwait 会阻塞住，并释放已经得到的锁。一直到有人调用 notify 或者 notifyAll，它会重新尝试得到锁，然后再唤醒。\n\n### 线程池\n\n#### 好处\n\n- 复用\n\n\n线程池中有一系列线程，这些线程在执行完任务后，并不会被销毁，而会从任务队列中取出任务，执行这些任务。这样，就避免为每个任务 都创建线程，销毁线程。 在有大量短命线程的场景下，如果创建线程和销毁线程的时间比线程执行任务的时间还长，显然是不划算的，这时候，使用线程池就会有明显 的好处。\n\n- 流控\n\n\n同时，可以设置线程数目，这样，线程不会增大到影响系统整体性能的程度。当任务太多时，可以在队列中排队， 如果有空闲线程，他们会从队列中取出任务执行。\n\n#### 使用\n\n- 线程数目\n\n\n那么，线程的数目要设置成多少呢？这需要根据任务类型的不同来设置，假如是大量计算型的任务，他们不会阻塞，那么可以将线程数目设置 为处理器数目。而如果任务中涉及大量IO，有些线程会阻塞住，这样就要根据阻塞线程数目与运行线程数目的比例，以及处理器数目来设置 线程总数目。例如阻塞线程数目与运行线程数目之比为n, 处理器数目为p，那么可以设置 n * (p + 1) 个线程，保证有 n 个线程处于运行 状态。\n\n- Executors\n\n\nJDK 的 java.util.concurrent.Executors 类提供了几个静态的方法，用于创建不同类型的线程池。\n\n```java\n\nExecutorService service = Executors.newFixedThreadPool(10);\nArrayList<Future<Integer>> results = new ArrayList<>();\nfor (int i = 0; i < 14; i++) {\n    Future<Integer> r = service.submit(new Callable<Integer>() {\n        public Integer call() {\n        return new Random().nextInt();\n    });\n    results.add(r);\n}\n\n```\nnewFixedThreadPool 可以创建固定数目的线程，一旦创建不会自动销毁线程，即便长期没有任务。除非显式关闭线程池。如果任务队列中有任务，就取出任务执行。\n\n另外，还可以使用 newCachedThreadPool 方法创建一个不设定固定线程数目的线程池，它有一个特性，线程完成任务后，如果一分钟之内又有新任务，就会复用这个线程执行新任务。如果超过一分钟还没有任务执行，就会自动销毁。\n\n另外，还提供了 newSingleThreadExecutor 创建有一个工作线程的线程池。\n\n### 原理\nJDK 中的线程池通过 HashSet 存储工作者线程，通过 BlockingQueue 来存储待处理任务。\n\n通过核心工作者数目(corePoolSize) 和 最大工作者数目(maximumPoolSize) 来确定如何处理任务。如果当前工作者线程数目 小于核心工作者数目，则创建一个工作者线程执行这个任务。否则，将这个任务放入待处理队列。如果入队失败，再看看当前工作 者数目是不是小于最大工作者数目，如果小于，则创建工作者线程执行这个任务。否则，拒绝执行这个任务。\n\n另外，如果待处理队列中没有任务要处理，并且工作者线程数目超过了核心工作者数目，那么，需要减少工作者线程数目。\n","source":"_posts/java-create-thread.md","raw":"---\nlayout: post\ntitle:  \"Java 中的并发\"\ndate:   2017-06-10 22:40:18 +0800\ncategories: Java\ntags:  [\"java\",\"线程\"]\nauthor: zhaojizhuang\nmathjax: true\n---\n\n\n## Java 中的并发\n### 如何创建一个线程\n\n按 Java 语言规范中的说法，创建线程只有一种方式，就是创建一个 Thread 对象。而从 HotSpot 虚拟机的角度看，创建一个虚拟机线程 有两种方式，一种是创建 Thread 对象，另一种是创建 一个本地线程，加入到虚拟机线程中。\n\n如果从 Java 语法的角度。有两种方法。\n\n第一是继承 Thread 类，实现 run 方法，并创建子类对象。\n\n```java\n\n    public void startThreadUseSubClass() {\n        class MyThread extends Thread {\n            public void run() {\n                System.out.println(\"start thread using Subclass of Thread\");\n            }\n        }\n        MyThread thread = new MyThread();\n        thread.start();\n    }\n```\n\n另一种是传递给 Thread 构造函数一个 Runnable 对象。\n\n```java\n\n    public void startThreadUseRunnalbe() {\n        Thread thread = new Thread(new Runnable() {\n            public void run() {\n                System.out.println(\"start thread using runnable\");\n            }\n        });\n        thread.start();\n    }\n```\n\n当然， Runnalbe 对象，也不是只有这一种形式，例如如果我们想要线程执行时返回一个值，就需要用到另一种 Runnalbe 对象，它 对原来的 Runnalbe 对象进行了包装。\n\n```java\n    public void startFutureTask() {\n        FutureTask<Integer> task = new FutureTask<>(new Callable<Integer>() {\n            public Integer call() {\n                return 1;\n            }\n        });\n\n        new Thread(task).start();\n\n        try {\n            Integer result = task.get();\n            System.out.println(\"future result \" + result);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        } catch (ExecutionException e) {\n            e.printStackTrace();\n        }\n    }\n ```\n\n### 结束线程\n\n#### wait 与 sleep\n\nsleep 会使得当前线程休眠一段时间，但并不会释放已经得到的锁。\n\nwait 会阻塞住，并释放已经得到的锁。一直到有人调用 notify 或者 notifyAll，它会重新尝试得到锁，然后再唤醒。\n\n### 线程池\n\n#### 好处\n\n- 复用\n\n\n线程池中有一系列线程，这些线程在执行完任务后，并不会被销毁，而会从任务队列中取出任务，执行这些任务。这样，就避免为每个任务 都创建线程，销毁线程。 在有大量短命线程的场景下，如果创建线程和销毁线程的时间比线程执行任务的时间还长，显然是不划算的，这时候，使用线程池就会有明显 的好处。\n\n- 流控\n\n\n同时，可以设置线程数目，这样，线程不会增大到影响系统整体性能的程度。当任务太多时，可以在队列中排队， 如果有空闲线程，他们会从队列中取出任务执行。\n\n#### 使用\n\n- 线程数目\n\n\n那么，线程的数目要设置成多少呢？这需要根据任务类型的不同来设置，假如是大量计算型的任务，他们不会阻塞，那么可以将线程数目设置 为处理器数目。而如果任务中涉及大量IO，有些线程会阻塞住，这样就要根据阻塞线程数目与运行线程数目的比例，以及处理器数目来设置 线程总数目。例如阻塞线程数目与运行线程数目之比为n, 处理器数目为p，那么可以设置 n * (p + 1) 个线程，保证有 n 个线程处于运行 状态。\n\n- Executors\n\n\nJDK 的 java.util.concurrent.Executors 类提供了几个静态的方法，用于创建不同类型的线程池。\n\n```java\n\nExecutorService service = Executors.newFixedThreadPool(10);\nArrayList<Future<Integer>> results = new ArrayList<>();\nfor (int i = 0; i < 14; i++) {\n    Future<Integer> r = service.submit(new Callable<Integer>() {\n        public Integer call() {\n        return new Random().nextInt();\n    });\n    results.add(r);\n}\n\n```\nnewFixedThreadPool 可以创建固定数目的线程，一旦创建不会自动销毁线程，即便长期没有任务。除非显式关闭线程池。如果任务队列中有任务，就取出任务执行。\n\n另外，还可以使用 newCachedThreadPool 方法创建一个不设定固定线程数目的线程池，它有一个特性，线程完成任务后，如果一分钟之内又有新任务，就会复用这个线程执行新任务。如果超过一分钟还没有任务执行，就会自动销毁。\n\n另外，还提供了 newSingleThreadExecutor 创建有一个工作线程的线程池。\n\n### 原理\nJDK 中的线程池通过 HashSet 存储工作者线程，通过 BlockingQueue 来存储待处理任务。\n\n通过核心工作者数目(corePoolSize) 和 最大工作者数目(maximumPoolSize) 来确定如何处理任务。如果当前工作者线程数目 小于核心工作者数目，则创建一个工作者线程执行这个任务。否则，将这个任务放入待处理队列。如果入队失败，再看看当前工作 者数目是不是小于最大工作者数目，如果小于，则创建工作者线程执行这个任务。否则，拒绝执行这个任务。\n\n另外，如果待处理队列中没有任务要处理，并且工作者线程数目超过了核心工作者数目，那么，需要减少工作者线程数目。\n","slug":"java-create-thread","published":1,"updated":"2020-05-19T13:30:57.269Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l8e000gr5flllcdi90o","content":"<h2 id=\"Java-中的并发\"><a href=\"#Java-中的并发\" class=\"headerlink\" title=\"Java 中的并发\"></a>Java 中的并发</h2><h3 id=\"如何创建一个线程\"><a href=\"#如何创建一个线程\" class=\"headerlink\" title=\"如何创建一个线程\"></a>如何创建一个线程</h3><p>按 Java 语言规范中的说法，创建线程只有一种方式，就是创建一个 Thread 对象。而从 HotSpot 虚拟机的角度看，创建一个虚拟机线程 有两种方式，一种是创建 Thread 对象，另一种是创建 一个本地线程，加入到虚拟机线程中。</p>\n<p>如果从 Java 语法的角度。有两种方法。</p>\n<p>第一是继承 Thread 类，实现 run 方法，并创建子类对象。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">startThreadUseSubClass</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"start thread using Subclass of Thread\"</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    MyThread thread = <span class=\"keyword\">new</span> MyThread();</span><br><span class=\"line\">    thread.start();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>另一种是传递给 Thread 构造函数一个 Runnable 对象。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">startThreadUseRunnalbe</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Thread thread = <span class=\"keyword\">new</span> Thread(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"start thread using runnable\"</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">    thread.start();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>当然， Runnalbe 对象，也不是只有这一种形式，例如如果我们想要线程执行时返回一个值，就需要用到另一种 Runnalbe 对象，它 对原来的 Runnalbe 对象进行了包装。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">startFutureTask</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    FutureTask&lt;Integer&gt; task = <span class=\"keyword\">new</span> FutureTask&lt;&gt;(<span class=\"keyword\">new</span> Callable&lt;Integer&gt;() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Thread(task).start();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        Integer result = task.get();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">\"future result \"</span> + result);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (ExecutionException e) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"结束线程\"><a href=\"#结束线程\" class=\"headerlink\" title=\"结束线程\"></a>结束线程</h3><h4 id=\"wait-与-sleep\"><a href=\"#wait-与-sleep\" class=\"headerlink\" title=\"wait 与 sleep\"></a>wait 与 sleep</h4><p>sleep 会使得当前线程休眠一段时间，但并不会释放已经得到的锁。</p>\n<p>wait 会阻塞住，并释放已经得到的锁。一直到有人调用 notify 或者 notifyAll，它会重新尝试得到锁，然后再唤醒。</p>\n<h3 id=\"线程池\"><a href=\"#线程池\" class=\"headerlink\" title=\"线程池\"></a>线程池</h3><h4 id=\"好处\"><a href=\"#好处\" class=\"headerlink\" title=\"好处\"></a>好处</h4><ul>\n<li>复用</li>\n</ul>\n<p>线程池中有一系列线程，这些线程在执行完任务后，并不会被销毁，而会从任务队列中取出任务，执行这些任务。这样，就避免为每个任务 都创建线程，销毁线程。 在有大量短命线程的场景下，如果创建线程和销毁线程的时间比线程执行任务的时间还长，显然是不划算的，这时候，使用线程池就会有明显 的好处。</p>\n<ul>\n<li>流控</li>\n</ul>\n<p>同时，可以设置线程数目，这样，线程不会增大到影响系统整体性能的程度。当任务太多时，可以在队列中排队， 如果有空闲线程，他们会从队列中取出任务执行。</p>\n<h4 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h4><ul>\n<li>线程数目</li>\n</ul>\n<p>那么，线程的数目要设置成多少呢？这需要根据任务类型的不同来设置，假如是大量计算型的任务，他们不会阻塞，那么可以将线程数目设置 为处理器数目。而如果任务中涉及大量IO，有些线程会阻塞住，这样就要根据阻塞线程数目与运行线程数目的比例，以及处理器数目来设置 线程总数目。例如阻塞线程数目与运行线程数目之比为n, 处理器数目为p，那么可以设置 n * (p + 1) 个线程，保证有 n 个线程处于运行 状态。</p>\n<ul>\n<li>Executors</li>\n</ul>\n<p>JDK 的 java.util.concurrent.Executors 类提供了几个静态的方法，用于创建不同类型的线程池。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">ExecutorService service = Executors.newFixedThreadPool(<span class=\"number\">10</span>);</span><br><span class=\"line\">ArrayList&lt;Future&lt;Integer&gt;&gt; results = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">14</span>; i++) &#123;</span><br><span class=\"line\">    Future&lt;Integer&gt; r = service.submit(<span class=\"keyword\">new</span> Callable&lt;Integer&gt;() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Random().nextInt();</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">    results.add(r);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>newFixedThreadPool 可以创建固定数目的线程，一旦创建不会自动销毁线程，即便长期没有任务。除非显式关闭线程池。如果任务队列中有任务，就取出任务执行。</p>\n<p>另外，还可以使用 newCachedThreadPool 方法创建一个不设定固定线程数目的线程池，它有一个特性，线程完成任务后，如果一分钟之内又有新任务，就会复用这个线程执行新任务。如果超过一分钟还没有任务执行，就会自动销毁。</p>\n<p>另外，还提供了 newSingleThreadExecutor 创建有一个工作线程的线程池。</p>\n<h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><p>JDK 中的线程池通过 HashSet 存储工作者线程，通过 BlockingQueue 来存储待处理任务。</p>\n<p>通过核心工作者数目(corePoolSize) 和 最大工作者数目(maximumPoolSize) 来确定如何处理任务。如果当前工作者线程数目 小于核心工作者数目，则创建一个工作者线程执行这个任务。否则，将这个任务放入待处理队列。如果入队失败，再看看当前工作 者数目是不是小于最大工作者数目，如果小于，则创建工作者线程执行这个任务。否则，拒绝执行这个任务。</p>\n<p>另外，如果待处理队列中没有任务要处理，并且工作者线程数目超过了核心工作者数目，那么，需要减少工作者线程数目。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Java-中的并发\"><a href=\"#Java-中的并发\" class=\"headerlink\" title=\"Java 中的并发\"></a>Java 中的并发</h2><h3 id=\"如何创建一个线程\"><a href=\"#如何创建一个线程\" class=\"headerlink\" title=\"如何创建一个线程\"></a>如何创建一个线程</h3><p>按 Java 语言规范中的说法，创建线程只有一种方式，就是创建一个 Thread 对象。而从 HotSpot 虚拟机的角度看，创建一个虚拟机线程 有两种方式，一种是创建 Thread 对象，另一种是创建 一个本地线程，加入到虚拟机线程中。</p>\n<p>如果从 Java 语法的角度。有两种方法。</p>\n<p>第一是继承 Thread 类，实现 run 方法，并创建子类对象。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">startThreadUseSubClass</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MyThread</span> <span class=\"keyword\">extends</span> <span class=\"title\">Thread</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"start thread using Subclass of Thread\"</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    MyThread thread = <span class=\"keyword\">new</span> MyThread();</span><br><span class=\"line\">    thread.start();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>另一种是传递给 Thread 构造函数一个 Runnable 对象。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">startThreadUseRunnalbe</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    Thread thread = <span class=\"keyword\">new</span> Thread(<span class=\"keyword\">new</span> Runnable() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">run</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            System.out.println(<span class=\"string\">\"start thread using runnable\"</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">    thread.start();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>当然， Runnalbe 对象，也不是只有这一种形式，例如如果我们想要线程执行时返回一个值，就需要用到另一种 Runnalbe 对象，它 对原来的 Runnalbe 对象进行了包装。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">startFutureTask</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">    FutureTask&lt;Integer&gt; task = <span class=\"keyword\">new</span> FutureTask&lt;&gt;(<span class=\"keyword\">new</span> Callable&lt;Integer&gt;() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">new</span> Thread(task).start();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">        Integer result = task.get();</span><br><span class=\"line\">        System.out.println(<span class=\"string\">\"future result \"</span> + result);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (InterruptedException e) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> (ExecutionException e) &#123;</span><br><span class=\"line\">        e.printStackTrace();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"结束线程\"><a href=\"#结束线程\" class=\"headerlink\" title=\"结束线程\"></a>结束线程</h3><h4 id=\"wait-与-sleep\"><a href=\"#wait-与-sleep\" class=\"headerlink\" title=\"wait 与 sleep\"></a>wait 与 sleep</h4><p>sleep 会使得当前线程休眠一段时间，但并不会释放已经得到的锁。</p>\n<p>wait 会阻塞住，并释放已经得到的锁。一直到有人调用 notify 或者 notifyAll，它会重新尝试得到锁，然后再唤醒。</p>\n<h3 id=\"线程池\"><a href=\"#线程池\" class=\"headerlink\" title=\"线程池\"></a>线程池</h3><h4 id=\"好处\"><a href=\"#好处\" class=\"headerlink\" title=\"好处\"></a>好处</h4><ul>\n<li>复用</li>\n</ul>\n<p>线程池中有一系列线程，这些线程在执行完任务后，并不会被销毁，而会从任务队列中取出任务，执行这些任务。这样，就避免为每个任务 都创建线程，销毁线程。 在有大量短命线程的场景下，如果创建线程和销毁线程的时间比线程执行任务的时间还长，显然是不划算的，这时候，使用线程池就会有明显 的好处。</p>\n<ul>\n<li>流控</li>\n</ul>\n<p>同时，可以设置线程数目，这样，线程不会增大到影响系统整体性能的程度。当任务太多时，可以在队列中排队， 如果有空闲线程，他们会从队列中取出任务执行。</p>\n<h4 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h4><ul>\n<li>线程数目</li>\n</ul>\n<p>那么，线程的数目要设置成多少呢？这需要根据任务类型的不同来设置，假如是大量计算型的任务，他们不会阻塞，那么可以将线程数目设置 为处理器数目。而如果任务中涉及大量IO，有些线程会阻塞住，这样就要根据阻塞线程数目与运行线程数目的比例，以及处理器数目来设置 线程总数目。例如阻塞线程数目与运行线程数目之比为n, 处理器数目为p，那么可以设置 n * (p + 1) 个线程，保证有 n 个线程处于运行 状态。</p>\n<ul>\n<li>Executors</li>\n</ul>\n<p>JDK 的 java.util.concurrent.Executors 类提供了几个静态的方法，用于创建不同类型的线程池。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">ExecutorService service = Executors.newFixedThreadPool(<span class=\"number\">10</span>);</span><br><span class=\"line\">ArrayList&lt;Future&lt;Integer&gt;&gt; results = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">14</span>; i++) &#123;</span><br><span class=\"line\">    Future&lt;Integer&gt; r = service.submit(<span class=\"keyword\">new</span> Callable&lt;Integer&gt;() &#123;</span><br><span class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Random().nextInt();</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">    results.add(r);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>newFixedThreadPool 可以创建固定数目的线程，一旦创建不会自动销毁线程，即便长期没有任务。除非显式关闭线程池。如果任务队列中有任务，就取出任务执行。</p>\n<p>另外，还可以使用 newCachedThreadPool 方法创建一个不设定固定线程数目的线程池，它有一个特性，线程完成任务后，如果一分钟之内又有新任务，就会复用这个线程执行新任务。如果超过一分钟还没有任务执行，就会自动销毁。</p>\n<p>另外，还提供了 newSingleThreadExecutor 创建有一个工作线程的线程池。</p>\n<h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><p>JDK 中的线程池通过 HashSet 存储工作者线程，通过 BlockingQueue 来存储待处理任务。</p>\n<p>通过核心工作者数目(corePoolSize) 和 最大工作者数目(maximumPoolSize) 来确定如何处理任务。如果当前工作者线程数目 小于核心工作者数目，则创建一个工作者线程执行这个任务。否则，将这个任务放入待处理队列。如果入队失败，再看看当前工作 者数目是不是小于最大工作者数目，如果小于，则创建工作者线程执行这个任务。否则，拒绝执行这个任务。</p>\n<p>另外，如果待处理队列中没有任务要处理，并且工作者线程数目超过了核心工作者数目，那么，需要减少工作者线程数目。</p>\n"},{"layout":"post","title":"kubernetes&容器网络（2）之iptables","date":"2018-12-05T05:19:10.000Z","author":"zhaojizhuang","_content":"\n\n## iptables规则\n\n> 参考 [http://www.zsythink.net/archives/tag/iptables/](http://www.zsythink.net/archives/tag/iptables/)\n\n![](http://www.zsythink.net/wp-content/uploads/2017/02/021217_0051_2.png)\n\n**用户空间，例如从pod中流出的流量就是从ouput链流出**\n\n上图表示的`iptables`的链，链 和表的关系如下，以`PREROUTING`链为例\n\n![](http://www.zsythink.net/wp-content/uploads/2017/02/021217_0051_4.png)\n\n这幅图是什么意思呢？它的意思是说，prerouting\"链\"只拥有nat表、raw表和mangle表所对应的功能，所以，prerouting中的规则只能存放于nat表、raw表和mangle表中。\n\n## NAT \n\n**NAT的三种类型:**\n\n- SNAT  \n```shell\niptables -t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3\n# 目标流向eth0，源地址是xxx的，做SNAT，源地址改为xxx\n```\n- DNAT\n```shell\niptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3-192.168.5.5\n```\n- MASQUERADE 是SNAT的一种，可以自动获取网卡的ip来做SNAT，如果是ADSL这种动态ip的，如果用SNAT需要经常更改iptables规则 \n```shell\niptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j MASQUERADE\n# 源地址是xxx，流向eth0的，流向做自动化SNAT\n```\n*masquerade* 应为英文伪装\n\n## iptabels 常用命令\n\n```shell\niptables [-t 表名] 管理选项 [链名] [匹配条件] [-j 控制类型]\n# 控制类型包括 ACCETP REJECT DROP LOG 还有自定义的链（k8s的链）等\niptabels -t nat（表名） -nvL POSTROUTING(链的名字)\n```\nhttps://www.jianshu.com/p/ee4ee15d3658\n\n## 分析k8s下的iptables规则\n\n以如下 `service` 为例 \n\n```yaml\nName:                     testapi-smzdm-com\nNamespace:                zhongce-v2-0\nLabels:                   <none>\nSelector:                 zdm-app-owner=testapi-smzdm-com\nType:                     LoadBalancer\nIP:                       172.17.185.22\nLoadBalancer Ingress:     10.42.162.216\nPort:                     <unset>  809/TCP\nTargetPort:               809/TCP\nNodePort:                 <unset>  39746/TCP\nEndpoints:                10.42.147.255:809,10.42.38.222:809\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>\n```\n\n即 `cluster ip` 为 `172.17.185.22` 后端 `podip` 为 `10.42.147.25510.42.38.222`\n此外还有1个 `loadbalancer ip` `10.42.162.216`\n\n### svc的访问路径\n\n    - 集群内部，通过 `clusterip` 到访问到后端 `pod\n    - 集群外部，通过直接访问`nodeport`；或者通过 `elb` 负载均衡到 `node` 上再通过 `nodeport` 访问\n\n### cluster ip 的基本原理\n\n如果是集群内的应用访问 cluster ip，那就是从**用户空间**访问**内核空间网络协议栈**,走的是 `OUTPUT` 链\n\n1. 从`OUTPUT` 链开始\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL OUTPUT\nChain OUTPUT (policy ACCEPT 4 packets, 240 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n3424K  209M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */\n\n```\n2. `OUTPUT`下的规则 直接把流量交给 `KUBE-SERVICES` 链\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICES\nChain KUBE-SERVICES (2 references)\n pkts bytes target     prot opt in     out     source               destination     \n    0     0 KUBE-MARK-MASQ  tcp  --  *      *      0.0.0.0/0        172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809\n    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809\n   10   520 KUBE-FW-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            10.42.162.216        /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809\n     0     0 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTY\n```\n上述3条规则是顺序执行的：\n\n- 第1条规则匹配发往 `Cluster IP` `172.17.185.22` 的流量，跳转到了 `KUBE-MARK-MASQ` 链进一步处理，其作用就是打了一个 `MARK` ，稍后展开说明。\n- 第2条规则匹配发往 `Cluster IP` `172.17.185.22` 的流量，跳转到了 `KUBE-SVC-G3OM5DSD2HHDMN6U` 链进一步处理，稍后展开说明。\n- 第3条规则匹配发往集群外 `LB IP` 的 `10.42.162.216` 的流量，跳转到了\n`KUBE-FW-G3OM5DSD2HHDMN6U` 链进一步处理，稍后展开说明。\n- 第4条  KUBE-NODEPORTS的规则在末尾，只要dst ip是node 本机ip的话 （（–dst-type LOCAL），就跳转到KUBE-NODEPORTS做进一步判定：）\n    **第2条规则要做dnat转发到后端具体的后端pod上**\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SVC-G3OM5DSD2HHDMN6U\nChain KUBE-SVC-G3OM5DSD2HHDMN6U (3 references)\n pkts bytes target     prot opt in     out     source               destination         \n   18   936 KUBE-SEP-JT2KW6YUTVPLLGV6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000\n   21  1092 KUBE-SEP-VETLC6CJY2HOK3EL  all  --  *      *       0.0.0.0/0            0.0.0.0/0\n```\n\n**两条 对应 后端pod的链**\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-JT2KW6YUTVPLLGV6\nChain KUBE-SEP-JT2KW6YUTVPLLGV6 (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 KUBE-MARK-MASQ  all  --  *      *       10.42.147.255        0.0.0.0/0           \n   26  1352 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.42.147.255:809\n\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-VETLC6CJY2HOK3EL\nChain KUBE-SEP-VETLC6CJY2HOK3EL (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 KUBE-MARK-MASQ  all  --  *      *       10.42.38.222         0.0.0.0/0           \n    2   104 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.42.38.222:809\n```\n\n流量经过路由表从eth0出去，在流量流出本机之前会经过POSTROUTING 链\n\n### 在流量离开本机的时候会经过 `POSTROUTING` 链\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL POSTROUTING\nChain POSTROUTING (policy ACCEPT 274 packets, 17340 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 632M   36G KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */\n\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-POSTROUTING\nChain KUBE-POSTROUTING (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n  526 27352 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ mark match 0x4000/0x4000\n```\n\n其实直接就跳转到了 `KUBE-POSTROUTING`，然后匹配打过`0x4000 MARK` 的流量，将其做 `SNAT` 转换，而这个 `MARK` 其实就是之前没说的 `KUBE-MARK-MASQ` 做的事情\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-MARK-MASQ\nChain KUBE-MARK-MASQ (183 references)\n pkts bytes target     prot opt in     out     source               destination         \n  492 25604 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000\n```\n当流量离开本机时，src IP会被修改为node的IP，而不是发出流量的POD IP了\n\n### 通过loadbalance ip进行访问\n\n最后还有一个KUBE-FW-G3OM5DSD2HHDMN6U链没有讲，从本机发往LB IP的流量要做啥事情呢？\n\n**其实也是让流量直接发往具体某个Endpoints，就别真的发往LB了，这样才能获得最佳的延迟**：\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-FW-G3OM5DSD2HHDMN6U\nChain KUBE-FW-G3OM5DSD2HHDMN6U (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    2   104 KUBE-MARK-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */\n    2   104 KUBE-SVC-G3OM5DSD2HHDMN6U  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */\n    0     0 KUBE-MARK-DROP  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */\n```\n\n### 通过nodeport 来访问\n\n回顾一下 KUBE_SERVICES规则\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICES\nChain KUBE-SERVICES (2 references)\n pkts bytes target     prot opt in     out     source               destination     \n    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !172.17.0.0/16        172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809\n    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809\n   10   520 KUBE-FW-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            10.42.162.216        /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809\n    0     0 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL\n```\n**`KUBE-NODEPORTS` 是最后一条规则**\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-NODEPORTS\nChain KUBE-NODEPORTS (1 references)\n pkts bytes target     prot opt in     out     source               destination        \n    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746\n    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746\n```\n第1条匹配dst port如果是39746，那么就打mark。\n第2条匹配dst port如果是39746，那么就跳到负载均衡链做DNAT改写。\n\n\n### 总结\n\n- `KUBE-SERVICES` 链的规则存在于 `OUTPUT POSTROUTING PREROUTING` 三个链上\n- 对于 `KUBE-SERVICES KUBE-NDOEPORTS-xxx KUBE-SEP-xxx` 下都会对符合条件（匹配条件）的规则打上MARK 可以重复打MARK\n- 在流量出node的时候做SNAT\n\n\n## 从集群内出去的流量怎么回来\n\n出node的流量在做SNAT的时候，netfilter有个连接跟踪机制，保存在 conntrack记录中\n\n这就是Netfilter的连接跟踪（conntrack）功能了。对于TCP协议来讲，肯定是上来先建立一个连接，可以用`源/目的IP+源/目的端口`  （四元组），唯一标识一条连接，这个连接会放在conntrack表里面。\n当时是这台机器去请求163网站的，虽然源地址已经Snat成公网IP地址了，但是 `conntrack` 表里面还是有这个连接的记录的。当163网站返回数据的时候，会找到记录，从而找到正确的私网IP地\n址。\n\n## 参考文档\n[k8s 的iptales规则详解](https://yuerblog.cc/2019/12/09/k8s-%E6%89%8B%E6%8A%8A%E6%89%8B%E5%88%86%E6%9E%90service%E7%94%9F%E6%88%90%E7%9A%84iptables%E8%A7%84%E5%88%99/)\n\n","source":"_posts/k8s-iptables.md","raw":"---\nlayout: post\ntitle:  \"kubernetes&容器网络（2）之iptables\"\ndate:   2018-12-05 13:19:10 +0800\ncategories: k8s\ntags:  [\"k8s\", \"iptables\"]\nauthor: zhaojizhuang\n\n---\n\n\n## iptables规则\n\n> 参考 [http://www.zsythink.net/archives/tag/iptables/](http://www.zsythink.net/archives/tag/iptables/)\n\n![](http://www.zsythink.net/wp-content/uploads/2017/02/021217_0051_2.png)\n\n**用户空间，例如从pod中流出的流量就是从ouput链流出**\n\n上图表示的`iptables`的链，链 和表的关系如下，以`PREROUTING`链为例\n\n![](http://www.zsythink.net/wp-content/uploads/2017/02/021217_0051_4.png)\n\n这幅图是什么意思呢？它的意思是说，prerouting\"链\"只拥有nat表、raw表和mangle表所对应的功能，所以，prerouting中的规则只能存放于nat表、raw表和mangle表中。\n\n## NAT \n\n**NAT的三种类型:**\n\n- SNAT  \n```shell\niptables -t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3\n# 目标流向eth0，源地址是xxx的，做SNAT，源地址改为xxx\n```\n- DNAT\n```shell\niptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3-192.168.5.5\n```\n- MASQUERADE 是SNAT的一种，可以自动获取网卡的ip来做SNAT，如果是ADSL这种动态ip的，如果用SNAT需要经常更改iptables规则 \n```shell\niptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j MASQUERADE\n# 源地址是xxx，流向eth0的，流向做自动化SNAT\n```\n*masquerade* 应为英文伪装\n\n## iptabels 常用命令\n\n```shell\niptables [-t 表名] 管理选项 [链名] [匹配条件] [-j 控制类型]\n# 控制类型包括 ACCETP REJECT DROP LOG 还有自定义的链（k8s的链）等\niptabels -t nat（表名） -nvL POSTROUTING(链的名字)\n```\nhttps://www.jianshu.com/p/ee4ee15d3658\n\n## 分析k8s下的iptables规则\n\n以如下 `service` 为例 \n\n```yaml\nName:                     testapi-smzdm-com\nNamespace:                zhongce-v2-0\nLabels:                   <none>\nSelector:                 zdm-app-owner=testapi-smzdm-com\nType:                     LoadBalancer\nIP:                       172.17.185.22\nLoadBalancer Ingress:     10.42.162.216\nPort:                     <unset>  809/TCP\nTargetPort:               809/TCP\nNodePort:                 <unset>  39746/TCP\nEndpoints:                10.42.147.255:809,10.42.38.222:809\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>\n```\n\n即 `cluster ip` 为 `172.17.185.22` 后端 `podip` 为 `10.42.147.25510.42.38.222`\n此外还有1个 `loadbalancer ip` `10.42.162.216`\n\n### svc的访问路径\n\n    - 集群内部，通过 `clusterip` 到访问到后端 `pod\n    - 集群外部，通过直接访问`nodeport`；或者通过 `elb` 负载均衡到 `node` 上再通过 `nodeport` 访问\n\n### cluster ip 的基本原理\n\n如果是集群内的应用访问 cluster ip，那就是从**用户空间**访问**内核空间网络协议栈**,走的是 `OUTPUT` 链\n\n1. 从`OUTPUT` 链开始\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL OUTPUT\nChain OUTPUT (policy ACCEPT 4 packets, 240 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n3424K  209M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */\n\n```\n2. `OUTPUT`下的规则 直接把流量交给 `KUBE-SERVICES` 链\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICES\nChain KUBE-SERVICES (2 references)\n pkts bytes target     prot opt in     out     source               destination     \n    0     0 KUBE-MARK-MASQ  tcp  --  *      *      0.0.0.0/0        172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809\n    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809\n   10   520 KUBE-FW-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            10.42.162.216        /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809\n     0     0 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTY\n```\n上述3条规则是顺序执行的：\n\n- 第1条规则匹配发往 `Cluster IP` `172.17.185.22` 的流量，跳转到了 `KUBE-MARK-MASQ` 链进一步处理，其作用就是打了一个 `MARK` ，稍后展开说明。\n- 第2条规则匹配发往 `Cluster IP` `172.17.185.22` 的流量，跳转到了 `KUBE-SVC-G3OM5DSD2HHDMN6U` 链进一步处理，稍后展开说明。\n- 第3条规则匹配发往集群外 `LB IP` 的 `10.42.162.216` 的流量，跳转到了\n`KUBE-FW-G3OM5DSD2HHDMN6U` 链进一步处理，稍后展开说明。\n- 第4条  KUBE-NODEPORTS的规则在末尾，只要dst ip是node 本机ip的话 （（–dst-type LOCAL），就跳转到KUBE-NODEPORTS做进一步判定：）\n    **第2条规则要做dnat转发到后端具体的后端pod上**\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SVC-G3OM5DSD2HHDMN6U\nChain KUBE-SVC-G3OM5DSD2HHDMN6U (3 references)\n pkts bytes target     prot opt in     out     source               destination         \n   18   936 KUBE-SEP-JT2KW6YUTVPLLGV6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000\n   21  1092 KUBE-SEP-VETLC6CJY2HOK3EL  all  --  *      *       0.0.0.0/0            0.0.0.0/0\n```\n\n**两条 对应 后端pod的链**\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-JT2KW6YUTVPLLGV6\nChain KUBE-SEP-JT2KW6YUTVPLLGV6 (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 KUBE-MARK-MASQ  all  --  *      *       10.42.147.255        0.0.0.0/0           \n   26  1352 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.42.147.255:809\n\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-VETLC6CJY2HOK3EL\nChain KUBE-SEP-VETLC6CJY2HOK3EL (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 KUBE-MARK-MASQ  all  --  *      *       10.42.38.222         0.0.0.0/0           \n    2   104 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.42.38.222:809\n```\n\n流量经过路由表从eth0出去，在流量流出本机之前会经过POSTROUTING 链\n\n### 在流量离开本机的时候会经过 `POSTROUTING` 链\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL POSTROUTING\nChain POSTROUTING (policy ACCEPT 274 packets, 17340 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 632M   36G KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */\n\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-POSTROUTING\nChain KUBE-POSTROUTING (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n  526 27352 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ mark match 0x4000/0x4000\n```\n\n其实直接就跳转到了 `KUBE-POSTROUTING`，然后匹配打过`0x4000 MARK` 的流量，将其做 `SNAT` 转换，而这个 `MARK` 其实就是之前没说的 `KUBE-MARK-MASQ` 做的事情\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-MARK-MASQ\nChain KUBE-MARK-MASQ (183 references)\n pkts bytes target     prot opt in     out     source               destination         \n  492 25604 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000\n```\n当流量离开本机时，src IP会被修改为node的IP，而不是发出流量的POD IP了\n\n### 通过loadbalance ip进行访问\n\n最后还有一个KUBE-FW-G3OM5DSD2HHDMN6U链没有讲，从本机发往LB IP的流量要做啥事情呢？\n\n**其实也是让流量直接发往具体某个Endpoints，就别真的发往LB了，这样才能获得最佳的延迟**：\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-FW-G3OM5DSD2HHDMN6U\nChain KUBE-FW-G3OM5DSD2HHDMN6U (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    2   104 KUBE-MARK-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */\n    2   104 KUBE-SVC-G3OM5DSD2HHDMN6U  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */\n    0     0 KUBE-MARK-DROP  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */\n```\n\n### 通过nodeport 来访问\n\n回顾一下 KUBE_SERVICES规则\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICES\nChain KUBE-SERVICES (2 references)\n pkts bytes target     prot opt in     out     source               destination     \n    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !172.17.0.0/16        172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809\n    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809\n   10   520 KUBE-FW-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            10.42.162.216        /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809\n    0     0 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL\n```\n**`KUBE-NODEPORTS` 是最后一条规则**\n\n```shell\n[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-NODEPORTS\nChain KUBE-NODEPORTS (1 references)\n pkts bytes target     prot opt in     out     source               destination        \n    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746\n    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746\n```\n第1条匹配dst port如果是39746，那么就打mark。\n第2条匹配dst port如果是39746，那么就跳到负载均衡链做DNAT改写。\n\n\n### 总结\n\n- `KUBE-SERVICES` 链的规则存在于 `OUTPUT POSTROUTING PREROUTING` 三个链上\n- 对于 `KUBE-SERVICES KUBE-NDOEPORTS-xxx KUBE-SEP-xxx` 下都会对符合条件（匹配条件）的规则打上MARK 可以重复打MARK\n- 在流量出node的时候做SNAT\n\n\n## 从集群内出去的流量怎么回来\n\n出node的流量在做SNAT的时候，netfilter有个连接跟踪机制，保存在 conntrack记录中\n\n这就是Netfilter的连接跟踪（conntrack）功能了。对于TCP协议来讲，肯定是上来先建立一个连接，可以用`源/目的IP+源/目的端口`  （四元组），唯一标识一条连接，这个连接会放在conntrack表里面。\n当时是这台机器去请求163网站的，虽然源地址已经Snat成公网IP地址了，但是 `conntrack` 表里面还是有这个连接的记录的。当163网站返回数据的时候，会找到记录，从而找到正确的私网IP地\n址。\n\n## 参考文档\n[k8s 的iptales规则详解](https://yuerblog.cc/2019/12/09/k8s-%E6%89%8B%E6%8A%8A%E6%89%8B%E5%88%86%E6%9E%90service%E7%94%9F%E6%88%90%E7%9A%84iptables%E8%A7%84%E5%88%99/)\n\n","slug":"k8s-iptables","published":1,"updated":"2020-05-19T13:24:32.496Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l8g000ir5flmmi6e30s","content":"<h2 id=\"iptables规则\"><a href=\"#iptables规则\" class=\"headerlink\" title=\"iptables规则\"></a>iptables规则</h2><blockquote>\n<p>参考 <a href=\"http://www.zsythink.net/archives/tag/iptables/\" target=\"_blank\" rel=\"noopener\">http://www.zsythink.net/archives/tag/iptables/</a></p>\n</blockquote>\n<p><img src=\"http://www.zsythink.net/wp-content/uploads/2017/02/021217_0051_2.png\" alt=\"\"></p>\n<p><strong>用户空间，例如从pod中流出的流量就是从ouput链流出</strong></p>\n<p>上图表示的<code>iptables</code>的链，链 和表的关系如下，以<code>PREROUTING</code>链为例</p>\n<p><img src=\"http://www.zsythink.net/wp-content/uploads/2017/02/021217_0051_4.png\" alt=\"\"></p>\n<p>这幅图是什么意思呢？它的意思是说，prerouting”链”只拥有nat表、raw表和mangle表所对应的功能，所以，prerouting中的规则只能存放于nat表、raw表和mangle表中。</p>\n<h2 id=\"NAT\"><a href=\"#NAT\" class=\"headerlink\" title=\"NAT\"></a>NAT</h2><p><strong>NAT的三种类型:</strong></p>\n<ul>\n<li><p>SNAT  </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">iptables -t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 目标流向eth0，源地址是xxx的，做SNAT，源地址改为xxx</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>DNAT</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">iptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3-192.168.5.5</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>MASQUERADE 是SNAT的一种，可以自动获取网卡的ip来做SNAT，如果是ADSL这种动态ip的，如果用SNAT需要经常更改iptables规则 </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">iptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j MASQUERADE</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 源地址是xxx，流向eth0的，流向做自动化SNAT</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><em>masquerade</em> 应为英文伪装</p>\n<h2 id=\"iptabels-常用命令\"><a href=\"#iptabels-常用命令\" class=\"headerlink\" title=\"iptabels 常用命令\"></a>iptabels 常用命令</h2><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">iptables [-t 表名] 管理选项 [链名] [匹配条件] [-j 控制类型]</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 控制类型包括 ACCETP REJECT DROP LOG 还有自定义的链（k8s的链）等</span></span><br><span class=\"line\">iptabels -t nat（表名） -nvL POSTROUTING(链的名字)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://www.jianshu.com/p/ee4ee15d3658\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/ee4ee15d3658</a></p>\n<h2 id=\"分析k8s下的iptables规则\"><a href=\"#分析k8s下的iptables规则\" class=\"headerlink\" title=\"分析k8s下的iptables规则\"></a>分析k8s下的iptables规则</h2><p>以如下 <code>service</code> 为例 </p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Name:</span>                     <span class=\"string\">testapi-smzdm-com</span></span><br><span class=\"line\"><span class=\"attr\">Namespace:</span>                <span class=\"string\">zhongce-v2-0</span></span><br><span class=\"line\"><span class=\"attr\">Labels:</span>                   <span class=\"string\">&lt;none&gt;</span></span><br><span class=\"line\"><span class=\"attr\">Selector:</span>                 <span class=\"string\">zdm-app-owner=testapi-smzdm-com</span></span><br><span class=\"line\"><span class=\"attr\">Type:</span>                     <span class=\"string\">LoadBalancer</span></span><br><span class=\"line\"><span class=\"attr\">IP:</span>                       <span class=\"number\">172.17</span><span class=\"number\">.185</span><span class=\"number\">.22</span></span><br><span class=\"line\"><span class=\"string\">LoadBalancer</span> <span class=\"attr\">Ingress:</span>     <span class=\"number\">10.42</span><span class=\"number\">.162</span><span class=\"number\">.216</span></span><br><span class=\"line\"><span class=\"attr\">Port:</span>                     <span class=\"string\">&lt;unset&gt;</span>  <span class=\"number\">809</span><span class=\"string\">/TCP</span></span><br><span class=\"line\"><span class=\"attr\">TargetPort:</span>               <span class=\"number\">809</span><span class=\"string\">/TCP</span></span><br><span class=\"line\"><span class=\"attr\">NodePort:</span>                 <span class=\"string\">&lt;unset&gt;</span>  <span class=\"number\">39746</span><span class=\"string\">/TCP</span></span><br><span class=\"line\"><span class=\"attr\">Endpoints:</span>                <span class=\"number\">10.42</span><span class=\"number\">.147</span><span class=\"number\">.255</span><span class=\"string\">:809,10.42.38.222:809</span></span><br><span class=\"line\"><span class=\"string\">Session</span> <span class=\"attr\">Affinity:</span>         <span class=\"string\">None</span></span><br><span class=\"line\"><span class=\"string\">External</span> <span class=\"string\">Traffic</span> <span class=\"attr\">Policy:</span>  <span class=\"string\">Cluster</span></span><br><span class=\"line\"><span class=\"attr\">Events:</span>                   <span class=\"string\">&lt;none&gt;</span></span><br></pre></td></tr></table></figure>\n<p>即 <code>cluster ip</code> 为 <code>172.17.185.22</code> 后端 <code>podip</code> 为 <code>10.42.147.25510.42.38.222</code><br>此外还有1个 <code>loadbalancer ip</code> <code>10.42.162.216</code></p>\n<h3 id=\"svc的访问路径\"><a href=\"#svc的访问路径\" class=\"headerlink\" title=\"svc的访问路径\"></a>svc的访问路径</h3><pre><code>- 集群内部，通过 `clusterip` 到访问到后端 `pod\n- 集群外部，通过直接访问`nodeport`；或者通过 `elb` 负载均衡到 `node` 上再通过 `nodeport` 访问\n</code></pre><h3 id=\"cluster-ip-的基本原理\"><a href=\"#cluster-ip-的基本原理\" class=\"headerlink\" title=\"cluster ip 的基本原理\"></a>cluster ip 的基本原理</h3><p>如果是集群内的应用访问 cluster ip，那就是从<strong>用户空间</strong>访问<strong>内核空间网络协议栈</strong>,走的是 <code>OUTPUT</code> 链</p>\n<ol>\n<li>从<code>OUTPUT</code> 链开始</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL OUTPUT</span><br><span class=\"line\">Chain OUTPUT (policy ACCEPT 4 packets, 240 bytes)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">3424K  209M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li><code>OUTPUT</code>下的规则 直接把流量交给 <code>KUBE-SERVICES</code> 链</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICES</span><br><span class=\"line\">Chain KUBE-SERVICES (2 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination     </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  tcp  --  *      *      0.0.0.0/0        172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809</span><br><span class=\"line\">    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809</span><br><span class=\"line\">   10   520 KUBE-FW-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            10.42.162.216        /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809</span><br><span class=\"line\">     0     0 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTY</span><br></pre></td></tr></table></figure>\n<p>上述3条规则是顺序执行的：</p>\n<ul>\n<li>第1条规则匹配发往 <code>Cluster IP</code> <code>172.17.185.22</code> 的流量，跳转到了 <code>KUBE-MARK-MASQ</code> 链进一步处理，其作用就是打了一个 <code>MARK</code> ，稍后展开说明。</li>\n<li>第2条规则匹配发往 <code>Cluster IP</code> <code>172.17.185.22</code> 的流量，跳转到了 <code>KUBE-SVC-G3OM5DSD2HHDMN6U</code> 链进一步处理，稍后展开说明。</li>\n<li>第3条规则匹配发往集群外 <code>LB IP</code> 的 <code>10.42.162.216</code> 的流量，跳转到了<br><code>KUBE-FW-G3OM5DSD2HHDMN6U</code> 链进一步处理，稍后展开说明。</li>\n<li>第4条  KUBE-NODEPORTS的规则在末尾，只要dst ip是node 本机ip的话 （（–dst-type LOCAL），就跳转到KUBE-NODEPORTS做进一步判定：）<br>  <strong>第2条规则要做dnat转发到后端具体的后端pod上</strong></li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SVC-G3OM5DSD2HHDMN6U</span><br><span class=\"line\">Chain KUBE-SVC-G3OM5DSD2HHDMN6U (3 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">   18   936 KUBE-SEP-JT2KW6YUTVPLLGV6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000</span><br><span class=\"line\">   21  1092 KUBE-SEP-VETLC6CJY2HOK3EL  all  --  *      *       0.0.0.0/0            0.0.0.0/0</span><br></pre></td></tr></table></figure>\n<p><strong>两条 对应 后端pod的链</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-JT2KW6YUTVPLLGV6</span><br><span class=\"line\">Chain KUBE-SEP-JT2KW6YUTVPLLGV6 (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  all  --  *      *       10.42.147.255        0.0.0.0/0           </span><br><span class=\"line\">   26  1352 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.42.147.255:809</span><br><span class=\"line\"></span><br><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-VETLC6CJY2HOK3EL</span><br><span class=\"line\">Chain KUBE-SEP-VETLC6CJY2HOK3EL (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  all  --  *      *       10.42.38.222         0.0.0.0/0           </span><br><span class=\"line\">    2   104 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.42.38.222:809</span><br></pre></td></tr></table></figure>\n<p>流量经过路由表从eth0出去，在流量流出本机之前会经过POSTROUTING 链</p>\n<h3 id=\"在流量离开本机的时候会经过-POSTROUTING-链\"><a href=\"#在流量离开本机的时候会经过-POSTROUTING-链\" class=\"headerlink\" title=\"在流量离开本机的时候会经过 POSTROUTING 链\"></a>在流量离开本机的时候会经过 <code>POSTROUTING</code> 链</h3><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL POSTROUTING</span><br><span class=\"line\">Chain POSTROUTING (policy ACCEPT 274 packets, 17340 bytes)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\"> 632M   36G KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */</span><br><span class=\"line\"></span><br><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-POSTROUTING</span><br><span class=\"line\">Chain KUBE-POSTROUTING (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">  526 27352 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ mark match 0x4000/0x4000</span><br></pre></td></tr></table></figure>\n<p>其实直接就跳转到了 <code>KUBE-POSTROUTING</code>，然后匹配打过<code>0x4000 MARK</code> 的流量，将其做 <code>SNAT</code> 转换，而这个 <code>MARK</code> 其实就是之前没说的 <code>KUBE-MARK-MASQ</code> 做的事情</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-MARK-MASQ</span><br><span class=\"line\">Chain KUBE-MARK-MASQ (183 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">  492 25604 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000</span><br></pre></td></tr></table></figure>\n<p>当流量离开本机时，src IP会被修改为node的IP，而不是发出流量的POD IP了</p>\n<h3 id=\"通过loadbalance-ip进行访问\"><a href=\"#通过loadbalance-ip进行访问\" class=\"headerlink\" title=\"通过loadbalance ip进行访问\"></a>通过loadbalance ip进行访问</h3><p>最后还有一个KUBE-FW-G3OM5DSD2HHDMN6U链没有讲，从本机发往LB IP的流量要做啥事情呢？</p>\n<p><strong>其实也是让流量直接发往具体某个Endpoints，就别真的发往LB了，这样才能获得最佳的延迟</strong>：<br><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-FW-G3OM5DSD2HHDMN6U</span><br><span class=\"line\">Chain KUBE-FW-G3OM5DSD2HHDMN6U (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">    2   104 KUBE-MARK-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */</span><br><span class=\"line\">    2   104 KUBE-SVC-G3OM5DSD2HHDMN6U  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */</span><br><span class=\"line\">    0     0 KUBE-MARK-DROP  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"通过nodeport-来访问\"><a href=\"#通过nodeport-来访问\" class=\"headerlink\" title=\"通过nodeport 来访问\"></a>通过nodeport 来访问</h3><p>回顾一下 KUBE_SERVICES规则</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICES</span><br><span class=\"line\">Chain KUBE-SERVICES (2 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination     </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !172.17.0.0/16        172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809</span><br><span class=\"line\">    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809</span><br><span class=\"line\">   10   520 KUBE-FW-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            10.42.162.216        /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809</span><br><span class=\"line\">    0     0 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>\n<p><strong><code>KUBE-NODEPORTS</code> 是最后一条规则</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-NODEPORTS</span><br><span class=\"line\">Chain KUBE-NODEPORTS (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination        </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746</span><br><span class=\"line\">    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746</span><br></pre></td></tr></table></figure>\n<p>第1条匹配dst port如果是39746，那么就打mark。<br>第2条匹配dst port如果是39746，那么就跳到负载均衡链做DNAT改写。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><ul>\n<li><code>KUBE-SERVICES</code> 链的规则存在于 <code>OUTPUT POSTROUTING PREROUTING</code> 三个链上</li>\n<li>对于 <code>KUBE-SERVICES KUBE-NDOEPORTS-xxx KUBE-SEP-xxx</code> 下都会对符合条件（匹配条件）的规则打上MARK 可以重复打MARK</li>\n<li>在流量出node的时候做SNAT</li>\n</ul>\n<h2 id=\"从集群内出去的流量怎么回来\"><a href=\"#从集群内出去的流量怎么回来\" class=\"headerlink\" title=\"从集群内出去的流量怎么回来\"></a>从集群内出去的流量怎么回来</h2><p>出node的流量在做SNAT的时候，netfilter有个连接跟踪机制，保存在 conntrack记录中</p>\n<p>这就是Netfilter的连接跟踪（conntrack）功能了。对于TCP协议来讲，肯定是上来先建立一个连接，可以用<code>源/目的IP+源/目的端口</code>  （四元组），唯一标识一条连接，这个连接会放在conntrack表里面。<br>当时是这台机器去请求163网站的，虽然源地址已经Snat成公网IP地址了，但是 <code>conntrack</code> 表里面还是有这个连接的记录的。当163网站返回数据的时候，会找到记录，从而找到正确的私网IP地<br>址。</p>\n<h2 id=\"参考文档\"><a href=\"#参考文档\" class=\"headerlink\" title=\"参考文档\"></a>参考文档</h2><p><a href=\"https://yuerblog.cc/2019/12/09/k8s-%E6%89%8B%E6%8A%8A%E6%89%8B%E5%88%86%E6%9E%90service%E7%94%9F%E6%88%90%E7%9A%84iptables%E8%A7%84%E5%88%99/\" target=\"_blank\" rel=\"noopener\">k8s 的iptales规则详解</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"iptables规则\"><a href=\"#iptables规则\" class=\"headerlink\" title=\"iptables规则\"></a>iptables规则</h2><blockquote>\n<p>参考 <a href=\"http://www.zsythink.net/archives/tag/iptables/\" target=\"_blank\" rel=\"noopener\">http://www.zsythink.net/archives/tag/iptables/</a></p>\n</blockquote>\n<p><img src=\"http://www.zsythink.net/wp-content/uploads/2017/02/021217_0051_2.png\" alt=\"\"></p>\n<p><strong>用户空间，例如从pod中流出的流量就是从ouput链流出</strong></p>\n<p>上图表示的<code>iptables</code>的链，链 和表的关系如下，以<code>PREROUTING</code>链为例</p>\n<p><img src=\"http://www.zsythink.net/wp-content/uploads/2017/02/021217_0051_4.png\" alt=\"\"></p>\n<p>这幅图是什么意思呢？它的意思是说，prerouting”链”只拥有nat表、raw表和mangle表所对应的功能，所以，prerouting中的规则只能存放于nat表、raw表和mangle表中。</p>\n<h2 id=\"NAT\"><a href=\"#NAT\" class=\"headerlink\" title=\"NAT\"></a>NAT</h2><p><strong>NAT的三种类型:</strong></p>\n<ul>\n<li><p>SNAT  </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">iptables -t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 目标流向eth0，源地址是xxx的，做SNAT，源地址改为xxx</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>DNAT</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">iptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3-192.168.5.5</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>MASQUERADE 是SNAT的一种，可以自动获取网卡的ip来做SNAT，如果是ADSL这种动态ip的，如果用SNAT需要经常更改iptables规则 </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">iptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j MASQUERADE</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 源地址是xxx，流向eth0的，流向做自动化SNAT</span></span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p><em>masquerade</em> 应为英文伪装</p>\n<h2 id=\"iptabels-常用命令\"><a href=\"#iptabels-常用命令\" class=\"headerlink\" title=\"iptabels 常用命令\"></a>iptabels 常用命令</h2><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">iptables [-t 表名] 管理选项 [链名] [匹配条件] [-j 控制类型]</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 控制类型包括 ACCETP REJECT DROP LOG 还有自定义的链（k8s的链）等</span></span><br><span class=\"line\">iptabels -t nat（表名） -nvL POSTROUTING(链的名字)</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://www.jianshu.com/p/ee4ee15d3658\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/ee4ee15d3658</a></p>\n<h2 id=\"分析k8s下的iptables规则\"><a href=\"#分析k8s下的iptables规则\" class=\"headerlink\" title=\"分析k8s下的iptables规则\"></a>分析k8s下的iptables规则</h2><p>以如下 <code>service</code> 为例 </p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Name:</span>                     <span class=\"string\">testapi-smzdm-com</span></span><br><span class=\"line\"><span class=\"attr\">Namespace:</span>                <span class=\"string\">zhongce-v2-0</span></span><br><span class=\"line\"><span class=\"attr\">Labels:</span>                   <span class=\"string\">&lt;none&gt;</span></span><br><span class=\"line\"><span class=\"attr\">Selector:</span>                 <span class=\"string\">zdm-app-owner=testapi-smzdm-com</span></span><br><span class=\"line\"><span class=\"attr\">Type:</span>                     <span class=\"string\">LoadBalancer</span></span><br><span class=\"line\"><span class=\"attr\">IP:</span>                       <span class=\"number\">172.17</span><span class=\"number\">.185</span><span class=\"number\">.22</span></span><br><span class=\"line\"><span class=\"string\">LoadBalancer</span> <span class=\"attr\">Ingress:</span>     <span class=\"number\">10.42</span><span class=\"number\">.162</span><span class=\"number\">.216</span></span><br><span class=\"line\"><span class=\"attr\">Port:</span>                     <span class=\"string\">&lt;unset&gt;</span>  <span class=\"number\">809</span><span class=\"string\">/TCP</span></span><br><span class=\"line\"><span class=\"attr\">TargetPort:</span>               <span class=\"number\">809</span><span class=\"string\">/TCP</span></span><br><span class=\"line\"><span class=\"attr\">NodePort:</span>                 <span class=\"string\">&lt;unset&gt;</span>  <span class=\"number\">39746</span><span class=\"string\">/TCP</span></span><br><span class=\"line\"><span class=\"attr\">Endpoints:</span>                <span class=\"number\">10.42</span><span class=\"number\">.147</span><span class=\"number\">.255</span><span class=\"string\">:809,10.42.38.222:809</span></span><br><span class=\"line\"><span class=\"string\">Session</span> <span class=\"attr\">Affinity:</span>         <span class=\"string\">None</span></span><br><span class=\"line\"><span class=\"string\">External</span> <span class=\"string\">Traffic</span> <span class=\"attr\">Policy:</span>  <span class=\"string\">Cluster</span></span><br><span class=\"line\"><span class=\"attr\">Events:</span>                   <span class=\"string\">&lt;none&gt;</span></span><br></pre></td></tr></table></figure>\n<p>即 <code>cluster ip</code> 为 <code>172.17.185.22</code> 后端 <code>podip</code> 为 <code>10.42.147.25510.42.38.222</code><br>此外还有1个 <code>loadbalancer ip</code> <code>10.42.162.216</code></p>\n<h3 id=\"svc的访问路径\"><a href=\"#svc的访问路径\" class=\"headerlink\" title=\"svc的访问路径\"></a>svc的访问路径</h3><pre><code>- 集群内部，通过 `clusterip` 到访问到后端 `pod\n- 集群外部，通过直接访问`nodeport`；或者通过 `elb` 负载均衡到 `node` 上再通过 `nodeport` 访问\n</code></pre><h3 id=\"cluster-ip-的基本原理\"><a href=\"#cluster-ip-的基本原理\" class=\"headerlink\" title=\"cluster ip 的基本原理\"></a>cluster ip 的基本原理</h3><p>如果是集群内的应用访问 cluster ip，那就是从<strong>用户空间</strong>访问<strong>内核空间网络协议栈</strong>,走的是 <code>OUTPUT</code> 链</p>\n<ol>\n<li>从<code>OUTPUT</code> 链开始</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL OUTPUT</span><br><span class=\"line\">Chain OUTPUT (policy ACCEPT 4 packets, 240 bytes)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">3424K  209M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li><code>OUTPUT</code>下的规则 直接把流量交给 <code>KUBE-SERVICES</code> 链</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICES</span><br><span class=\"line\">Chain KUBE-SERVICES (2 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination     </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  tcp  --  *      *      0.0.0.0/0        172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809</span><br><span class=\"line\">    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809</span><br><span class=\"line\">   10   520 KUBE-FW-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            10.42.162.216        /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809</span><br><span class=\"line\">     0     0 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTY</span><br></pre></td></tr></table></figure>\n<p>上述3条规则是顺序执行的：</p>\n<ul>\n<li>第1条规则匹配发往 <code>Cluster IP</code> <code>172.17.185.22</code> 的流量，跳转到了 <code>KUBE-MARK-MASQ</code> 链进一步处理，其作用就是打了一个 <code>MARK</code> ，稍后展开说明。</li>\n<li>第2条规则匹配发往 <code>Cluster IP</code> <code>172.17.185.22</code> 的流量，跳转到了 <code>KUBE-SVC-G3OM5DSD2HHDMN6U</code> 链进一步处理，稍后展开说明。</li>\n<li>第3条规则匹配发往集群外 <code>LB IP</code> 的 <code>10.42.162.216</code> 的流量，跳转到了<br><code>KUBE-FW-G3OM5DSD2HHDMN6U</code> 链进一步处理，稍后展开说明。</li>\n<li>第4条  KUBE-NODEPORTS的规则在末尾，只要dst ip是node 本机ip的话 （（–dst-type LOCAL），就跳转到KUBE-NODEPORTS做进一步判定：）<br>  <strong>第2条规则要做dnat转发到后端具体的后端pod上</strong></li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SVC-G3OM5DSD2HHDMN6U</span><br><span class=\"line\">Chain KUBE-SVC-G3OM5DSD2HHDMN6U (3 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">   18   936 KUBE-SEP-JT2KW6YUTVPLLGV6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000</span><br><span class=\"line\">   21  1092 KUBE-SEP-VETLC6CJY2HOK3EL  all  --  *      *       0.0.0.0/0            0.0.0.0/0</span><br></pre></td></tr></table></figure>\n<p><strong>两条 对应 后端pod的链</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-JT2KW6YUTVPLLGV6</span><br><span class=\"line\">Chain KUBE-SEP-JT2KW6YUTVPLLGV6 (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  all  --  *      *       10.42.147.255        0.0.0.0/0           </span><br><span class=\"line\">   26  1352 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.42.147.255:809</span><br><span class=\"line\"></span><br><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-VETLC6CJY2HOK3EL</span><br><span class=\"line\">Chain KUBE-SEP-VETLC6CJY2HOK3EL (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  all  --  *      *       10.42.38.222         0.0.0.0/0           </span><br><span class=\"line\">    2   104 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.42.38.222:809</span><br></pre></td></tr></table></figure>\n<p>流量经过路由表从eth0出去，在流量流出本机之前会经过POSTROUTING 链</p>\n<h3 id=\"在流量离开本机的时候会经过-POSTROUTING-链\"><a href=\"#在流量离开本机的时候会经过-POSTROUTING-链\" class=\"headerlink\" title=\"在流量离开本机的时候会经过 POSTROUTING 链\"></a>在流量离开本机的时候会经过 <code>POSTROUTING</code> 链</h3><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL POSTROUTING</span><br><span class=\"line\">Chain POSTROUTING (policy ACCEPT 274 packets, 17340 bytes)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\"> 632M   36G KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */</span><br><span class=\"line\"></span><br><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-POSTROUTING</span><br><span class=\"line\">Chain KUBE-POSTROUTING (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">  526 27352 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ mark match 0x4000/0x4000</span><br></pre></td></tr></table></figure>\n<p>其实直接就跳转到了 <code>KUBE-POSTROUTING</code>，然后匹配打过<code>0x4000 MARK</code> 的流量，将其做 <code>SNAT</code> 转换，而这个 <code>MARK</code> 其实就是之前没说的 <code>KUBE-MARK-MASQ</code> 做的事情</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-MARK-MASQ</span><br><span class=\"line\">Chain KUBE-MARK-MASQ (183 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">  492 25604 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000</span><br></pre></td></tr></table></figure>\n<p>当流量离开本机时，src IP会被修改为node的IP，而不是发出流量的POD IP了</p>\n<h3 id=\"通过loadbalance-ip进行访问\"><a href=\"#通过loadbalance-ip进行访问\" class=\"headerlink\" title=\"通过loadbalance ip进行访问\"></a>通过loadbalance ip进行访问</h3><p>最后还有一个KUBE-FW-G3OM5DSD2HHDMN6U链没有讲，从本机发往LB IP的流量要做啥事情呢？</p>\n<p><strong>其实也是让流量直接发往具体某个Endpoints，就别真的发往LB了，这样才能获得最佳的延迟</strong>：<br><figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-FW-G3OM5DSD2HHDMN6U</span><br><span class=\"line\">Chain KUBE-FW-G3OM5DSD2HHDMN6U (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class=\"line\">    2   104 KUBE-MARK-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */</span><br><span class=\"line\">    2   104 KUBE-SVC-G3OM5DSD2HHDMN6U  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */</span><br><span class=\"line\">    0     0 KUBE-MARK-DROP  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"通过nodeport-来访问\"><a href=\"#通过nodeport-来访问\" class=\"headerlink\" title=\"通过nodeport 来访问\"></a>通过nodeport 来访问</h3><p>回顾一下 KUBE_SERVICES规则</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICES</span><br><span class=\"line\">Chain KUBE-SERVICES (2 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination     </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !172.17.0.0/16        172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809</span><br><span class=\"line\">    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            172.17.185.22        /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809</span><br><span class=\"line\">   10   520 KUBE-FW-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            10.42.162.216        /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809</span><br><span class=\"line\">    0     0 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure>\n<p><strong><code>KUBE-NODEPORTS</code> 是最后一条规则</strong></p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-NODEPORTS</span><br><span class=\"line\">Chain KUBE-NODEPORTS (1 references)</span><br><span class=\"line\"> pkts bytes target     prot opt in     out     source               destination        </span><br><span class=\"line\">    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746</span><br><span class=\"line\">    0     0 KUBE-SVC-G3OM5DSD2HHDMN6U  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746</span><br></pre></td></tr></table></figure>\n<p>第1条匹配dst port如果是39746，那么就打mark。<br>第2条匹配dst port如果是39746，那么就跳到负载均衡链做DNAT改写。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><ul>\n<li><code>KUBE-SERVICES</code> 链的规则存在于 <code>OUTPUT POSTROUTING PREROUTING</code> 三个链上</li>\n<li>对于 <code>KUBE-SERVICES KUBE-NDOEPORTS-xxx KUBE-SEP-xxx</code> 下都会对符合条件（匹配条件）的规则打上MARK 可以重复打MARK</li>\n<li>在流量出node的时候做SNAT</li>\n</ul>\n<h2 id=\"从集群内出去的流量怎么回来\"><a href=\"#从集群内出去的流量怎么回来\" class=\"headerlink\" title=\"从集群内出去的流量怎么回来\"></a>从集群内出去的流量怎么回来</h2><p>出node的流量在做SNAT的时候，netfilter有个连接跟踪机制，保存在 conntrack记录中</p>\n<p>这就是Netfilter的连接跟踪（conntrack）功能了。对于TCP协议来讲，肯定是上来先建立一个连接，可以用<code>源/目的IP+源/目的端口</code>  （四元组），唯一标识一条连接，这个连接会放在conntrack表里面。<br>当时是这台机器去请求163网站的，虽然源地址已经Snat成公网IP地址了，但是 <code>conntrack</code> 表里面还是有这个连接的记录的。当163网站返回数据的时候，会找到记录，从而找到正确的私网IP地<br>址。</p>\n<h2 id=\"参考文档\"><a href=\"#参考文档\" class=\"headerlink\" title=\"参考文档\"></a>参考文档</h2><p><a href=\"https://yuerblog.cc/2019/12/09/k8s-%E6%89%8B%E6%8A%8A%E6%89%8B%E5%88%86%E6%9E%90service%E7%94%9F%E6%88%90%E7%9A%84iptables%E8%A7%84%E5%88%99/\" target=\"_blank\" rel=\"noopener\">k8s 的iptales规则详解</a></p>\n"},{"layout":"post","title":"Go 内存管理","date":"2019-11-12T10:16:18.000Z","author":"zhaojizhuang","_content":"\n\n\n Go这门语言抛弃了C/C++中的开发者管理内存的方式：主动申请与主动释放，增加了**逃逸分析和GC**，将开发者从内存管理中释放出来，让开发者有更多的精力去关注软件设计，而不是底层的内存问题。这是Go语言成为高生产力语言的原因之一  引自【 [Go内存分配那些事，就这么简单！]】\n\n## 堆内存的分配\n\n\n先看下面这段代码，思考下 `smallStruct` 会被分配在堆上还是栈上:\n\n```go\npackage main\n\ntype smallStruct struct {\n   a, b int64\n   c, d float64\n}\n\nfunc main() {\n   smallAllocation()\n}\n\n//go:noinline\nfunc smallAllocation() *smallStruct {\n   return &smallStruct{}\n}\n```\n\n通过 `annotation //go:noinline` 禁用内联函数，不然这里不会产生堆内存的分配 **【逃逸分析】**\n\n**Inline 内联**: 是在编译期间发生的，将函数调用调用处替换为被调用函数主体的一种编译器优化手段。\n\n将文件保存为 `main.go`, 并执行 `go tool compile \"-m\" main.go` ,查看Go 堆内存的分配 **【逃逸分析】**的过程\n\n如果不加 `annotation //go:noinline` 可以用 \n`go build -gcflags '-m -l' main.go` `-l`可以禁止内联函数，效果是一样的，下面是逃逸分析的结果：\n\n```shell\nmain.go:14:9: &smallStruct literal escapes to heap\n```\n\n再来看这段代码生成的汇编指令来详细的展示内存分配的过程, 执行下面\n\n ```shell\ngo tool compile -S  main.go \n\n0x001d 00029 (main.go:14)   LEAQ   type.\"\".smallStruct(SB), AX\n0x0024 00036 (main.go:14)  PCDATA $0, $0\n0x0024 00036 (main.go:14)  MOVQ   AX, (SP)\n0x0028 00040 (main.go:14)  CALL   runtime.newobject(SB)\n```\n\n`runtime.newobject` 是 `Go` 内置的申请堆内存的函数，对于堆内存的分配，`Go` 中有两种策略: **大内存的分配和小内存的分配**\n\n### 小内存的分配\n\n#### 从 P 的 `mcache` 中分配\n\n对于小于 `32kb`的小内存，`Go` 会尝试在 `P` 的 本地缓存 `mcache` 中分配, `mcache` 保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以 **无锁访问**\n\n![](/images/neicun2.png)\n\n每个 `M` 绑定一个 `P` 来运行一个`goroutine`, 在分配内存时，当前的 `goroutine` 在当前 `P` 的本地缓存 `mcache` 中查找对应的span， 从 `span list` 中来查找第一个可用的空闲 `span`\n\n`span class` 分为 `8 bytes ~ 32k bytes` 共 66 种类型（还有个大小为0的 size class0，并未用到，用于大对象的堆内存分配），分别对应不同的内存大小,`mspan`里保存对应大小的object，\n\n1个 `size class` 对应2个 `span class`，2个 `span class` 的 `span` 大小相同，只是功能不同，1个用来存放包含指针的对象，一个用来存放不包含指针的对象，不包含指针对象的 `Span` 就无需 `GC` 扫描了。\n\n前面的例子里，struct的大小为 `(64bit/8)*4=8bit*4=32b` 所以会在 `span class` 大小为 `32bytes` 的 `mspan` 里分配\n\n![](/images/neicun3.png)\n\n#### 从全局的缓存  `mcentral` 中分配\n\n当 `mcache`中没有空闲的 `span` 时怎么办呢，Go 还维护了一个全局的缓存  `mcentral`,\n\n`mcentral` 和 `mcache` 一样，都134个 `span class` 级别(67个 `size class` )，但每个级别都保存了2个span list，即2个span链表：\n\n- nonempty：这个链表里的span，所有span都**至少有1个空闲的对象空间**。这些span是mcache释放span时加入到该链表的。\n- empty：这个链表里的span，所有的span都不确定里面是否有空闲的对象空间。当一个span交给mcache的时候，就会加入到empty链表。\n\n**mcache从mcentral获取和归还mspan的流程**：引自【 [图解Go语言内存分配|码农桃花源]】\n\n- 获取：加锁；从 `nonempty` 链表找到一个可用的 `mspan` ；并将其从 `nonempty` 链表删除；将取出的 `mspan` 加入到 `empty` 链表；将 `mspan` 返回给工作线程；解锁。\n- 归还：加锁；将 `mspan` 从 `empty` 链表删除；将 `mspan` 加入到 `nonempty` 链表；解锁。\n- \n另外，GC 扫描的时候会把部分 `mspan` 标记为未使用，并将对应的 `mspan` 加入到 `nonempty list` 中\n\n![](/images/nc4.png)\n\n`mcache` 从 `mcentral`获取 过程如下：\n\n![](/images/nc5.png)\n\n#### `mcentral ` 从 `heap` 中分配\n\n当 `mcentral` 中的 `nonempty list` 没有可分配的对象的时候，Go会从 `mheap` 中分配对象，并链接到 `nonempty list` 上,  `mheap` 必要时会向系统申请内存\n\n![](/images/nc6.png)\n\n`mheap` 中还有 `arenas` ,主要是为了大块内存需要，`arena` 也是用 `mspan` 组织的\n\n![](/images/nc7.png)\n\n### 大内存的分配\n\n大内存的分配就比较简单了，大于 `32kb` 的内存都会在 `mheap` 中直接分配\n\n![](/images/nc8.png)\n\n\n## 总结\n\ngo 内存分配的概览\n\n![](/images/nc9.png)\n\n\n----\n\n## 参考文章\n\n- Go内存分配那些事，就这么简单！:[https://lessisbetter.site/2019/07/06/go-memory-allocation](https://lessisbetter.site/2019/07/06/go-memory-allocation)\n- 图解Go语言内存分配|码农桃花源:[https://qcrao.com/2019/03/13/graphic-go-memory-allocation](https://qcrao.com/2019/03/13/graphic-go-memory-allocation)\n- 聊一聊goroutine stack：[https://zhuanlan.zhihu.com/p/28409657](https://zhuanlan.zhihu.com/p/28409657)\n\n\n[Go内存分配那些事，就这么简单！]:https://lessisbetter.site/2019/07/06/go-memory-allocation\n\n[图解Go语言内存分配|码农桃花源]:https://qcrao.com/2019/03/13/graphic-go-memory-allocation\n\n\n\n","source":"_posts/neicunguanliyufenpei.md","raw":"---\nlayout: post\ntitle:  \"Go 内存管理\"\ndate:   2019-11-12 18:16:18 +0800\ncategories: Go\ntags: [\"Go\",\"内存管理\"]\nauthor: zhaojizhuang\n---\n\n\n\n Go这门语言抛弃了C/C++中的开发者管理内存的方式：主动申请与主动释放，增加了**逃逸分析和GC**，将开发者从内存管理中释放出来，让开发者有更多的精力去关注软件设计，而不是底层的内存问题。这是Go语言成为高生产力语言的原因之一  引自【 [Go内存分配那些事，就这么简单！]】\n\n## 堆内存的分配\n\n\n先看下面这段代码，思考下 `smallStruct` 会被分配在堆上还是栈上:\n\n```go\npackage main\n\ntype smallStruct struct {\n   a, b int64\n   c, d float64\n}\n\nfunc main() {\n   smallAllocation()\n}\n\n//go:noinline\nfunc smallAllocation() *smallStruct {\n   return &smallStruct{}\n}\n```\n\n通过 `annotation //go:noinline` 禁用内联函数，不然这里不会产生堆内存的分配 **【逃逸分析】**\n\n**Inline 内联**: 是在编译期间发生的，将函数调用调用处替换为被调用函数主体的一种编译器优化手段。\n\n将文件保存为 `main.go`, 并执行 `go tool compile \"-m\" main.go` ,查看Go 堆内存的分配 **【逃逸分析】**的过程\n\n如果不加 `annotation //go:noinline` 可以用 \n`go build -gcflags '-m -l' main.go` `-l`可以禁止内联函数，效果是一样的，下面是逃逸分析的结果：\n\n```shell\nmain.go:14:9: &smallStruct literal escapes to heap\n```\n\n再来看这段代码生成的汇编指令来详细的展示内存分配的过程, 执行下面\n\n ```shell\ngo tool compile -S  main.go \n\n0x001d 00029 (main.go:14)   LEAQ   type.\"\".smallStruct(SB), AX\n0x0024 00036 (main.go:14)  PCDATA $0, $0\n0x0024 00036 (main.go:14)  MOVQ   AX, (SP)\n0x0028 00040 (main.go:14)  CALL   runtime.newobject(SB)\n```\n\n`runtime.newobject` 是 `Go` 内置的申请堆内存的函数，对于堆内存的分配，`Go` 中有两种策略: **大内存的分配和小内存的分配**\n\n### 小内存的分配\n\n#### 从 P 的 `mcache` 中分配\n\n对于小于 `32kb`的小内存，`Go` 会尝试在 `P` 的 本地缓存 `mcache` 中分配, `mcache` 保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以 **无锁访问**\n\n![](/images/neicun2.png)\n\n每个 `M` 绑定一个 `P` 来运行一个`goroutine`, 在分配内存时，当前的 `goroutine` 在当前 `P` 的本地缓存 `mcache` 中查找对应的span， 从 `span list` 中来查找第一个可用的空闲 `span`\n\n`span class` 分为 `8 bytes ~ 32k bytes` 共 66 种类型（还有个大小为0的 size class0，并未用到，用于大对象的堆内存分配），分别对应不同的内存大小,`mspan`里保存对应大小的object，\n\n1个 `size class` 对应2个 `span class`，2个 `span class` 的 `span` 大小相同，只是功能不同，1个用来存放包含指针的对象，一个用来存放不包含指针的对象，不包含指针对象的 `Span` 就无需 `GC` 扫描了。\n\n前面的例子里，struct的大小为 `(64bit/8)*4=8bit*4=32b` 所以会在 `span class` 大小为 `32bytes` 的 `mspan` 里分配\n\n![](/images/neicun3.png)\n\n#### 从全局的缓存  `mcentral` 中分配\n\n当 `mcache`中没有空闲的 `span` 时怎么办呢，Go 还维护了一个全局的缓存  `mcentral`,\n\n`mcentral` 和 `mcache` 一样，都134个 `span class` 级别(67个 `size class` )，但每个级别都保存了2个span list，即2个span链表：\n\n- nonempty：这个链表里的span，所有span都**至少有1个空闲的对象空间**。这些span是mcache释放span时加入到该链表的。\n- empty：这个链表里的span，所有的span都不确定里面是否有空闲的对象空间。当一个span交给mcache的时候，就会加入到empty链表。\n\n**mcache从mcentral获取和归还mspan的流程**：引自【 [图解Go语言内存分配|码农桃花源]】\n\n- 获取：加锁；从 `nonempty` 链表找到一个可用的 `mspan` ；并将其从 `nonempty` 链表删除；将取出的 `mspan` 加入到 `empty` 链表；将 `mspan` 返回给工作线程；解锁。\n- 归还：加锁；将 `mspan` 从 `empty` 链表删除；将 `mspan` 加入到 `nonempty` 链表；解锁。\n- \n另外，GC 扫描的时候会把部分 `mspan` 标记为未使用，并将对应的 `mspan` 加入到 `nonempty list` 中\n\n![](/images/nc4.png)\n\n`mcache` 从 `mcentral`获取 过程如下：\n\n![](/images/nc5.png)\n\n#### `mcentral ` 从 `heap` 中分配\n\n当 `mcentral` 中的 `nonempty list` 没有可分配的对象的时候，Go会从 `mheap` 中分配对象，并链接到 `nonempty list` 上,  `mheap` 必要时会向系统申请内存\n\n![](/images/nc6.png)\n\n`mheap` 中还有 `arenas` ,主要是为了大块内存需要，`arena` 也是用 `mspan` 组织的\n\n![](/images/nc7.png)\n\n### 大内存的分配\n\n大内存的分配就比较简单了，大于 `32kb` 的内存都会在 `mheap` 中直接分配\n\n![](/images/nc8.png)\n\n\n## 总结\n\ngo 内存分配的概览\n\n![](/images/nc9.png)\n\n\n----\n\n## 参考文章\n\n- Go内存分配那些事，就这么简单！:[https://lessisbetter.site/2019/07/06/go-memory-allocation](https://lessisbetter.site/2019/07/06/go-memory-allocation)\n- 图解Go语言内存分配|码农桃花源:[https://qcrao.com/2019/03/13/graphic-go-memory-allocation](https://qcrao.com/2019/03/13/graphic-go-memory-allocation)\n- 聊一聊goroutine stack：[https://zhuanlan.zhihu.com/p/28409657](https://zhuanlan.zhihu.com/p/28409657)\n\n\n[Go内存分配那些事，就这么简单！]:https://lessisbetter.site/2019/07/06/go-memory-allocation\n\n[图解Go语言内存分配|码农桃花源]:https://qcrao.com/2019/03/13/graphic-go-memory-allocation\n\n\n\n","slug":"neicunguanliyufenpei","published":1,"updated":"2020-05-15T15:05:47.221Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l8j000mr5fla2lhy1xn","content":"<p> Go这门语言抛弃了C/C++中的开发者管理内存的方式：主动申请与主动释放，增加了<strong>逃逸分析和GC</strong>，将开发者从内存管理中释放出来，让开发者有更多的精力去关注软件设计，而不是底层的内存问题。这是Go语言成为高生产力语言的原因之一  引自【 <a href=\"https://lessisbetter.site/2019/07/06/go-memory-allocation\" target=\"_blank\" rel=\"noopener\">Go内存分配那些事，就这么简单！</a>】</p>\n<h2 id=\"堆内存的分配\"><a href=\"#堆内存的分配\" class=\"headerlink\" title=\"堆内存的分配\"></a>堆内存的分配</h2><p>先看下面这段代码，思考下 <code>smallStruct</code> 会被分配在堆上还是栈上:</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> main</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">type</span> smallStruct <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">   a, b <span class=\"keyword\">int64</span></span><br><span class=\"line\">   c, d <span class=\"keyword\">float64</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">   smallAllocation()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//go:noinline</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">smallAllocation</span><span class=\"params\">()</span> *<span class=\"title\">smallStruct</span></span> &#123;</span><br><span class=\"line\">   <span class=\"keyword\">return</span> &amp;smallStruct&#123;&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过 <code>annotation //go:noinline</code> 禁用内联函数，不然这里不会产生堆内存的分配 <strong>【逃逸分析】</strong></p>\n<p><strong>Inline 内联</strong>: 是在编译期间发生的，将函数调用调用处替换为被调用函数主体的一种编译器优化手段。</p>\n<p>将文件保存为 <code>main.go</code>, 并执行 <code>go tool compile &quot;-m&quot; main.go</code> ,查看Go 堆内存的分配 <strong>【逃逸分析】</strong>的过程</p>\n<p>如果不加 <code>annotation //go:noinline</code> 可以用<br><code>go build -gcflags &#39;-m -l&#39; main.go</code> <code>-l</code>可以禁止内联函数，效果是一样的，下面是逃逸分析的结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">main.go:14:9: &amp;smallStruct literal escapes to heap</span><br></pre></td></tr></table></figure>\n<p>再来看这段代码生成的汇编指令来详细的展示内存分配的过程, 执行下面</p>\n <figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">go tool compile -S  main.go </span><br><span class=\"line\"></span><br><span class=\"line\">0x001d 00029 (main.go:14)   LEAQ   type.\"\".smallStruct(SB), AX</span><br><span class=\"line\">0x0024 00036 (main.go:14)  PCDATA $0, $0</span><br><span class=\"line\">0x0024 00036 (main.go:14)  MOVQ   AX, (SP)</span><br><span class=\"line\">0x0028 00040 (main.go:14)  CALL   runtime.newobject(SB)</span><br></pre></td></tr></table></figure>\n<p><code>runtime.newobject</code> 是 <code>Go</code> 内置的申请堆内存的函数，对于堆内存的分配，<code>Go</code> 中有两种策略: <strong>大内存的分配和小内存的分配</strong></p>\n<h3 id=\"小内存的分配\"><a href=\"#小内存的分配\" class=\"headerlink\" title=\"小内存的分配\"></a>小内存的分配</h3><h4 id=\"从-P-的-mcache-中分配\"><a href=\"#从-P-的-mcache-中分配\" class=\"headerlink\" title=\"从 P 的 mcache 中分配\"></a>从 P 的 <code>mcache</code> 中分配</h4><p>对于小于 <code>32kb</code>的小内存，<code>Go</code> 会尝试在 <code>P</code> 的 本地缓存 <code>mcache</code> 中分配, <code>mcache</code> 保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以 <strong>无锁访问</strong></p>\n<p><img src=\"/images/neicun2.png\" alt=\"\"></p>\n<p>每个 <code>M</code> 绑定一个 <code>P</code> 来运行一个<code>goroutine</code>, 在分配内存时，当前的 <code>goroutine</code> 在当前 <code>P</code> 的本地缓存 <code>mcache</code> 中查找对应的span， 从 <code>span list</code> 中来查找第一个可用的空闲 <code>span</code></p>\n<p><code>span class</code> 分为 <code>8 bytes ~ 32k bytes</code> 共 66 种类型（还有个大小为0的 size class0，并未用到，用于大对象的堆内存分配），分别对应不同的内存大小,<code>mspan</code>里保存对应大小的object，</p>\n<p>1个 <code>size class</code> 对应2个 <code>span class</code>，2个 <code>span class</code> 的 <code>span</code> 大小相同，只是功能不同，1个用来存放包含指针的对象，一个用来存放不包含指针的对象，不包含指针对象的 <code>Span</code> 就无需 <code>GC</code> 扫描了。</p>\n<p>前面的例子里，struct的大小为 <code>(64bit/8)*4=8bit*4=32b</code> 所以会在 <code>span class</code> 大小为 <code>32bytes</code> 的 <code>mspan</code> 里分配</p>\n<p><img src=\"/images/neicun3.png\" alt=\"\"></p>\n<h4 id=\"从全局的缓存-mcentral-中分配\"><a href=\"#从全局的缓存-mcentral-中分配\" class=\"headerlink\" title=\"从全局的缓存  mcentral 中分配\"></a>从全局的缓存  <code>mcentral</code> 中分配</h4><p>当 <code>mcache</code>中没有空闲的 <code>span</code> 时怎么办呢，Go 还维护了一个全局的缓存  <code>mcentral</code>,</p>\n<p><code>mcentral</code> 和 <code>mcache</code> 一样，都134个 <code>span class</code> 级别(67个 <code>size class</code> )，但每个级别都保存了2个span list，即2个span链表：</p>\n<ul>\n<li>nonempty：这个链表里的span，所有span都<strong>至少有1个空闲的对象空间</strong>。这些span是mcache释放span时加入到该链表的。</li>\n<li>empty：这个链表里的span，所有的span都不确定里面是否有空闲的对象空间。当一个span交给mcache的时候，就会加入到empty链表。</li>\n</ul>\n<p><strong>mcache从mcentral获取和归还mspan的流程</strong>：引自【 <a href=\"https://qcrao.com/2019/03/13/graphic-go-memory-allocation\" target=\"_blank\" rel=\"noopener\">图解Go语言内存分配|码农桃花源</a>】</p>\n<ul>\n<li>获取：加锁；从 <code>nonempty</code> 链表找到一个可用的 <code>mspan</code> ；并将其从 <code>nonempty</code> 链表删除；将取出的 <code>mspan</code> 加入到 <code>empty</code> 链表；将 <code>mspan</code> 返回给工作线程；解锁。</li>\n<li>归还：加锁；将 <code>mspan</code> 从 <code>empty</code> 链表删除；将 <code>mspan</code> 加入到 <code>nonempty</code> 链表；解锁。</li>\n<li>另外，GC 扫描的时候会把部分 <code>mspan</code> 标记为未使用，并将对应的 <code>mspan</code> 加入到 <code>nonempty list</code> 中</li>\n</ul>\n<p><img src=\"/images/nc4.png\" alt=\"\"></p>\n<p><code>mcache</code> 从 <code>mcentral</code>获取 过程如下：</p>\n<p><img src=\"/images/nc5.png\" alt=\"\"></p>\n<h4 id=\"mcentral-从-heap-中分配\"><a href=\"#mcentral-从-heap-中分配\" class=\"headerlink\" title=\"mcentral 从 heap 中分配\"></a><code>mcentral</code> 从 <code>heap</code> 中分配</h4><p>当 <code>mcentral</code> 中的 <code>nonempty list</code> 没有可分配的对象的时候，Go会从 <code>mheap</code> 中分配对象，并链接到 <code>nonempty list</code> 上,  <code>mheap</code> 必要时会向系统申请内存</p>\n<p><img src=\"/images/nc6.png\" alt=\"\"></p>\n<p><code>mheap</code> 中还有 <code>arenas</code> ,主要是为了大块内存需要，<code>arena</code> 也是用 <code>mspan</code> 组织的</p>\n<p><img src=\"/images/nc7.png\" alt=\"\"></p>\n<h3 id=\"大内存的分配\"><a href=\"#大内存的分配\" class=\"headerlink\" title=\"大内存的分配\"></a>大内存的分配</h3><p>大内存的分配就比较简单了，大于 <code>32kb</code> 的内存都会在 <code>mheap</code> 中直接分配</p>\n<p><img src=\"/images/nc8.png\" alt=\"\"></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>go 内存分配的概览</p>\n<p><img src=\"/images/nc9.png\" alt=\"\"></p>\n<hr>\n<h2 id=\"参考文章\"><a href=\"#参考文章\" class=\"headerlink\" title=\"参考文章\"></a>参考文章</h2><ul>\n<li>Go内存分配那些事，就这么简单！:<a href=\"https://lessisbetter.site/2019/07/06/go-memory-allocation\" target=\"_blank\" rel=\"noopener\">https://lessisbetter.site/2019/07/06/go-memory-allocation</a></li>\n<li>图解Go语言内存分配|码农桃花源:<a href=\"https://qcrao.com/2019/03/13/graphic-go-memory-allocation\" target=\"_blank\" rel=\"noopener\">https://qcrao.com/2019/03/13/graphic-go-memory-allocation</a></li>\n<li>聊一聊goroutine stack：<a href=\"https://zhuanlan.zhihu.com/p/28409657\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/28409657</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p> Go这门语言抛弃了C/C++中的开发者管理内存的方式：主动申请与主动释放，增加了<strong>逃逸分析和GC</strong>，将开发者从内存管理中释放出来，让开发者有更多的精力去关注软件设计，而不是底层的内存问题。这是Go语言成为高生产力语言的原因之一  引自【 <a href=\"https://lessisbetter.site/2019/07/06/go-memory-allocation\" target=\"_blank\" rel=\"noopener\">Go内存分配那些事，就这么简单！</a>】</p>\n<h2 id=\"堆内存的分配\"><a href=\"#堆内存的分配\" class=\"headerlink\" title=\"堆内存的分配\"></a>堆内存的分配</h2><p>先看下面这段代码，思考下 <code>smallStruct</code> 会被分配在堆上还是栈上:</p>\n<figure class=\"highlight go\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">package</span> main</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">type</span> smallStruct <span class=\"keyword\">struct</span> &#123;</span><br><span class=\"line\">   a, b <span class=\"keyword\">int64</span></span><br><span class=\"line\">   c, d <span class=\"keyword\">float64</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">main</span><span class=\"params\">()</span></span> &#123;</span><br><span class=\"line\">   smallAllocation()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//go:noinline</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">func</span> <span class=\"title\">smallAllocation</span><span class=\"params\">()</span> *<span class=\"title\">smallStruct</span></span> &#123;</span><br><span class=\"line\">   <span class=\"keyword\">return</span> &amp;smallStruct&#123;&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>通过 <code>annotation //go:noinline</code> 禁用内联函数，不然这里不会产生堆内存的分配 <strong>【逃逸分析】</strong></p>\n<p><strong>Inline 内联</strong>: 是在编译期间发生的，将函数调用调用处替换为被调用函数主体的一种编译器优化手段。</p>\n<p>将文件保存为 <code>main.go</code>, 并执行 <code>go tool compile &quot;-m&quot; main.go</code> ,查看Go 堆内存的分配 <strong>【逃逸分析】</strong>的过程</p>\n<p>如果不加 <code>annotation //go:noinline</code> 可以用<br><code>go build -gcflags &#39;-m -l&#39; main.go</code> <code>-l</code>可以禁止内联函数，效果是一样的，下面是逃逸分析的结果：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">main.go:14:9: &amp;smallStruct literal escapes to heap</span><br></pre></td></tr></table></figure>\n<p>再来看这段代码生成的汇编指令来详细的展示内存分配的过程, 执行下面</p>\n <figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">go tool compile -S  main.go </span><br><span class=\"line\"></span><br><span class=\"line\">0x001d 00029 (main.go:14)   LEAQ   type.\"\".smallStruct(SB), AX</span><br><span class=\"line\">0x0024 00036 (main.go:14)  PCDATA $0, $0</span><br><span class=\"line\">0x0024 00036 (main.go:14)  MOVQ   AX, (SP)</span><br><span class=\"line\">0x0028 00040 (main.go:14)  CALL   runtime.newobject(SB)</span><br></pre></td></tr></table></figure>\n<p><code>runtime.newobject</code> 是 <code>Go</code> 内置的申请堆内存的函数，对于堆内存的分配，<code>Go</code> 中有两种策略: <strong>大内存的分配和小内存的分配</strong></p>\n<h3 id=\"小内存的分配\"><a href=\"#小内存的分配\" class=\"headerlink\" title=\"小内存的分配\"></a>小内存的分配</h3><h4 id=\"从-P-的-mcache-中分配\"><a href=\"#从-P-的-mcache-中分配\" class=\"headerlink\" title=\"从 P 的 mcache 中分配\"></a>从 P 的 <code>mcache</code> 中分配</h4><p>对于小于 <code>32kb</code>的小内存，<code>Go</code> 会尝试在 <code>P</code> 的 本地缓存 <code>mcache</code> 中分配, <code>mcache</code> 保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以 <strong>无锁访问</strong></p>\n<p><img src=\"/images/neicun2.png\" alt=\"\"></p>\n<p>每个 <code>M</code> 绑定一个 <code>P</code> 来运行一个<code>goroutine</code>, 在分配内存时，当前的 <code>goroutine</code> 在当前 <code>P</code> 的本地缓存 <code>mcache</code> 中查找对应的span， 从 <code>span list</code> 中来查找第一个可用的空闲 <code>span</code></p>\n<p><code>span class</code> 分为 <code>8 bytes ~ 32k bytes</code> 共 66 种类型（还有个大小为0的 size class0，并未用到，用于大对象的堆内存分配），分别对应不同的内存大小,<code>mspan</code>里保存对应大小的object，</p>\n<p>1个 <code>size class</code> 对应2个 <code>span class</code>，2个 <code>span class</code> 的 <code>span</code> 大小相同，只是功能不同，1个用来存放包含指针的对象，一个用来存放不包含指针的对象，不包含指针对象的 <code>Span</code> 就无需 <code>GC</code> 扫描了。</p>\n<p>前面的例子里，struct的大小为 <code>(64bit/8)*4=8bit*4=32b</code> 所以会在 <code>span class</code> 大小为 <code>32bytes</code> 的 <code>mspan</code> 里分配</p>\n<p><img src=\"/images/neicun3.png\" alt=\"\"></p>\n<h4 id=\"从全局的缓存-mcentral-中分配\"><a href=\"#从全局的缓存-mcentral-中分配\" class=\"headerlink\" title=\"从全局的缓存  mcentral 中分配\"></a>从全局的缓存  <code>mcentral</code> 中分配</h4><p>当 <code>mcache</code>中没有空闲的 <code>span</code> 时怎么办呢，Go 还维护了一个全局的缓存  <code>mcentral</code>,</p>\n<p><code>mcentral</code> 和 <code>mcache</code> 一样，都134个 <code>span class</code> 级别(67个 <code>size class</code> )，但每个级别都保存了2个span list，即2个span链表：</p>\n<ul>\n<li>nonempty：这个链表里的span，所有span都<strong>至少有1个空闲的对象空间</strong>。这些span是mcache释放span时加入到该链表的。</li>\n<li>empty：这个链表里的span，所有的span都不确定里面是否有空闲的对象空间。当一个span交给mcache的时候，就会加入到empty链表。</li>\n</ul>\n<p><strong>mcache从mcentral获取和归还mspan的流程</strong>：引自【 <a href=\"https://qcrao.com/2019/03/13/graphic-go-memory-allocation\" target=\"_blank\" rel=\"noopener\">图解Go语言内存分配|码农桃花源</a>】</p>\n<ul>\n<li>获取：加锁；从 <code>nonempty</code> 链表找到一个可用的 <code>mspan</code> ；并将其从 <code>nonempty</code> 链表删除；将取出的 <code>mspan</code> 加入到 <code>empty</code> 链表；将 <code>mspan</code> 返回给工作线程；解锁。</li>\n<li>归还：加锁；将 <code>mspan</code> 从 <code>empty</code> 链表删除；将 <code>mspan</code> 加入到 <code>nonempty</code> 链表；解锁。</li>\n<li>另外，GC 扫描的时候会把部分 <code>mspan</code> 标记为未使用，并将对应的 <code>mspan</code> 加入到 <code>nonempty list</code> 中</li>\n</ul>\n<p><img src=\"/images/nc4.png\" alt=\"\"></p>\n<p><code>mcache</code> 从 <code>mcentral</code>获取 过程如下：</p>\n<p><img src=\"/images/nc5.png\" alt=\"\"></p>\n<h4 id=\"mcentral-从-heap-中分配\"><a href=\"#mcentral-从-heap-中分配\" class=\"headerlink\" title=\"mcentral 从 heap 中分配\"></a><code>mcentral</code> 从 <code>heap</code> 中分配</h4><p>当 <code>mcentral</code> 中的 <code>nonempty list</code> 没有可分配的对象的时候，Go会从 <code>mheap</code> 中分配对象，并链接到 <code>nonempty list</code> 上,  <code>mheap</code> 必要时会向系统申请内存</p>\n<p><img src=\"/images/nc6.png\" alt=\"\"></p>\n<p><code>mheap</code> 中还有 <code>arenas</code> ,主要是为了大块内存需要，<code>arena</code> 也是用 <code>mspan</code> 组织的</p>\n<p><img src=\"/images/nc7.png\" alt=\"\"></p>\n<h3 id=\"大内存的分配\"><a href=\"#大内存的分配\" class=\"headerlink\" title=\"大内存的分配\"></a>大内存的分配</h3><p>大内存的分配就比较简单了，大于 <code>32kb</code> 的内存都会在 <code>mheap</code> 中直接分配</p>\n<p><img src=\"/images/nc8.png\" alt=\"\"></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>go 内存分配的概览</p>\n<p><img src=\"/images/nc9.png\" alt=\"\"></p>\n<hr>\n<h2 id=\"参考文章\"><a href=\"#参考文章\" class=\"headerlink\" title=\"参考文章\"></a>参考文章</h2><ul>\n<li>Go内存分配那些事，就这么简单！:<a href=\"https://lessisbetter.site/2019/07/06/go-memory-allocation\" target=\"_blank\" rel=\"noopener\">https://lessisbetter.site/2019/07/06/go-memory-allocation</a></li>\n<li>图解Go语言内存分配|码农桃花源:<a href=\"https://qcrao.com/2019/03/13/graphic-go-memory-allocation\" target=\"_blank\" rel=\"noopener\">https://qcrao.com/2019/03/13/graphic-go-memory-allocation</a></li>\n<li>聊一聊goroutine stack：<a href=\"https://zhuanlan.zhihu.com/p/28409657\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/28409657</a></li>\n</ul>\n"},{"layout":"post","title":"kubernetes&容器网络（1）之seivice","date":"2018-11-28T05:19:10.000Z","author":"zhaojizhuang","_content":"\n\n## 1. Service介绍\n\n`Kubernetes` 中有很多概念，例如 `ReplicationController、Service、Pod`等。我认为 `Service` 是 `Kubernetes` 中最重要的概念，没有之一。\n\n为什么 `Service` 如此重要？因为它解耦了前端用户和后端真正提供服务的 `Pods` 。在进一步理解 `Service` 之前，我们先简单地了解下 `Pod` ， `Pod` 也是 `Kubernetes` 中很重要的概念之一。\n\n在 `Kubernetes` 中，`Pod` 是能够创建、调度、和管理的最小部署单元，而不是单独的应用容器。`Pod` 是容器组，一个`Pod`中容器运行在一个共享的应用上下文中。这里的共享上下文是为多个 `Linux Namespace` 的联合。例如：\n\n- `PID`命名空间（在同一个 `Pod` 中的应用可以看到其它应用的进程）\n- `Network`名字空间（在同一个 `Pod` 中的应用可以访问同样的IP和端口空间）\n- `IPC`命名空间（在同一个 `Pod` 中的应用可以使用`SystemV IPC`或者`POSIX`消息队列进行通信）\n- `UTS`命名空间（在同一个 `Pod` 中的应用可以共享一个主机名称）\n-  `Pod` 是一个和应用相关的“逻辑主机”， `Pod` 中的容器共享一个网络名字空间。 `Pod` 为它的组件之间的数据共享、通信和管理提供了便利。\n\n我们可以看出， `Pod` 在资源层面抽象了容器。\n\n因此，在`Kubernetes`中，让我们暂时先忘记容器，记住 `Pod` 。\n\n`Kubernetes`对`Service`的定义是：` Service  is an abstraction which defines a logical set of  Pods  and a policy by which to access them。`我们下面理解下这句话。\n\n刚开始的时候，生活其实是很简单的。一个提供特定服务的进程，运行在一个容器中，监听容器的`IP`地址和端口号。客户端通过`<Container IP>:<ContainerPort>`，或者通过使用Docker的端口映射`<Host IP>:<Host Port>`就可以访问到这个服务了。`The simplest, the best`，简单的生活很美好。\n\n![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/A1HKVXsfHNmswyx38Qh8WVkLPHpr9pex3e7DRk9H0AicQGXP2r1xP8ibkx5QqSnsMc6pQf3wbHPAKcxibFVicz1ChA/640)\n\n但是美好的日子总是短暂的。小伙伴们太热情了，单独由一个容器提供服务不够用了，怎么办？很简单啊，由多个容器提供服务不就可以了吗。问题似乎得到了解决。\n\n可是那么多的容器，客户端到底访问哪个容器中提供的服务呢？访问容器的请求不均衡怎么办？假如容器所在的主机故障了，容器在另外一台主机上拉起了，这个时候容器的IP地址变了，客户端怎么维护这个容器列表呢？所以，由多个容器提供服务的情况下，一般有两种做法：\n\n客户端自己维护提供服务的容器列表，自己做负载均衡，某个容器故障时自己做故障转移；\n提供一个负载均衡器，解耦用户和后端提供服务的容器。负载均衡器负责向后端容器转发流量，并对后端容器进行健康检查。客户端只要访问负载均衡器的IP和端口号就行了。\n我们在前面说Service解耦了前端用户和后端真正提供服务的 `Pod` s。从这个意义上讲，`Service`就是`Kubernetes`中 `Pod` 的负载均衡器。\n\n从生命周期来说， `Pod` 是短暂的而不是长久的应用。 `Pod` 被调度到节点，保持在这个节点上直到被销毁。当节点死亡时，分配到这个节点的 `Pod` 将会被删掉。\n\n但`Service`在其生命周期内，`IP`地址是稳定的。对于`Kubernetes`原生的应用，`Kubernetes`提供了一个`Endpoints`的对象，这个`Endpoints`的名字和`Service`的名字相同，它是一个<Pod IP>:<targetPort>的列表，负责维护`Service`后端的`Pods`的变化。\n\n总结一下，`Service`解耦了前端用户和后端真正提供服务的`Pods`，`Pod`在资源层面抽象了容器。由于它们的存在，使得这个简单的世界变得复杂了。\n\n\n对了，`Service`怎么知道是哪些后端的Pods在真正提供自己定义的服务呢？在创建`Pods`的时候，会定义一些label；在创建`Service`的时候，会定义L`abel Selector，Kubernetes`就是通过`Label Selector`来匹配后端真正服务于`Service`的后端`Pods`的。\n\n## 2. 定义一个`Service`\n接下来就有点没意思了，我要开始翻译上面说的很重要的`Services in Kubernetes`了。当然，我会加入自己的理解。\n\n`Service`也是`Kubernetes`中的一个`REST`对象。可以通过向`apiserver`发送`POST`请求进行创建。当然，`Kubernetes`为我们提供了一个强大和好用的客户端`kubectl`，代替我们向`apiserver`发送请求。但是`kubectl`不仅仅是一个简单的`apiserver`客户端，它为我们提供了很多额外的功能，例如`rolling update`等。\n\n我们可以创建一个`yaml`或者`json`格式的`Service`规范文件，然后通过`kubectl create -f <service spec file>`创建之。一个`Service`的例子如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"selector\": {\n            \"app\": \"MyApp\"\n        },\n        \"ports\": [\n            {\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376\n            }\n        ]\n    }\n}\n```\n\n\n以上规范创建了一个名字为`my-service`的`Service`对象，它指向任何有`app=MyApp`标签的、监听`TCP`端口`9376`的任何`Pods`。\n\n`Kubernetes`会自动地创建一个和`Service`名字相同的`Endpoints`对象。`Service`的`selector`会被持续地评估哪些`Pods`属于这个`Service`，结果会被更新到相应的`Endpoints`对象。\n\n当然，你也可以在定义`Service`的时候为其指定一个`IP`地址（ClusterIP，必须在kube-apiserver的`--service-cluster-ip-range`参数定义内，且不能冲突）。\n\n`Service`会把到<ClusterIP>:<Port>的流量转发到`targetPort`。缺省情况下`targetPort`等于`port`的值。一个有意思的情况是，这里你可以定义`targetPort`为一个字符串（我们可以看到`targetPort`的类型为`IntOrString`），这意味着在后端每个`Pods`中实际监听的端口值可能不一样，这极大地提高了部署`Service`的灵活性。\n\n`Kubernetes`的`Service`支持`TCP`和`UDP`协议，缺省是`TCP`。\n\n## 3. `Service`发布服务的方式\n`Service`有三种类型：`ClusterIP，NodePort和LoadBalancer`。\n\n### 3.1 ClusterIP\n\n关于`ClusterIP`：\n\n通过`Service`的`spec.type: ClusterIP`指定；\n使用`cluster-internal ip`，即`kube-apiserver`的`--service-cluster-ip-range`参数定义的IP范围中的IP地址；\n缺省方式；\n只能从集群内访问，访问方式：`<ClusterIP>:<Port>`；\n`kube-proxy`会为每个`Service`，打开一个本地随机端口，通过`iptables`规则把到`Service`的流量trap到这个随机端口，由`kube-proxy`或者`iptables`接收，进而转发到后端`Pods`。\n\n### 3.2 NodePort\n\n关于`NodePort`：\n\n通过`Service`的`spec.type: NodePort`指定；\n包含`ClusterIP`功能；\n在集群的每个节点上为`Service`开放一个端口（默认是`30000-32767`，由`kube-apiserver`的`--service-node-port-range`参数定义的节点端口范围）；\n可从集群外部通过<`NodeIP>:<NodePort>`访问；\n集群中的每个节点上，都会监听`NodePort`端口，因此，可以通过访问集群中的任意一个节点访问到服务。\n\n### 3.3 LoadBalancer\n\n关于`LoadBalancer`：\n\n通过`Service`的`spec.type: LoadBalancer`指定；\n包含`NodePort`功能；\n通过`Cloud Provider`（例如`GCE`）提供的外部`LoadBalancer`访问服务，即`<LoadBalancerIP>:<Port>`；\n`Service`通过集群的每个节点上的`<NodeIP>:<NodePort>`向外暴露；\n有的`cloudprovider`支持直接从`LoadBalancer`转发流量到后端`Pods`（例如`GCE`），更多的是转发流量到集群节点（例如`AWS`，还有`HWS`）；\n你可以在`Service`定义中指定`loadBalancerIP`，但这需要`cloudprovider`的支持，如果不支持则忽略。真正的`IP`在`status.loadBalancer.ingress.ip`中。\n一个例子如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"selector\": {\n            \"app\": \"MyApp\"\n        },\n        \"ports\": [\n            {\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376,\n                \"nodePort\": 30061\n            }\n        ],\n        \"clusterIP\": \"10.0.171.239\",\n        \"loadBalancerIP\": \"78.11.24.19\",\n        \"type\": \"LoadBalancer\"\n    },\n    \"status\": {\n        \"loadBalancer\": {\n            \"ingress\": [\n                {\n                    \"ip\": \"146.148.47.155\"\n                }\n            ]\n        }\n    }\n}\n```\n\n目前对接`ELB`的实现几大厂商，比如 `HW` 是`ELB`转发流量到集群节点，后面再由`kube-proxy`或者`iptables`转发到后端的`Pods`。\n\n\n### 3.4 External IPs\n\n`External IPs` 不是一种`Service`类型，它不由`Kubernetes`管理，但是我们也可以通过它暴露服务。数据包通过`<External IP>:<Port>`到达集群，然后被路由到`Service`的`Endpoints`。一个例子如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"selector\": {\n            \"app\": \"MyApp\"\n        },\n        \"ports\": [\n            {\n                \"name\": \"http\",\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376\n            }\n        ],\n        \"externalIPs\" : [\n            \"80.11.12.10\"\n        ]\n    }\n}\n```\n\n## 4. 几种特殊的`Service`\n\n### 4.1 没有`selector`的`Service`\n上面说`Kubernetes`的`Service`抽象了到`Kubernetes`的`Pods`的访问。但是它也能抽象到其它类型的后端的访问。举几个场景：\n\n你想接入一个外部的数据库服务；\n你想把一个服务指向另外一个`Namespace`或者集群的服务；\n你把部分负载迁移到`Kubernetes`，而另外一部分后端服务运行在`Kubernetes`之外。\n在这几种情况下，你可以定义一个没有`selector`的服务。如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"ports\": [\n            {\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376\n            }\n        ]\n    }\n}\n```\n\n因为没有`selector`，`Kubernetes`不会自己创建`Endpoints`对象，你需要自己手动创建一个`Endpoints`对象，把Service映射到后端指定的`Endpoints`上。\n\n```json\n{\n    \"kind\": \"Endpoints\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"subsets\": [\n        {\n            \"addresses\": [\n                { \"IP\": \"1.2.3.4\" }\n            ],\n            \"ports\": [\n                { \"port\": 9376 }\n            ]\n        }\n    ]\n}\n```\n\n注意：`Endpoint IP`不能是`loopback（127.0.0.1）`地址、`link-local（169.254.0.0/16）`和`link-local multicast（224.0.0.0/24）`地址。\n\n看到这里，我们似乎明白了，这不就是`Kubernetes`提供的外部服务接入的方式吗？和`CloudFoundry`的`ServiceBroker`的功能类似。\n\n### 4.2 多端口`（multi-port）`的`Service`\n`Kubernetes`的`Service`还支持多端口，比如同时暴露`80`和`443`端口。在这种情况下，你必须为每个端口定义一个名字以示区分。一个例子如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"selector\": {\n            \"app\": \"MyApp\"\n        },\n        \"ports\": [\n            {\n                \"name\": \"http\",\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376\n            },\n            {\n                \"name\": \"https\",\n                \"protocol\": \"TCP\",\n                \"port\": 443,\n                \"targetPort\": 9377\n            }\n        ]\n    }\n}\n```\n\n注意：\n\n多端口必须指定`ports.name`以示区分，端口名称不能一样；\n如果是`spec.type: NodePort`，则每个端口的`NodePort`必须不一样，否则`Kubernetes`不知道一个`NodePort`对应的是后端哪个`targetPort`；\n协议`protocol`和`port`可以一样。\n## 4.3 `Headless services`\n有时候你不想或者不需要`Kubernetes`为你的服务做负载均衡，以及一个`Service`的`IP`地址。在这种情况下，你可以创建一个`headless`的`Service`，通过指定`spec.clusterIP: None`。\n\n对这类`Service`，不会分配`ClusterIP`。对这类`Service`的`DNS`查询会返回一堆`A`记录，即后端`Pods`的`IP`地址。另外，`kube-proxy`不会处理这类`Service`，`Kubernetes`不会对这类`Service`做负载均衡或者代理。但是`Endpoints Controller`还是会为此类`Service`创建`Endpoints`对象。\n\n这允许开发者减少和k`ubernetes`的耦合性，允许他们自己做服务发现等。我在最后讨论的一些基于`Kubernetes`的容器服务，除了彻底不使用`Service`的概念外，也可以创建这类`headless`的`Service`，自己直接通过`LoadBalancer`把流量转发（负载均衡和代理）到后端`Pods`。\n\n## 5. `Service`的流量转发模式\n\n### 5.1 `Proxy-mode: userspace`\n\n`userspace`的代理模式是指由用户态的`kube-proxy`转发流量到后端`Pods`。如下图所示。\n\n![](https://d33wubrfki0l68.cloudfront.net/e351b830334b8622a700a8da6568cb081c464a9b/13020/images/docs/services-userspace-overview.svg)\n\n关于`userspace`：\n\n`Kube-proxy`通过`apiserver`监控`（watch）Service`和E`ndpoints`的变化；\n`Kube-proxy`安装`iptables`规则；\n`Kube-proxy`把访问`Service`的流量转发到后端真正的`Pods`上`（Round-Robin）`；\n`Kubernetes v1.0`只支持这种转发方式；\n通过设置`service.spec.sessionAffinity`: `ClientIP`支持基于`ClientIP`的会话亲和性。\n\n### 5.2 `proxy-mode: iptables`\n\n`iptables`的代理模式是指由内核态的`iptables`转发流量到后端`Pods`。如下图所示。\n\n![](https://d33wubrfki0l68.cloudfront.net/27b2978647a8d7bdc2a96b213f0c0d3242ef9ce0/e8c9b/images/docs/services-iptables-overview.svg)\n\n关于`iptables`：\n\n- `Kube-proxy`通过`apiserver`监控`（watch）Service`和`Endpoints`的变化；\n- `Kube-proxy`安装`iptables`规则；\n- `iptables`把访问`Service`的流量转发到后端真正的`Pods上（Random）`；\n- `Kubernetes v1.1`已支持，但不是默认方式，`v1.2`中将会是默认方式；\n- 通过设置`service.spec.sessionAffinity`: `ClientIP`支持基于`ClientIP`的会话亲和性；\n- 需要`iptables`和内核版本的支持。`iptables > 1.4.11`，内核支持`route_localnet`参数`(kernel >= 3.6)`；\n\n相比`userspace`的优点：\n- 1，数据包不需要拷贝到用户态的`kube-proxy`再做转发，因此效率更高、更可靠。\n- 2，不修改`Client IP`。\n\n### 5.3 `proxy-mode: iptables`\n\n> kubernetes v1.8 引入， 1.11正式可用\n\n在 `ipvs` 模式下，`kube-proxy`监视`Kubernetes`服务和端点，调用 `netlink `接口相应地创建 `IPVS` 规则， 并定期将 `IPVS` 规则与 `Kubernetes` 服务和端点同步。 该控制循环可确保　`IPVS`　状态与所需状态匹配。 访问服务时，`IPVS`　将流量定向到后端Pod之一。\n\n![](https://d33wubrfki0l68.cloudfront.net/2d3d2b521cf7f9ff83238218dac1c019c270b1ed/9ac5c/images/docs/services-ipvs-overview.svg)\n\n`IPVS`代理模式基于类似于 `iptables `模式的 `netfilter` 挂钩函数，但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 `iptables` 模式下的 `kube-proxy` 相比`，IPVS` 模式下的 `kube-proxy` 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，`IPVS` 模式还支持更高的网络流量吞吐量。\n\n`IPVS`提供了更多选项来平衡后端`Pod`的流量。 这些是：\n\n- rr: round-robin\n- lc: least connection (最小连接数)\n- dh: destination hashing（目的地址has）\n- sh: source hashing（源地址has）\n- 等等 \n\n### 5.3 `userspace`和`iptables`转发方式的主要不同点\n`userspace`和`iptables`转发方式的主要不同点如下：\n\n| 比较项 | `userspace`\t| `iptables` |\n|--|--|--|\n 谁转发流量到`Pods `|\t`kube-proxy`把访问`Service`的流量转发到后端真正的`Pods`上 |`iptables`把访问`Service`的流量转发到后端真正的`Pods`上 \n|转发算法\t|轮询`Round-Robin`|\t随机`Random`\n|用户态和内核态\t|数据包需要拷贝到用户态的`kube-proxy`再做转发，因此效率低、不可靠\t|数据包直接在内核态转发，因此效率更高、更可靠\n|是否修改`Client IP`\t|因为`kube-proxy`在中间做代理，会修改数据包的`Client IP`|\t不修改数据包的`Client IP`\n`iptables`版本和内核支持|\t不依赖|\t`iptables > 1.4.11`，内核支持`route_localnet`参数(`kernel >= 3.6`)\n\n通过设置`kube-proxy`的启动参数`--proxy-mode`设定使用`userspace`还是`iptables`代理模式。\n\n\n## 6. Service发现方式\n现在服务创建了，得让别人来使用了。别人要使用首先得知道这些服务呀，服务治理很基本的一个功能就是提供服务发现。`Kubernetes`为我们提供了两种基本的服务发现方式：环境变量和`DNS`。\n\n### 6.1 环境变量\n当一个`Pod`在节点Node上运行时，`kubelet`会为每个活动的服务设置一系列的环境变量。它支持`Docker links compatible`变量，以及更简单的`{SVCNAME}_SERVICE_HOST`和`{SVCNAME}_SERVICE_PORT`变量。后者是把服务名字大写，然后把中划线（-）转换为下划线（_）。\n\n以服务`redis-master`为例，它暴露TCP协议的6379端口，被分配了集群IP地址10.0.0.11，则会创建如下环境变量：\n\n```shell\nREDIS_MASTER_SERVICE_HOST=10.0.0.11\nREDIS_MASTER_SERVICE_PORT=6379\nREDIS_MASTER_PORT=tcp://10.0.0.11:6379\nREDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379\nREDIS_MASTER_PORT_6379_TCP_PROTO=tcp\nREDIS_MASTER_PORT_6379_TCP_PORT=6379\nREDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11\n```\n\n这里有一个注意点是，如果一个`Pod`要访问一个`Service`，则必须在该Service之前创建，否则这些环境变量不会被注入此Pod。DNS方式的服务发现就没有此限制。\n\n## 6.2 DNS\n虽然`DNS`是一个`cluster add-on`特性，但是我们还是强烈推荐使用`DNS`作为服务发现的方式。`DNS`服务器通过`KubernetesAPI`监控新的`Service`的生成，然后为每个`Service`设置一堆`DNS`记录。如果集群设置了`DNS`，则该集群中所有的`Pods`都能够使用`DNS`解析`Sercice`。\n\n例如，如果在`Kubernertes`中的`my-ns`名字空间中有一个服务叫做`my-service`，则会创建一个`my-service.my-ns`的`DNS`记录。在同一个名字空间`my-ns`的`Pods`能直接通过服务名`my-service`查找到该服务。如果是其它的`Namespace`中的`Pods`，则需加上名字空间，例如`my-service.my-ns`。返回的结果是服务的`ClusterIP`。当然，对于我们上面讲的`headless`的`Service`，返回的则是该`Service`对应的一堆后端`Pods`的`IP`地址。\n\n对于知名服务端口，`Kubernetes`还支持`DNS SRV`记录。例如`my-service.my-ns`的服务支持`TCP`协议的`http`端口，则你可以通过一个`DNS SRV`查询`_http._tcp.my-service.my-ns`来发现http的端口。\n\n对于每个`Service`的`DNS`记录，`Kubernetes`还会加上一个集群域名的后缀，作为完全域名（FQDN）。这个集群域名通过svc+安装集群`DNS`的`DNS_DOMAIN`参数指定，默认是`svc.cluster.local`。如果不是一个标准的`Kubernetes`支持的安装，则启动`kubelet`的时候指定参数`--cluster-domain`，你还需要指定`--cluster-dns`告诉`kubelet`集群`DNS`的地址。\n\n\n## 6.3 如何发现和使用服务？\n一般在创建`Pod`的时候，指定一个环境变量`GET_HOSTS_FROM`，值可以设为`env`或者`dns`。在`Pod`中的应用先获取这个环境变量，得到获取服务的方式。如果是`env`，则通过`getenv`获取相应的服务的环境变量，例如`REDIS_SLAVE_SERVICE_HOST`；如果是`dns`，则可以在`Pod`内通过标准的`gethostbyname`获取服务主机名。有个例外是P`od`的定义中，不能设置h`ostNetwork: true`。\n\n获取到服务的地址，就可以通过正常方式使用服务了。\n\n如下是`Kubernetes`自带的`guestbook.php`中的一段相关代码，供参考：\n\n```shell\n$host = 'redis-slave';\nif (getenv('GET_HOSTS_FROM') == 'env') {\n  $host = getenv('REDIS_SLAVE_SERVICE_HOST');\n}\n$client = new Predis\\Client([\n  'scheme' => 'tcp',\n  'host'   => $host,\n  'port'   => 6379,\n]);\n```\n\n## 7. 一些容器服务中的`Service`\n虽然`Service`在`Kubernetes`中如此重要，但是对一些基于`Kubernetes`的容器服务，并没有使用`Service`，或者用的是上面讨论的`headless`类型的`Service`。这种方式基本上是把容器当做`VM`使用的典型，`LoadBalancer`和`Pods`网络互通，通过`LoadBalancer`直接把流量转发到`Pods`上，省却了中间由`kube-proxy`或者`iptables`的转发方式，从而提高了流量转发效率，但是也由`LoadBalancer`自己提供对后端P`ods`的维护，一般需要`LoadBalancer`提供动态路由的功能（即后端`Pods`可以动态地从`LoadBalancer`上注册/注销）。\n\n\n\n","source":"_posts/k8s-svc.md","raw":"---\nlayout: post\ntitle:  \"kubernetes&容器网络（1）之seivice\"\ndate:   2018-11-28 13:19:10 +0800\ncategories: k8s\ntags:  [\"k8s\", \"iptables\"]\nauthor: zhaojizhuang\n\n---\n\n\n## 1. Service介绍\n\n`Kubernetes` 中有很多概念，例如 `ReplicationController、Service、Pod`等。我认为 `Service` 是 `Kubernetes` 中最重要的概念，没有之一。\n\n为什么 `Service` 如此重要？因为它解耦了前端用户和后端真正提供服务的 `Pods` 。在进一步理解 `Service` 之前，我们先简单地了解下 `Pod` ， `Pod` 也是 `Kubernetes` 中很重要的概念之一。\n\n在 `Kubernetes` 中，`Pod` 是能够创建、调度、和管理的最小部署单元，而不是单独的应用容器。`Pod` 是容器组，一个`Pod`中容器运行在一个共享的应用上下文中。这里的共享上下文是为多个 `Linux Namespace` 的联合。例如：\n\n- `PID`命名空间（在同一个 `Pod` 中的应用可以看到其它应用的进程）\n- `Network`名字空间（在同一个 `Pod` 中的应用可以访问同样的IP和端口空间）\n- `IPC`命名空间（在同一个 `Pod` 中的应用可以使用`SystemV IPC`或者`POSIX`消息队列进行通信）\n- `UTS`命名空间（在同一个 `Pod` 中的应用可以共享一个主机名称）\n-  `Pod` 是一个和应用相关的“逻辑主机”， `Pod` 中的容器共享一个网络名字空间。 `Pod` 为它的组件之间的数据共享、通信和管理提供了便利。\n\n我们可以看出， `Pod` 在资源层面抽象了容器。\n\n因此，在`Kubernetes`中，让我们暂时先忘记容器，记住 `Pod` 。\n\n`Kubernetes`对`Service`的定义是：` Service  is an abstraction which defines a logical set of  Pods  and a policy by which to access them。`我们下面理解下这句话。\n\n刚开始的时候，生活其实是很简单的。一个提供特定服务的进程，运行在一个容器中，监听容器的`IP`地址和端口号。客户端通过`<Container IP>:<ContainerPort>`，或者通过使用Docker的端口映射`<Host IP>:<Host Port>`就可以访问到这个服务了。`The simplest, the best`，简单的生活很美好。\n\n![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/A1HKVXsfHNmswyx38Qh8WVkLPHpr9pex3e7DRk9H0AicQGXP2r1xP8ibkx5QqSnsMc6pQf3wbHPAKcxibFVicz1ChA/640)\n\n但是美好的日子总是短暂的。小伙伴们太热情了，单独由一个容器提供服务不够用了，怎么办？很简单啊，由多个容器提供服务不就可以了吗。问题似乎得到了解决。\n\n可是那么多的容器，客户端到底访问哪个容器中提供的服务呢？访问容器的请求不均衡怎么办？假如容器所在的主机故障了，容器在另外一台主机上拉起了，这个时候容器的IP地址变了，客户端怎么维护这个容器列表呢？所以，由多个容器提供服务的情况下，一般有两种做法：\n\n客户端自己维护提供服务的容器列表，自己做负载均衡，某个容器故障时自己做故障转移；\n提供一个负载均衡器，解耦用户和后端提供服务的容器。负载均衡器负责向后端容器转发流量，并对后端容器进行健康检查。客户端只要访问负载均衡器的IP和端口号就行了。\n我们在前面说Service解耦了前端用户和后端真正提供服务的 `Pod` s。从这个意义上讲，`Service`就是`Kubernetes`中 `Pod` 的负载均衡器。\n\n从生命周期来说， `Pod` 是短暂的而不是长久的应用。 `Pod` 被调度到节点，保持在这个节点上直到被销毁。当节点死亡时，分配到这个节点的 `Pod` 将会被删掉。\n\n但`Service`在其生命周期内，`IP`地址是稳定的。对于`Kubernetes`原生的应用，`Kubernetes`提供了一个`Endpoints`的对象，这个`Endpoints`的名字和`Service`的名字相同，它是一个<Pod IP>:<targetPort>的列表，负责维护`Service`后端的`Pods`的变化。\n\n总结一下，`Service`解耦了前端用户和后端真正提供服务的`Pods`，`Pod`在资源层面抽象了容器。由于它们的存在，使得这个简单的世界变得复杂了。\n\n\n对了，`Service`怎么知道是哪些后端的Pods在真正提供自己定义的服务呢？在创建`Pods`的时候，会定义一些label；在创建`Service`的时候，会定义L`abel Selector，Kubernetes`就是通过`Label Selector`来匹配后端真正服务于`Service`的后端`Pods`的。\n\n## 2. 定义一个`Service`\n接下来就有点没意思了，我要开始翻译上面说的很重要的`Services in Kubernetes`了。当然，我会加入自己的理解。\n\n`Service`也是`Kubernetes`中的一个`REST`对象。可以通过向`apiserver`发送`POST`请求进行创建。当然，`Kubernetes`为我们提供了一个强大和好用的客户端`kubectl`，代替我们向`apiserver`发送请求。但是`kubectl`不仅仅是一个简单的`apiserver`客户端，它为我们提供了很多额外的功能，例如`rolling update`等。\n\n我们可以创建一个`yaml`或者`json`格式的`Service`规范文件，然后通过`kubectl create -f <service spec file>`创建之。一个`Service`的例子如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"selector\": {\n            \"app\": \"MyApp\"\n        },\n        \"ports\": [\n            {\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376\n            }\n        ]\n    }\n}\n```\n\n\n以上规范创建了一个名字为`my-service`的`Service`对象，它指向任何有`app=MyApp`标签的、监听`TCP`端口`9376`的任何`Pods`。\n\n`Kubernetes`会自动地创建一个和`Service`名字相同的`Endpoints`对象。`Service`的`selector`会被持续地评估哪些`Pods`属于这个`Service`，结果会被更新到相应的`Endpoints`对象。\n\n当然，你也可以在定义`Service`的时候为其指定一个`IP`地址（ClusterIP，必须在kube-apiserver的`--service-cluster-ip-range`参数定义内，且不能冲突）。\n\n`Service`会把到<ClusterIP>:<Port>的流量转发到`targetPort`。缺省情况下`targetPort`等于`port`的值。一个有意思的情况是，这里你可以定义`targetPort`为一个字符串（我们可以看到`targetPort`的类型为`IntOrString`），这意味着在后端每个`Pods`中实际监听的端口值可能不一样，这极大地提高了部署`Service`的灵活性。\n\n`Kubernetes`的`Service`支持`TCP`和`UDP`协议，缺省是`TCP`。\n\n## 3. `Service`发布服务的方式\n`Service`有三种类型：`ClusterIP，NodePort和LoadBalancer`。\n\n### 3.1 ClusterIP\n\n关于`ClusterIP`：\n\n通过`Service`的`spec.type: ClusterIP`指定；\n使用`cluster-internal ip`，即`kube-apiserver`的`--service-cluster-ip-range`参数定义的IP范围中的IP地址；\n缺省方式；\n只能从集群内访问，访问方式：`<ClusterIP>:<Port>`；\n`kube-proxy`会为每个`Service`，打开一个本地随机端口，通过`iptables`规则把到`Service`的流量trap到这个随机端口，由`kube-proxy`或者`iptables`接收，进而转发到后端`Pods`。\n\n### 3.2 NodePort\n\n关于`NodePort`：\n\n通过`Service`的`spec.type: NodePort`指定；\n包含`ClusterIP`功能；\n在集群的每个节点上为`Service`开放一个端口（默认是`30000-32767`，由`kube-apiserver`的`--service-node-port-range`参数定义的节点端口范围）；\n可从集群外部通过<`NodeIP>:<NodePort>`访问；\n集群中的每个节点上，都会监听`NodePort`端口，因此，可以通过访问集群中的任意一个节点访问到服务。\n\n### 3.3 LoadBalancer\n\n关于`LoadBalancer`：\n\n通过`Service`的`spec.type: LoadBalancer`指定；\n包含`NodePort`功能；\n通过`Cloud Provider`（例如`GCE`）提供的外部`LoadBalancer`访问服务，即`<LoadBalancerIP>:<Port>`；\n`Service`通过集群的每个节点上的`<NodeIP>:<NodePort>`向外暴露；\n有的`cloudprovider`支持直接从`LoadBalancer`转发流量到后端`Pods`（例如`GCE`），更多的是转发流量到集群节点（例如`AWS`，还有`HWS`）；\n你可以在`Service`定义中指定`loadBalancerIP`，但这需要`cloudprovider`的支持，如果不支持则忽略。真正的`IP`在`status.loadBalancer.ingress.ip`中。\n一个例子如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"selector\": {\n            \"app\": \"MyApp\"\n        },\n        \"ports\": [\n            {\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376,\n                \"nodePort\": 30061\n            }\n        ],\n        \"clusterIP\": \"10.0.171.239\",\n        \"loadBalancerIP\": \"78.11.24.19\",\n        \"type\": \"LoadBalancer\"\n    },\n    \"status\": {\n        \"loadBalancer\": {\n            \"ingress\": [\n                {\n                    \"ip\": \"146.148.47.155\"\n                }\n            ]\n        }\n    }\n}\n```\n\n目前对接`ELB`的实现几大厂商，比如 `HW` 是`ELB`转发流量到集群节点，后面再由`kube-proxy`或者`iptables`转发到后端的`Pods`。\n\n\n### 3.4 External IPs\n\n`External IPs` 不是一种`Service`类型，它不由`Kubernetes`管理，但是我们也可以通过它暴露服务。数据包通过`<External IP>:<Port>`到达集群，然后被路由到`Service`的`Endpoints`。一个例子如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"selector\": {\n            \"app\": \"MyApp\"\n        },\n        \"ports\": [\n            {\n                \"name\": \"http\",\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376\n            }\n        ],\n        \"externalIPs\" : [\n            \"80.11.12.10\"\n        ]\n    }\n}\n```\n\n## 4. 几种特殊的`Service`\n\n### 4.1 没有`selector`的`Service`\n上面说`Kubernetes`的`Service`抽象了到`Kubernetes`的`Pods`的访问。但是它也能抽象到其它类型的后端的访问。举几个场景：\n\n你想接入一个外部的数据库服务；\n你想把一个服务指向另外一个`Namespace`或者集群的服务；\n你把部分负载迁移到`Kubernetes`，而另外一部分后端服务运行在`Kubernetes`之外。\n在这几种情况下，你可以定义一个没有`selector`的服务。如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"ports\": [\n            {\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376\n            }\n        ]\n    }\n}\n```\n\n因为没有`selector`，`Kubernetes`不会自己创建`Endpoints`对象，你需要自己手动创建一个`Endpoints`对象，把Service映射到后端指定的`Endpoints`上。\n\n```json\n{\n    \"kind\": \"Endpoints\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"subsets\": [\n        {\n            \"addresses\": [\n                { \"IP\": \"1.2.3.4\" }\n            ],\n            \"ports\": [\n                { \"port\": 9376 }\n            ]\n        }\n    ]\n}\n```\n\n注意：`Endpoint IP`不能是`loopback（127.0.0.1）`地址、`link-local（169.254.0.0/16）`和`link-local multicast（224.0.0.0/24）`地址。\n\n看到这里，我们似乎明白了，这不就是`Kubernetes`提供的外部服务接入的方式吗？和`CloudFoundry`的`ServiceBroker`的功能类似。\n\n### 4.2 多端口`（multi-port）`的`Service`\n`Kubernetes`的`Service`还支持多端口，比如同时暴露`80`和`443`端口。在这种情况下，你必须为每个端口定义一个名字以示区分。一个例子如下：\n\n```json\n{\n    \"kind\": \"Service\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"my-service\"\n    },\n    \"spec\": {\n        \"selector\": {\n            \"app\": \"MyApp\"\n        },\n        \"ports\": [\n            {\n                \"name\": \"http\",\n                \"protocol\": \"TCP\",\n                \"port\": 80,\n                \"targetPort\": 9376\n            },\n            {\n                \"name\": \"https\",\n                \"protocol\": \"TCP\",\n                \"port\": 443,\n                \"targetPort\": 9377\n            }\n        ]\n    }\n}\n```\n\n注意：\n\n多端口必须指定`ports.name`以示区分，端口名称不能一样；\n如果是`spec.type: NodePort`，则每个端口的`NodePort`必须不一样，否则`Kubernetes`不知道一个`NodePort`对应的是后端哪个`targetPort`；\n协议`protocol`和`port`可以一样。\n## 4.3 `Headless services`\n有时候你不想或者不需要`Kubernetes`为你的服务做负载均衡，以及一个`Service`的`IP`地址。在这种情况下，你可以创建一个`headless`的`Service`，通过指定`spec.clusterIP: None`。\n\n对这类`Service`，不会分配`ClusterIP`。对这类`Service`的`DNS`查询会返回一堆`A`记录，即后端`Pods`的`IP`地址。另外，`kube-proxy`不会处理这类`Service`，`Kubernetes`不会对这类`Service`做负载均衡或者代理。但是`Endpoints Controller`还是会为此类`Service`创建`Endpoints`对象。\n\n这允许开发者减少和k`ubernetes`的耦合性，允许他们自己做服务发现等。我在最后讨论的一些基于`Kubernetes`的容器服务，除了彻底不使用`Service`的概念外，也可以创建这类`headless`的`Service`，自己直接通过`LoadBalancer`把流量转发（负载均衡和代理）到后端`Pods`。\n\n## 5. `Service`的流量转发模式\n\n### 5.1 `Proxy-mode: userspace`\n\n`userspace`的代理模式是指由用户态的`kube-proxy`转发流量到后端`Pods`。如下图所示。\n\n![](https://d33wubrfki0l68.cloudfront.net/e351b830334b8622a700a8da6568cb081c464a9b/13020/images/docs/services-userspace-overview.svg)\n\n关于`userspace`：\n\n`Kube-proxy`通过`apiserver`监控`（watch）Service`和E`ndpoints`的变化；\n`Kube-proxy`安装`iptables`规则；\n`Kube-proxy`把访问`Service`的流量转发到后端真正的`Pods`上`（Round-Robin）`；\n`Kubernetes v1.0`只支持这种转发方式；\n通过设置`service.spec.sessionAffinity`: `ClientIP`支持基于`ClientIP`的会话亲和性。\n\n### 5.2 `proxy-mode: iptables`\n\n`iptables`的代理模式是指由内核态的`iptables`转发流量到后端`Pods`。如下图所示。\n\n![](https://d33wubrfki0l68.cloudfront.net/27b2978647a8d7bdc2a96b213f0c0d3242ef9ce0/e8c9b/images/docs/services-iptables-overview.svg)\n\n关于`iptables`：\n\n- `Kube-proxy`通过`apiserver`监控`（watch）Service`和`Endpoints`的变化；\n- `Kube-proxy`安装`iptables`规则；\n- `iptables`把访问`Service`的流量转发到后端真正的`Pods上（Random）`；\n- `Kubernetes v1.1`已支持，但不是默认方式，`v1.2`中将会是默认方式；\n- 通过设置`service.spec.sessionAffinity`: `ClientIP`支持基于`ClientIP`的会话亲和性；\n- 需要`iptables`和内核版本的支持。`iptables > 1.4.11`，内核支持`route_localnet`参数`(kernel >= 3.6)`；\n\n相比`userspace`的优点：\n- 1，数据包不需要拷贝到用户态的`kube-proxy`再做转发，因此效率更高、更可靠。\n- 2，不修改`Client IP`。\n\n### 5.3 `proxy-mode: iptables`\n\n> kubernetes v1.8 引入， 1.11正式可用\n\n在 `ipvs` 模式下，`kube-proxy`监视`Kubernetes`服务和端点，调用 `netlink `接口相应地创建 `IPVS` 规则， 并定期将 `IPVS` 规则与 `Kubernetes` 服务和端点同步。 该控制循环可确保　`IPVS`　状态与所需状态匹配。 访问服务时，`IPVS`　将流量定向到后端Pod之一。\n\n![](https://d33wubrfki0l68.cloudfront.net/2d3d2b521cf7f9ff83238218dac1c019c270b1ed/9ac5c/images/docs/services-ipvs-overview.svg)\n\n`IPVS`代理模式基于类似于 `iptables `模式的 `netfilter` 挂钩函数，但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 `iptables` 模式下的 `kube-proxy` 相比`，IPVS` 模式下的 `kube-proxy` 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，`IPVS` 模式还支持更高的网络流量吞吐量。\n\n`IPVS`提供了更多选项来平衡后端`Pod`的流量。 这些是：\n\n- rr: round-robin\n- lc: least connection (最小连接数)\n- dh: destination hashing（目的地址has）\n- sh: source hashing（源地址has）\n- 等等 \n\n### 5.3 `userspace`和`iptables`转发方式的主要不同点\n`userspace`和`iptables`转发方式的主要不同点如下：\n\n| 比较项 | `userspace`\t| `iptables` |\n|--|--|--|\n 谁转发流量到`Pods `|\t`kube-proxy`把访问`Service`的流量转发到后端真正的`Pods`上 |`iptables`把访问`Service`的流量转发到后端真正的`Pods`上 \n|转发算法\t|轮询`Round-Robin`|\t随机`Random`\n|用户态和内核态\t|数据包需要拷贝到用户态的`kube-proxy`再做转发，因此效率低、不可靠\t|数据包直接在内核态转发，因此效率更高、更可靠\n|是否修改`Client IP`\t|因为`kube-proxy`在中间做代理，会修改数据包的`Client IP`|\t不修改数据包的`Client IP`\n`iptables`版本和内核支持|\t不依赖|\t`iptables > 1.4.11`，内核支持`route_localnet`参数(`kernel >= 3.6`)\n\n通过设置`kube-proxy`的启动参数`--proxy-mode`设定使用`userspace`还是`iptables`代理模式。\n\n\n## 6. Service发现方式\n现在服务创建了，得让别人来使用了。别人要使用首先得知道这些服务呀，服务治理很基本的一个功能就是提供服务发现。`Kubernetes`为我们提供了两种基本的服务发现方式：环境变量和`DNS`。\n\n### 6.1 环境变量\n当一个`Pod`在节点Node上运行时，`kubelet`会为每个活动的服务设置一系列的环境变量。它支持`Docker links compatible`变量，以及更简单的`{SVCNAME}_SERVICE_HOST`和`{SVCNAME}_SERVICE_PORT`变量。后者是把服务名字大写，然后把中划线（-）转换为下划线（_）。\n\n以服务`redis-master`为例，它暴露TCP协议的6379端口，被分配了集群IP地址10.0.0.11，则会创建如下环境变量：\n\n```shell\nREDIS_MASTER_SERVICE_HOST=10.0.0.11\nREDIS_MASTER_SERVICE_PORT=6379\nREDIS_MASTER_PORT=tcp://10.0.0.11:6379\nREDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379\nREDIS_MASTER_PORT_6379_TCP_PROTO=tcp\nREDIS_MASTER_PORT_6379_TCP_PORT=6379\nREDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11\n```\n\n这里有一个注意点是，如果一个`Pod`要访问一个`Service`，则必须在该Service之前创建，否则这些环境变量不会被注入此Pod。DNS方式的服务发现就没有此限制。\n\n## 6.2 DNS\n虽然`DNS`是一个`cluster add-on`特性，但是我们还是强烈推荐使用`DNS`作为服务发现的方式。`DNS`服务器通过`KubernetesAPI`监控新的`Service`的生成，然后为每个`Service`设置一堆`DNS`记录。如果集群设置了`DNS`，则该集群中所有的`Pods`都能够使用`DNS`解析`Sercice`。\n\n例如，如果在`Kubernertes`中的`my-ns`名字空间中有一个服务叫做`my-service`，则会创建一个`my-service.my-ns`的`DNS`记录。在同一个名字空间`my-ns`的`Pods`能直接通过服务名`my-service`查找到该服务。如果是其它的`Namespace`中的`Pods`，则需加上名字空间，例如`my-service.my-ns`。返回的结果是服务的`ClusterIP`。当然，对于我们上面讲的`headless`的`Service`，返回的则是该`Service`对应的一堆后端`Pods`的`IP`地址。\n\n对于知名服务端口，`Kubernetes`还支持`DNS SRV`记录。例如`my-service.my-ns`的服务支持`TCP`协议的`http`端口，则你可以通过一个`DNS SRV`查询`_http._tcp.my-service.my-ns`来发现http的端口。\n\n对于每个`Service`的`DNS`记录，`Kubernetes`还会加上一个集群域名的后缀，作为完全域名（FQDN）。这个集群域名通过svc+安装集群`DNS`的`DNS_DOMAIN`参数指定，默认是`svc.cluster.local`。如果不是一个标准的`Kubernetes`支持的安装，则启动`kubelet`的时候指定参数`--cluster-domain`，你还需要指定`--cluster-dns`告诉`kubelet`集群`DNS`的地址。\n\n\n## 6.3 如何发现和使用服务？\n一般在创建`Pod`的时候，指定一个环境变量`GET_HOSTS_FROM`，值可以设为`env`或者`dns`。在`Pod`中的应用先获取这个环境变量，得到获取服务的方式。如果是`env`，则通过`getenv`获取相应的服务的环境变量，例如`REDIS_SLAVE_SERVICE_HOST`；如果是`dns`，则可以在`Pod`内通过标准的`gethostbyname`获取服务主机名。有个例外是P`od`的定义中，不能设置h`ostNetwork: true`。\n\n获取到服务的地址，就可以通过正常方式使用服务了。\n\n如下是`Kubernetes`自带的`guestbook.php`中的一段相关代码，供参考：\n\n```shell\n$host = 'redis-slave';\nif (getenv('GET_HOSTS_FROM') == 'env') {\n  $host = getenv('REDIS_SLAVE_SERVICE_HOST');\n}\n$client = new Predis\\Client([\n  'scheme' => 'tcp',\n  'host'   => $host,\n  'port'   => 6379,\n]);\n```\n\n## 7. 一些容器服务中的`Service`\n虽然`Service`在`Kubernetes`中如此重要，但是对一些基于`Kubernetes`的容器服务，并没有使用`Service`，或者用的是上面讨论的`headless`类型的`Service`。这种方式基本上是把容器当做`VM`使用的典型，`LoadBalancer`和`Pods`网络互通，通过`LoadBalancer`直接把流量转发到`Pods`上，省却了中间由`kube-proxy`或者`iptables`的转发方式，从而提高了流量转发效率，但是也由`LoadBalancer`自己提供对后端P`ods`的维护，一般需要`LoadBalancer`提供动态路由的功能（即后端`Pods`可以动态地从`LoadBalancer`上注册/注销）。\n\n\n\n","slug":"k8s-svc","published":1,"updated":"2020-05-19T13:23:56.312Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l8k000or5fltfwv9xns","content":"<h2 id=\"1-Service介绍\"><a href=\"#1-Service介绍\" class=\"headerlink\" title=\"1. Service介绍\"></a>1. Service介绍</h2><p><code>Kubernetes</code> 中有很多概念，例如 <code>ReplicationController、Service、Pod</code>等。我认为 <code>Service</code> 是 <code>Kubernetes</code> 中最重要的概念，没有之一。</p>\n<p>为什么 <code>Service</code> 如此重要？因为它解耦了前端用户和后端真正提供服务的 <code>Pods</code> 。在进一步理解 <code>Service</code> 之前，我们先简单地了解下 <code>Pod</code> ， <code>Pod</code> 也是 <code>Kubernetes</code> 中很重要的概念之一。</p>\n<p>在 <code>Kubernetes</code> 中，<code>Pod</code> 是能够创建、调度、和管理的最小部署单元，而不是单独的应用容器。<code>Pod</code> 是容器组，一个<code>Pod</code>中容器运行在一个共享的应用上下文中。这里的共享上下文是为多个 <code>Linux Namespace</code> 的联合。例如：</p>\n<ul>\n<li><code>PID</code>命名空间（在同一个 <code>Pod</code> 中的应用可以看到其它应用的进程）</li>\n<li><code>Network</code>名字空间（在同一个 <code>Pod</code> 中的应用可以访问同样的IP和端口空间）</li>\n<li><code>IPC</code>命名空间（在同一个 <code>Pod</code> 中的应用可以使用<code>SystemV IPC</code>或者<code>POSIX</code>消息队列进行通信）</li>\n<li><code>UTS</code>命名空间（在同一个 <code>Pod</code> 中的应用可以共享一个主机名称）</li>\n<li><code>Pod</code> 是一个和应用相关的“逻辑主机”， <code>Pod</code> 中的容器共享一个网络名字空间。 <code>Pod</code> 为它的组件之间的数据共享、通信和管理提供了便利。</li>\n</ul>\n<p>我们可以看出， <code>Pod</code> 在资源层面抽象了容器。</p>\n<p>因此，在<code>Kubernetes</code>中，让我们暂时先忘记容器，记住 <code>Pod</code> 。</p>\n<p><code>Kubernetes</code>对<code>Service</code>的定义是：<code>Service  is an abstraction which defines a logical set of  Pods  and a policy by which to access them。</code>我们下面理解下这句话。</p>\n<p>刚开始的时候，生活其实是很简单的。一个提供特定服务的进程，运行在一个容器中，监听容器的<code>IP</code>地址和端口号。客户端通过<code>&lt;Container IP&gt;:&lt;ContainerPort&gt;</code>，或者通过使用Docker的端口映射<code>&lt;Host IP&gt;:&lt;Host Port&gt;</code>就可以访问到这个服务了。<code>The simplest, the best</code>，简单的生活很美好。</p>\n<p><img src=\"https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/A1HKVXsfHNmswyx38Qh8WVkLPHpr9pex3e7DRk9H0AicQGXP2r1xP8ibkx5QqSnsMc6pQf3wbHPAKcxibFVicz1ChA/640\" alt=\"\"></p>\n<p>但是美好的日子总是短暂的。小伙伴们太热情了，单独由一个容器提供服务不够用了，怎么办？很简单啊，由多个容器提供服务不就可以了吗。问题似乎得到了解决。</p>\n<p>可是那么多的容器，客户端到底访问哪个容器中提供的服务呢？访问容器的请求不均衡怎么办？假如容器所在的主机故障了，容器在另外一台主机上拉起了，这个时候容器的IP地址变了，客户端怎么维护这个容器列表呢？所以，由多个容器提供服务的情况下，一般有两种做法：</p>\n<p>客户端自己维护提供服务的容器列表，自己做负载均衡，某个容器故障时自己做故障转移；<br>提供一个负载均衡器，解耦用户和后端提供服务的容器。负载均衡器负责向后端容器转发流量，并对后端容器进行健康检查。客户端只要访问负载均衡器的IP和端口号就行了。<br>我们在前面说Service解耦了前端用户和后端真正提供服务的 <code>Pod</code> s。从这个意义上讲，<code>Service</code>就是<code>Kubernetes</code>中 <code>Pod</code> 的负载均衡器。</p>\n<p>从生命周期来说， <code>Pod</code> 是短暂的而不是长久的应用。 <code>Pod</code> 被调度到节点，保持在这个节点上直到被销毁。当节点死亡时，分配到这个节点的 <code>Pod</code> 将会被删掉。</p>\n<p>但<code>Service</code>在其生命周期内，<code>IP</code>地址是稳定的。对于<code>Kubernetes</code>原生的应用，<code>Kubernetes</code>提供了一个<code>Endpoints</code>的对象，这个<code>Endpoints</code>的名字和<code>Service</code>的名字相同，它是一个<pod ip=\"\">:<targetport>的列表，负责维护<code>Service</code>后端的<code>Pods</code>的变化。</targetport></pod></p>\n<p>总结一下，<code>Service</code>解耦了前端用户和后端真正提供服务的<code>Pods</code>，<code>Pod</code>在资源层面抽象了容器。由于它们的存在，使得这个简单的世界变得复杂了。</p>\n<p>对了，<code>Service</code>怎么知道是哪些后端的Pods在真正提供自己定义的服务呢？在创建<code>Pods</code>的时候，会定义一些label；在创建<code>Service</code>的时候，会定义L<code>abel Selector，Kubernetes</code>就是通过<code>Label Selector</code>来匹配后端真正服务于<code>Service</code>的后端<code>Pods</code>的。</p>\n<h2 id=\"2-定义一个Service\"><a href=\"#2-定义一个Service\" class=\"headerlink\" title=\"2. 定义一个Service\"></a>2. 定义一个<code>Service</code></h2><p>接下来就有点没意思了，我要开始翻译上面说的很重要的<code>Services in Kubernetes</code>了。当然，我会加入自己的理解。</p>\n<p><code>Service</code>也是<code>Kubernetes</code>中的一个<code>REST</code>对象。可以通过向<code>apiserver</code>发送<code>POST</code>请求进行创建。当然，<code>Kubernetes</code>为我们提供了一个强大和好用的客户端<code>kubectl</code>，代替我们向<code>apiserver</code>发送请求。但是<code>kubectl</code>不仅仅是一个简单的<code>apiserver</code>客户端，它为我们提供了很多额外的功能，例如<code>rolling update</code>等。</p>\n<p>我们可以创建一个<code>yaml</code>或者<code>json</code>格式的<code>Service</code>规范文件，然后通过<code>kubectl create -f &lt;service spec file&gt;</code>创建之。一个<code>Service</code>的例子如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"selector\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"app\"</span>: <span class=\"string\">\"MyApp\"</span></span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>以上规范创建了一个名字为<code>my-service</code>的<code>Service</code>对象，它指向任何有<code>app=MyApp</code>标签的、监听<code>TCP</code>端口<code>9376</code>的任何<code>Pods</code>。</p>\n<p><code>Kubernetes</code>会自动地创建一个和<code>Service</code>名字相同的<code>Endpoints</code>对象。<code>Service</code>的<code>selector</code>会被持续地评估哪些<code>Pods</code>属于这个<code>Service</code>，结果会被更新到相应的<code>Endpoints</code>对象。</p>\n<p>当然，你也可以在定义<code>Service</code>的时候为其指定一个<code>IP</code>地址（ClusterIP，必须在kube-apiserver的<code>--service-cluster-ip-range</code>参数定义内，且不能冲突）。</p>\n<p><code>Service</code>会把到<clusterip>:<port>的流量转发到<code>targetPort</code>。缺省情况下<code>targetPort</code>等于<code>port</code>的值。一个有意思的情况是，这里你可以定义<code>targetPort</code>为一个字符串（我们可以看到<code>targetPort</code>的类型为<code>IntOrString</code>），这意味着在后端每个<code>Pods</code>中实际监听的端口值可能不一样，这极大地提高了部署<code>Service</code>的灵活性。</port></clusterip></p>\n<p><code>Kubernetes</code>的<code>Service</code>支持<code>TCP</code>和<code>UDP</code>协议，缺省是<code>TCP</code>。</p>\n<h2 id=\"3-Service发布服务的方式\"><a href=\"#3-Service发布服务的方式\" class=\"headerlink\" title=\"3. Service发布服务的方式\"></a>3. <code>Service</code>发布服务的方式</h2><p><code>Service</code>有三种类型：<code>ClusterIP，NodePort和LoadBalancer</code>。</p>\n<h3 id=\"3-1-ClusterIP\"><a href=\"#3-1-ClusterIP\" class=\"headerlink\" title=\"3.1 ClusterIP\"></a>3.1 ClusterIP</h3><p>关于<code>ClusterIP</code>：</p>\n<p>通过<code>Service</code>的<code>spec.type: ClusterIP</code>指定；<br>使用<code>cluster-internal ip</code>，即<code>kube-apiserver</code>的<code>--service-cluster-ip-range</code>参数定义的IP范围中的IP地址；<br>缺省方式；<br>只能从集群内访问，访问方式：<code>&lt;ClusterIP&gt;:&lt;Port&gt;</code>；<br><code>kube-proxy</code>会为每个<code>Service</code>，打开一个本地随机端口，通过<code>iptables</code>规则把到<code>Service</code>的流量trap到这个随机端口，由<code>kube-proxy</code>或者<code>iptables</code>接收，进而转发到后端<code>Pods</code>。</p>\n<h3 id=\"3-2-NodePort\"><a href=\"#3-2-NodePort\" class=\"headerlink\" title=\"3.2 NodePort\"></a>3.2 NodePort</h3><p>关于<code>NodePort</code>：</p>\n<p>通过<code>Service</code>的<code>spec.type: NodePort</code>指定；<br>包含<code>ClusterIP</code>功能；<br>在集群的每个节点上为<code>Service</code>开放一个端口（默认是<code>30000-32767</code>，由<code>kube-apiserver</code>的<code>--service-node-port-range</code>参数定义的节点端口范围）；<br>可从集群外部通过&lt;<code>NodeIP&gt;:&lt;NodePort&gt;</code>访问；<br>集群中的每个节点上，都会监听<code>NodePort</code>端口，因此，可以通过访问集群中的任意一个节点访问到服务。</p>\n<h3 id=\"3-3-LoadBalancer\"><a href=\"#3-3-LoadBalancer\" class=\"headerlink\" title=\"3.3 LoadBalancer\"></a>3.3 LoadBalancer</h3><p>关于<code>LoadBalancer</code>：</p>\n<p>通过<code>Service</code>的<code>spec.type: LoadBalancer</code>指定；<br>包含<code>NodePort</code>功能；<br>通过<code>Cloud Provider</code>（例如<code>GCE</code>）提供的外部<code>LoadBalancer</code>访问服务，即<code>&lt;LoadBalancerIP&gt;:&lt;Port&gt;</code>；<br><code>Service</code>通过集群的每个节点上的<code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>向外暴露；<br>有的<code>cloudprovider</code>支持直接从<code>LoadBalancer</code>转发流量到后端<code>Pods</code>（例如<code>GCE</code>），更多的是转发流量到集群节点（例如<code>AWS</code>，还有<code>HWS</code>）；<br>你可以在<code>Service</code>定义中指定<code>loadBalancerIP</code>，但这需要<code>cloudprovider</code>的支持，如果不支持则忽略。真正的<code>IP</code>在<code>status.loadBalancer.ingress.ip</code>中。<br>一个例子如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"selector\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"app\"</span>: <span class=\"string\">\"MyApp\"</span></span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"nodePort\"</span>: <span class=\"number\">30061</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        <span class=\"attr\">\"clusterIP\"</span>: <span class=\"string\">\"10.0.171.239\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"loadBalancerIP\"</span>: <span class=\"string\">\"78.11.24.19\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"type\"</span>: <span class=\"string\">\"LoadBalancer\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"status\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"loadBalancer\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"ingress\"</span>: [</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"attr\">\"ip\"</span>: <span class=\"string\">\"146.148.47.155\"</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>目前对接<code>ELB</code>的实现几大厂商，比如 <code>HW</code> 是<code>ELB</code>转发流量到集群节点，后面再由<code>kube-proxy</code>或者<code>iptables</code>转发到后端的<code>Pods</code>。</p>\n<h3 id=\"3-4-External-IPs\"><a href=\"#3-4-External-IPs\" class=\"headerlink\" title=\"3.4 External IPs\"></a>3.4 External IPs</h3><p><code>External IPs</code> 不是一种<code>Service</code>类型，它不由<code>Kubernetes</code>管理，但是我们也可以通过它暴露服务。数据包通过<code>&lt;External IP&gt;:&lt;Port&gt;</code>到达集群，然后被路由到<code>Service</code>的<code>Endpoints</code>。一个例子如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"selector\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"app\"</span>: <span class=\"string\">\"MyApp\"</span></span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"http\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        <span class=\"attr\">\"externalIPs\"</span> : [</span><br><span class=\"line\">            <span class=\"string\">\"80.11.12.10\"</span></span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-几种特殊的Service\"><a href=\"#4-几种特殊的Service\" class=\"headerlink\" title=\"4. 几种特殊的Service\"></a>4. 几种特殊的<code>Service</code></h2><h3 id=\"4-1-没有selector的Service\"><a href=\"#4-1-没有selector的Service\" class=\"headerlink\" title=\"4.1 没有selector的Service\"></a>4.1 没有<code>selector</code>的<code>Service</code></h3><p>上面说<code>Kubernetes</code>的<code>Service</code>抽象了到<code>Kubernetes</code>的<code>Pods</code>的访问。但是它也能抽象到其它类型的后端的访问。举几个场景：</p>\n<p>你想接入一个外部的数据库服务；<br>你想把一个服务指向另外一个<code>Namespace</code>或者集群的服务；<br>你把部分负载迁移到<code>Kubernetes</code>，而另外一部分后端服务运行在<code>Kubernetes</code>之外。<br>在这几种情况下，你可以定义一个没有<code>selector</code>的服务。如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>因为没有<code>selector</code>，<code>Kubernetes</code>不会自己创建<code>Endpoints</code>对象，你需要自己手动创建一个<code>Endpoints</code>对象，把Service映射到后端指定的<code>Endpoints</code>上。</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Endpoints\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"subsets\"</span>: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"addresses\"</span>: [</span><br><span class=\"line\">                &#123; <span class=\"attr\">\"IP\"</span>: <span class=\"string\">\"1.2.3.4\"</span> &#125;</span><br><span class=\"line\">            ],</span><br><span class=\"line\">            <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">                &#123; <span class=\"attr\">\"port\"</span>: <span class=\"number\">9376</span> &#125;</span><br><span class=\"line\">            ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>注意：<code>Endpoint IP</code>不能是<code>loopback（127.0.0.1）</code>地址、<code>link-local（169.254.0.0/16）</code>和<code>link-local multicast（224.0.0.0/24）</code>地址。</p>\n<p>看到这里，我们似乎明白了，这不就是<code>Kubernetes</code>提供的外部服务接入的方式吗？和<code>CloudFoundry</code>的<code>ServiceBroker</code>的功能类似。</p>\n<h3 id=\"4-2-多端口（multi-port）的Service\"><a href=\"#4-2-多端口（multi-port）的Service\" class=\"headerlink\" title=\"4.2 多端口（multi-port）的Service\"></a>4.2 多端口<code>（multi-port）</code>的<code>Service</code></h3><p><code>Kubernetes</code>的<code>Service</code>还支持多端口，比如同时暴露<code>80</code>和<code>443</code>端口。在这种情况下，你必须为每个端口定义一个名字以示区分。一个例子如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"selector\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"app\"</span>: <span class=\"string\">\"MyApp\"</span></span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"http\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span></span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"https\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">443</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9377</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>注意：</p>\n<p>多端口必须指定<code>ports.name</code>以示区分，端口名称不能一样；<br>如果是<code>spec.type: NodePort</code>，则每个端口的<code>NodePort</code>必须不一样，否则<code>Kubernetes</code>不知道一个<code>NodePort</code>对应的是后端哪个<code>targetPort</code>；<br>协议<code>protocol</code>和<code>port</code>可以一样。</p>\n<h2 id=\"4-3-Headless-services\"><a href=\"#4-3-Headless-services\" class=\"headerlink\" title=\"4.3 Headless services\"></a>4.3 <code>Headless services</code></h2><p>有时候你不想或者不需要<code>Kubernetes</code>为你的服务做负载均衡，以及一个<code>Service</code>的<code>IP</code>地址。在这种情况下，你可以创建一个<code>headless</code>的<code>Service</code>，通过指定<code>spec.clusterIP: None</code>。</p>\n<p>对这类<code>Service</code>，不会分配<code>ClusterIP</code>。对这类<code>Service</code>的<code>DNS</code>查询会返回一堆<code>A</code>记录，即后端<code>Pods</code>的<code>IP</code>地址。另外，<code>kube-proxy</code>不会处理这类<code>Service</code>，<code>Kubernetes</code>不会对这类<code>Service</code>做负载均衡或者代理。但是<code>Endpoints Controller</code>还是会为此类<code>Service</code>创建<code>Endpoints</code>对象。</p>\n<p>这允许开发者减少和k<code>ubernetes</code>的耦合性，允许他们自己做服务发现等。我在最后讨论的一些基于<code>Kubernetes</code>的容器服务，除了彻底不使用<code>Service</code>的概念外，也可以创建这类<code>headless</code>的<code>Service</code>，自己直接通过<code>LoadBalancer</code>把流量转发（负载均衡和代理）到后端<code>Pods</code>。</p>\n<h2 id=\"5-Service的流量转发模式\"><a href=\"#5-Service的流量转发模式\" class=\"headerlink\" title=\"5. Service的流量转发模式\"></a>5. <code>Service</code>的流量转发模式</h2><h3 id=\"5-1-Proxy-mode-userspace\"><a href=\"#5-1-Proxy-mode-userspace\" class=\"headerlink\" title=\"5.1 Proxy-mode: userspace\"></a>5.1 <code>Proxy-mode: userspace</code></h3><p><code>userspace</code>的代理模式是指由用户态的<code>kube-proxy</code>转发流量到后端<code>Pods</code>。如下图所示。</p>\n<p><img src=\"https://d33wubrfki0l68.cloudfront.net/e351b830334b8622a700a8da6568cb081c464a9b/13020/images/docs/services-userspace-overview.svg\" alt=\"\"></p>\n<p>关于<code>userspace</code>：</p>\n<p><code>Kube-proxy</code>通过<code>apiserver</code>监控<code>（watch）Service</code>和E<code>ndpoints</code>的变化；<br><code>Kube-proxy</code>安装<code>iptables</code>规则；<br><code>Kube-proxy</code>把访问<code>Service</code>的流量转发到后端真正的<code>Pods</code>上<code>（Round-Robin）</code>；<br><code>Kubernetes v1.0</code>只支持这种转发方式；<br>通过设置<code>service.spec.sessionAffinity</code>: <code>ClientIP</code>支持基于<code>ClientIP</code>的会话亲和性。</p>\n<h3 id=\"5-2-proxy-mode-iptables\"><a href=\"#5-2-proxy-mode-iptables\" class=\"headerlink\" title=\"5.2 proxy-mode: iptables\"></a>5.2 <code>proxy-mode: iptables</code></h3><p><code>iptables</code>的代理模式是指由内核态的<code>iptables</code>转发流量到后端<code>Pods</code>。如下图所示。</p>\n<p><img src=\"https://d33wubrfki0l68.cloudfront.net/27b2978647a8d7bdc2a96b213f0c0d3242ef9ce0/e8c9b/images/docs/services-iptables-overview.svg\" alt=\"\"></p>\n<p>关于<code>iptables</code>：</p>\n<ul>\n<li><code>Kube-proxy</code>通过<code>apiserver</code>监控<code>（watch）Service</code>和<code>Endpoints</code>的变化；</li>\n<li><code>Kube-proxy</code>安装<code>iptables</code>规则；</li>\n<li><code>iptables</code>把访问<code>Service</code>的流量转发到后端真正的<code>Pods上（Random）</code>；</li>\n<li><code>Kubernetes v1.1</code>已支持，但不是默认方式，<code>v1.2</code>中将会是默认方式；</li>\n<li>通过设置<code>service.spec.sessionAffinity</code>: <code>ClientIP</code>支持基于<code>ClientIP</code>的会话亲和性；</li>\n<li>需要<code>iptables</code>和内核版本的支持。<code>iptables &gt; 1.4.11</code>，内核支持<code>route_localnet</code>参数<code>(kernel &gt;= 3.6)</code>；</li>\n</ul>\n<p>相比<code>userspace</code>的优点：</p>\n<ul>\n<li>1，数据包不需要拷贝到用户态的<code>kube-proxy</code>再做转发，因此效率更高、更可靠。</li>\n<li>2，不修改<code>Client IP</code>。</li>\n</ul>\n<h3 id=\"5-3-proxy-mode-iptables\"><a href=\"#5-3-proxy-mode-iptables\" class=\"headerlink\" title=\"5.3 proxy-mode: iptables\"></a>5.3 <code>proxy-mode: iptables</code></h3><blockquote>\n<p>kubernetes v1.8 引入， 1.11正式可用</p>\n</blockquote>\n<p>在 <code>ipvs</code> 模式下，<code>kube-proxy</code>监视<code>Kubernetes</code>服务和端点，调用 <code>netlink</code>接口相应地创建 <code>IPVS</code> 规则， 并定期将 <code>IPVS</code> 规则与 <code>Kubernetes</code> 服务和端点同步。 该控制循环可确保　<code>IPVS</code>　状态与所需状态匹配。 访问服务时，<code>IPVS</code>　将流量定向到后端Pod之一。</p>\n<p><img src=\"https://d33wubrfki0l68.cloudfront.net/2d3d2b521cf7f9ff83238218dac1c019c270b1ed/9ac5c/images/docs/services-ipvs-overview.svg\" alt=\"\"></p>\n<p><code>IPVS</code>代理模式基于类似于 <code>iptables</code>模式的 <code>netfilter</code> 挂钩函数，但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 <code>iptables</code> 模式下的 <code>kube-proxy</code> 相比<code>，IPVS</code> 模式下的 <code>kube-proxy</code> 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，<code>IPVS</code> 模式还支持更高的网络流量吞吐量。</p>\n<p><code>IPVS</code>提供了更多选项来平衡后端<code>Pod</code>的流量。 这些是：</p>\n<ul>\n<li>rr: round-robin</li>\n<li>lc: least connection (最小连接数)</li>\n<li>dh: destination hashing（目的地址has）</li>\n<li>sh: source hashing（源地址has）</li>\n<li>等等 </li>\n</ul>\n<h3 id=\"5-3-userspace和iptables转发方式的主要不同点\"><a href=\"#5-3-userspace和iptables转发方式的主要不同点\" class=\"headerlink\" title=\"5.3 userspace和iptables转发方式的主要不同点\"></a>5.3 <code>userspace</code>和<code>iptables</code>转发方式的主要不同点</h3><p><code>userspace</code>和<code>iptables</code>转发方式的主要不同点如下：</p>\n<table>\n<thead>\n<tr>\n<th>比较项</th>\n<th><code>userspace</code></th>\n<th><code>iptables</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p> 谁转发流量到<code>Pods</code>|    <code>kube-proxy</code>把访问<code>Service</code>的流量转发到后端真正的<code>Pods</code>上 |<code>iptables</code>把访问<code>Service</code>的流量转发到后端真正的<code>Pods</code>上<br>|转发算法    |轮询<code>Round-Robin</code>|    随机<code>Random</code><br>|用户态和内核态    |数据包需要拷贝到用户态的<code>kube-proxy</code>再做转发，因此效率低、不可靠    |数据包直接在内核态转发，因此效率更高、更可靠<br>|是否修改<code>Client IP</code>    |因为<code>kube-proxy</code>在中间做代理，会修改数据包的<code>Client IP</code>|    不修改数据包的<code>Client IP</code><br><code>iptables</code>版本和内核支持|    不依赖|    <code>iptables &gt; 1.4.11</code>，内核支持<code>route_localnet</code>参数(<code>kernel &gt;= 3.6</code>)</p>\n<p>通过设置<code>kube-proxy</code>的启动参数<code>--proxy-mode</code>设定使用<code>userspace</code>还是<code>iptables</code>代理模式。</p>\n<h2 id=\"6-Service发现方式\"><a href=\"#6-Service发现方式\" class=\"headerlink\" title=\"6. Service发现方式\"></a>6. Service发现方式</h2><p>现在服务创建了，得让别人来使用了。别人要使用首先得知道这些服务呀，服务治理很基本的一个功能就是提供服务发现。<code>Kubernetes</code>为我们提供了两种基本的服务发现方式：环境变量和<code>DNS</code>。</p>\n<h3 id=\"6-1-环境变量\"><a href=\"#6-1-环境变量\" class=\"headerlink\" title=\"6.1 环境变量\"></a>6.1 环境变量</h3><p>当一个<code>Pod</code>在节点Node上运行时，<code>kubelet</code>会为每个活动的服务设置一系列的环境变量。它支持<code>Docker links compatible</code>变量，以及更简单的<code>{SVCNAME}_SERVICE_HOST</code>和<code>{SVCNAME}_SERVICE_PORT</code>变量。后者是把服务名字大写，然后把中划线（-）转换为下划线（_）。</p>\n<p>以服务<code>redis-master</code>为例，它暴露TCP协议的6379端口，被分配了集群IP地址10.0.0.11，则会创建如下环境变量：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">REDIS_MASTER_SERVICE_HOST=10.0.0.11</span><br><span class=\"line\">REDIS_MASTER_SERVICE_PORT=6379</span><br><span class=\"line\">REDIS_MASTER_PORT=tcp://10.0.0.11:6379</span><br><span class=\"line\">REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379</span><br><span class=\"line\">REDIS_MASTER_PORT_6379_TCP_PROTO=tcp</span><br><span class=\"line\">REDIS_MASTER_PORT_6379_TCP_PORT=6379</span><br><span class=\"line\">REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11</span><br></pre></td></tr></table></figure>\n<p>这里有一个注意点是，如果一个<code>Pod</code>要访问一个<code>Service</code>，则必须在该Service之前创建，否则这些环境变量不会被注入此Pod。DNS方式的服务发现就没有此限制。</p>\n<h2 id=\"6-2-DNS\"><a href=\"#6-2-DNS\" class=\"headerlink\" title=\"6.2 DNS\"></a>6.2 DNS</h2><p>虽然<code>DNS</code>是一个<code>cluster add-on</code>特性，但是我们还是强烈推荐使用<code>DNS</code>作为服务发现的方式。<code>DNS</code>服务器通过<code>KubernetesAPI</code>监控新的<code>Service</code>的生成，然后为每个<code>Service</code>设置一堆<code>DNS</code>记录。如果集群设置了<code>DNS</code>，则该集群中所有的<code>Pods</code>都能够使用<code>DNS</code>解析<code>Sercice</code>。</p>\n<p>例如，如果在<code>Kubernertes</code>中的<code>my-ns</code>名字空间中有一个服务叫做<code>my-service</code>，则会创建一个<code>my-service.my-ns</code>的<code>DNS</code>记录。在同一个名字空间<code>my-ns</code>的<code>Pods</code>能直接通过服务名<code>my-service</code>查找到该服务。如果是其它的<code>Namespace</code>中的<code>Pods</code>，则需加上名字空间，例如<code>my-service.my-ns</code>。返回的结果是服务的<code>ClusterIP</code>。当然，对于我们上面讲的<code>headless</code>的<code>Service</code>，返回的则是该<code>Service</code>对应的一堆后端<code>Pods</code>的<code>IP</code>地址。</p>\n<p>对于知名服务端口，<code>Kubernetes</code>还支持<code>DNS SRV</code>记录。例如<code>my-service.my-ns</code>的服务支持<code>TCP</code>协议的<code>http</code>端口，则你可以通过一个<code>DNS SRV</code>查询<code>_http._tcp.my-service.my-ns</code>来发现http的端口。</p>\n<p>对于每个<code>Service</code>的<code>DNS</code>记录，<code>Kubernetes</code>还会加上一个集群域名的后缀，作为完全域名（FQDN）。这个集群域名通过svc+安装集群<code>DNS</code>的<code>DNS_DOMAIN</code>参数指定，默认是<code>svc.cluster.local</code>。如果不是一个标准的<code>Kubernetes</code>支持的安装，则启动<code>kubelet</code>的时候指定参数<code>--cluster-domain</code>，你还需要指定<code>--cluster-dns</code>告诉<code>kubelet</code>集群<code>DNS</code>的地址。</p>\n<h2 id=\"6-3-如何发现和使用服务？\"><a href=\"#6-3-如何发现和使用服务？\" class=\"headerlink\" title=\"6.3 如何发现和使用服务？\"></a>6.3 如何发现和使用服务？</h2><p>一般在创建<code>Pod</code>的时候，指定一个环境变量<code>GET_HOSTS_FROM</code>，值可以设为<code>env</code>或者<code>dns</code>。在<code>Pod</code>中的应用先获取这个环境变量，得到获取服务的方式。如果是<code>env</code>，则通过<code>getenv</code>获取相应的服务的环境变量，例如<code>REDIS_SLAVE_SERVICE_HOST</code>；如果是<code>dns</code>，则可以在<code>Pod</code>内通过标准的<code>gethostbyname</code>获取服务主机名。有个例外是P<code>od</code>的定义中，不能设置h<code>ostNetwork: true</code>。</p>\n<p>获取到服务的地址，就可以通过正常方式使用服务了。</p>\n<p>如下是<code>Kubernetes</code>自带的<code>guestbook.php</code>中的一段相关代码，供参考：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\">host = <span class=\"string\">'redis-slave'</span>;</span></span><br><span class=\"line\">if (getenv('GET_HOSTS_FROM') == 'env') &#123;</span><br><span class=\"line\"><span class=\"meta\">  $</span><span class=\"bash\">host = getenv(<span class=\"string\">'REDIS_SLAVE_SERVICE_HOST'</span>);</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\">client = new Predis\\Client([</span></span><br><span class=\"line\">  'scheme' =&gt; 'tcp',</span><br><span class=\"line\">  'host'   =&gt; $host,</span><br><span class=\"line\">  'port'   =&gt; 6379,</span><br><span class=\"line\">]);</span><br></pre></td></tr></table></figure>\n<h2 id=\"7-一些容器服务中的Service\"><a href=\"#7-一些容器服务中的Service\" class=\"headerlink\" title=\"7. 一些容器服务中的Service\"></a>7. 一些容器服务中的<code>Service</code></h2><p>虽然<code>Service</code>在<code>Kubernetes</code>中如此重要，但是对一些基于<code>Kubernetes</code>的容器服务，并没有使用<code>Service</code>，或者用的是上面讨论的<code>headless</code>类型的<code>Service</code>。这种方式基本上是把容器当做<code>VM</code>使用的典型，<code>LoadBalancer</code>和<code>Pods</code>网络互通，通过<code>LoadBalancer</code>直接把流量转发到<code>Pods</code>上，省却了中间由<code>kube-proxy</code>或者<code>iptables</code>的转发方式，从而提高了流量转发效率，但是也由<code>LoadBalancer</code>自己提供对后端P<code>ods</code>的维护，一般需要<code>LoadBalancer</code>提供动态路由的功能（即后端<code>Pods</code>可以动态地从<code>LoadBalancer</code>上注册/注销）。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"1-Service介绍\"><a href=\"#1-Service介绍\" class=\"headerlink\" title=\"1. Service介绍\"></a>1. Service介绍</h2><p><code>Kubernetes</code> 中有很多概念，例如 <code>ReplicationController、Service、Pod</code>等。我认为 <code>Service</code> 是 <code>Kubernetes</code> 中最重要的概念，没有之一。</p>\n<p>为什么 <code>Service</code> 如此重要？因为它解耦了前端用户和后端真正提供服务的 <code>Pods</code> 。在进一步理解 <code>Service</code> 之前，我们先简单地了解下 <code>Pod</code> ， <code>Pod</code> 也是 <code>Kubernetes</code> 中很重要的概念之一。</p>\n<p>在 <code>Kubernetes</code> 中，<code>Pod</code> 是能够创建、调度、和管理的最小部署单元，而不是单独的应用容器。<code>Pod</code> 是容器组，一个<code>Pod</code>中容器运行在一个共享的应用上下文中。这里的共享上下文是为多个 <code>Linux Namespace</code> 的联合。例如：</p>\n<ul>\n<li><code>PID</code>命名空间（在同一个 <code>Pod</code> 中的应用可以看到其它应用的进程）</li>\n<li><code>Network</code>名字空间（在同一个 <code>Pod</code> 中的应用可以访问同样的IP和端口空间）</li>\n<li><code>IPC</code>命名空间（在同一个 <code>Pod</code> 中的应用可以使用<code>SystemV IPC</code>或者<code>POSIX</code>消息队列进行通信）</li>\n<li><code>UTS</code>命名空间（在同一个 <code>Pod</code> 中的应用可以共享一个主机名称）</li>\n<li><code>Pod</code> 是一个和应用相关的“逻辑主机”， <code>Pod</code> 中的容器共享一个网络名字空间。 <code>Pod</code> 为它的组件之间的数据共享、通信和管理提供了便利。</li>\n</ul>\n<p>我们可以看出， <code>Pod</code> 在资源层面抽象了容器。</p>\n<p>因此，在<code>Kubernetes</code>中，让我们暂时先忘记容器，记住 <code>Pod</code> 。</p>\n<p><code>Kubernetes</code>对<code>Service</code>的定义是：<code>Service  is an abstraction which defines a logical set of  Pods  and a policy by which to access them。</code>我们下面理解下这句话。</p>\n<p>刚开始的时候，生活其实是很简单的。一个提供特定服务的进程，运行在一个容器中，监听容器的<code>IP</code>地址和端口号。客户端通过<code>&lt;Container IP&gt;:&lt;ContainerPort&gt;</code>，或者通过使用Docker的端口映射<code>&lt;Host IP&gt;:&lt;Host Port&gt;</code>就可以访问到这个服务了。<code>The simplest, the best</code>，简单的生活很美好。</p>\n<p><img src=\"https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/A1HKVXsfHNmswyx38Qh8WVkLPHpr9pex3e7DRk9H0AicQGXP2r1xP8ibkx5QqSnsMc6pQf3wbHPAKcxibFVicz1ChA/640\" alt=\"\"></p>\n<p>但是美好的日子总是短暂的。小伙伴们太热情了，单独由一个容器提供服务不够用了，怎么办？很简单啊，由多个容器提供服务不就可以了吗。问题似乎得到了解决。</p>\n<p>可是那么多的容器，客户端到底访问哪个容器中提供的服务呢？访问容器的请求不均衡怎么办？假如容器所在的主机故障了，容器在另外一台主机上拉起了，这个时候容器的IP地址变了，客户端怎么维护这个容器列表呢？所以，由多个容器提供服务的情况下，一般有两种做法：</p>\n<p>客户端自己维护提供服务的容器列表，自己做负载均衡，某个容器故障时自己做故障转移；<br>提供一个负载均衡器，解耦用户和后端提供服务的容器。负载均衡器负责向后端容器转发流量，并对后端容器进行健康检查。客户端只要访问负载均衡器的IP和端口号就行了。<br>我们在前面说Service解耦了前端用户和后端真正提供服务的 <code>Pod</code> s。从这个意义上讲，<code>Service</code>就是<code>Kubernetes</code>中 <code>Pod</code> 的负载均衡器。</p>\n<p>从生命周期来说， <code>Pod</code> 是短暂的而不是长久的应用。 <code>Pod</code> 被调度到节点，保持在这个节点上直到被销毁。当节点死亡时，分配到这个节点的 <code>Pod</code> 将会被删掉。</p>\n<p>但<code>Service</code>在其生命周期内，<code>IP</code>地址是稳定的。对于<code>Kubernetes</code>原生的应用，<code>Kubernetes</code>提供了一个<code>Endpoints</code>的对象，这个<code>Endpoints</code>的名字和<code>Service</code>的名字相同，它是一个<pod ip=\"\">:<targetport>的列表，负责维护<code>Service</code>后端的<code>Pods</code>的变化。</targetport></pod></p>\n<p>总结一下，<code>Service</code>解耦了前端用户和后端真正提供服务的<code>Pods</code>，<code>Pod</code>在资源层面抽象了容器。由于它们的存在，使得这个简单的世界变得复杂了。</p>\n<p>对了，<code>Service</code>怎么知道是哪些后端的Pods在真正提供自己定义的服务呢？在创建<code>Pods</code>的时候，会定义一些label；在创建<code>Service</code>的时候，会定义L<code>abel Selector，Kubernetes</code>就是通过<code>Label Selector</code>来匹配后端真正服务于<code>Service</code>的后端<code>Pods</code>的。</p>\n<h2 id=\"2-定义一个Service\"><a href=\"#2-定义一个Service\" class=\"headerlink\" title=\"2. 定义一个Service\"></a>2. 定义一个<code>Service</code></h2><p>接下来就有点没意思了，我要开始翻译上面说的很重要的<code>Services in Kubernetes</code>了。当然，我会加入自己的理解。</p>\n<p><code>Service</code>也是<code>Kubernetes</code>中的一个<code>REST</code>对象。可以通过向<code>apiserver</code>发送<code>POST</code>请求进行创建。当然，<code>Kubernetes</code>为我们提供了一个强大和好用的客户端<code>kubectl</code>，代替我们向<code>apiserver</code>发送请求。但是<code>kubectl</code>不仅仅是一个简单的<code>apiserver</code>客户端，它为我们提供了很多额外的功能，例如<code>rolling update</code>等。</p>\n<p>我们可以创建一个<code>yaml</code>或者<code>json</code>格式的<code>Service</code>规范文件，然后通过<code>kubectl create -f &lt;service spec file&gt;</code>创建之。一个<code>Service</code>的例子如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"selector\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"app\"</span>: <span class=\"string\">\"MyApp\"</span></span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>以上规范创建了一个名字为<code>my-service</code>的<code>Service</code>对象，它指向任何有<code>app=MyApp</code>标签的、监听<code>TCP</code>端口<code>9376</code>的任何<code>Pods</code>。</p>\n<p><code>Kubernetes</code>会自动地创建一个和<code>Service</code>名字相同的<code>Endpoints</code>对象。<code>Service</code>的<code>selector</code>会被持续地评估哪些<code>Pods</code>属于这个<code>Service</code>，结果会被更新到相应的<code>Endpoints</code>对象。</p>\n<p>当然，你也可以在定义<code>Service</code>的时候为其指定一个<code>IP</code>地址（ClusterIP，必须在kube-apiserver的<code>--service-cluster-ip-range</code>参数定义内，且不能冲突）。</p>\n<p><code>Service</code>会把到<clusterip>:<port>的流量转发到<code>targetPort</code>。缺省情况下<code>targetPort</code>等于<code>port</code>的值。一个有意思的情况是，这里你可以定义<code>targetPort</code>为一个字符串（我们可以看到<code>targetPort</code>的类型为<code>IntOrString</code>），这意味着在后端每个<code>Pods</code>中实际监听的端口值可能不一样，这极大地提高了部署<code>Service</code>的灵活性。</port></clusterip></p>\n<p><code>Kubernetes</code>的<code>Service</code>支持<code>TCP</code>和<code>UDP</code>协议，缺省是<code>TCP</code>。</p>\n<h2 id=\"3-Service发布服务的方式\"><a href=\"#3-Service发布服务的方式\" class=\"headerlink\" title=\"3. Service发布服务的方式\"></a>3. <code>Service</code>发布服务的方式</h2><p><code>Service</code>有三种类型：<code>ClusterIP，NodePort和LoadBalancer</code>。</p>\n<h3 id=\"3-1-ClusterIP\"><a href=\"#3-1-ClusterIP\" class=\"headerlink\" title=\"3.1 ClusterIP\"></a>3.1 ClusterIP</h3><p>关于<code>ClusterIP</code>：</p>\n<p>通过<code>Service</code>的<code>spec.type: ClusterIP</code>指定；<br>使用<code>cluster-internal ip</code>，即<code>kube-apiserver</code>的<code>--service-cluster-ip-range</code>参数定义的IP范围中的IP地址；<br>缺省方式；<br>只能从集群内访问，访问方式：<code>&lt;ClusterIP&gt;:&lt;Port&gt;</code>；<br><code>kube-proxy</code>会为每个<code>Service</code>，打开一个本地随机端口，通过<code>iptables</code>规则把到<code>Service</code>的流量trap到这个随机端口，由<code>kube-proxy</code>或者<code>iptables</code>接收，进而转发到后端<code>Pods</code>。</p>\n<h3 id=\"3-2-NodePort\"><a href=\"#3-2-NodePort\" class=\"headerlink\" title=\"3.2 NodePort\"></a>3.2 NodePort</h3><p>关于<code>NodePort</code>：</p>\n<p>通过<code>Service</code>的<code>spec.type: NodePort</code>指定；<br>包含<code>ClusterIP</code>功能；<br>在集群的每个节点上为<code>Service</code>开放一个端口（默认是<code>30000-32767</code>，由<code>kube-apiserver</code>的<code>--service-node-port-range</code>参数定义的节点端口范围）；<br>可从集群外部通过&lt;<code>NodeIP&gt;:&lt;NodePort&gt;</code>访问；<br>集群中的每个节点上，都会监听<code>NodePort</code>端口，因此，可以通过访问集群中的任意一个节点访问到服务。</p>\n<h3 id=\"3-3-LoadBalancer\"><a href=\"#3-3-LoadBalancer\" class=\"headerlink\" title=\"3.3 LoadBalancer\"></a>3.3 LoadBalancer</h3><p>关于<code>LoadBalancer</code>：</p>\n<p>通过<code>Service</code>的<code>spec.type: LoadBalancer</code>指定；<br>包含<code>NodePort</code>功能；<br>通过<code>Cloud Provider</code>（例如<code>GCE</code>）提供的外部<code>LoadBalancer</code>访问服务，即<code>&lt;LoadBalancerIP&gt;:&lt;Port&gt;</code>；<br><code>Service</code>通过集群的每个节点上的<code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>向外暴露；<br>有的<code>cloudprovider</code>支持直接从<code>LoadBalancer</code>转发流量到后端<code>Pods</code>（例如<code>GCE</code>），更多的是转发流量到集群节点（例如<code>AWS</code>，还有<code>HWS</code>）；<br>你可以在<code>Service</code>定义中指定<code>loadBalancerIP</code>，但这需要<code>cloudprovider</code>的支持，如果不支持则忽略。真正的<code>IP</code>在<code>status.loadBalancer.ingress.ip</code>中。<br>一个例子如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"selector\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"app\"</span>: <span class=\"string\">\"MyApp\"</span></span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"nodePort\"</span>: <span class=\"number\">30061</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        <span class=\"attr\">\"clusterIP\"</span>: <span class=\"string\">\"10.0.171.239\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"loadBalancerIP\"</span>: <span class=\"string\">\"78.11.24.19\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"type\"</span>: <span class=\"string\">\"LoadBalancer\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"status\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"loadBalancer\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"ingress\"</span>: [</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    <span class=\"attr\">\"ip\"</span>: <span class=\"string\">\"146.148.47.155\"</span></span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>目前对接<code>ELB</code>的实现几大厂商，比如 <code>HW</code> 是<code>ELB</code>转发流量到集群节点，后面再由<code>kube-proxy</code>或者<code>iptables</code>转发到后端的<code>Pods</code>。</p>\n<h3 id=\"3-4-External-IPs\"><a href=\"#3-4-External-IPs\" class=\"headerlink\" title=\"3.4 External IPs\"></a>3.4 External IPs</h3><p><code>External IPs</code> 不是一种<code>Service</code>类型，它不由<code>Kubernetes</code>管理，但是我们也可以通过它暴露服务。数据包通过<code>&lt;External IP&gt;:&lt;Port&gt;</code>到达集群，然后被路由到<code>Service</code>的<code>Endpoints</code>。一个例子如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"selector\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"app\"</span>: <span class=\"string\">\"MyApp\"</span></span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"http\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        <span class=\"attr\">\"externalIPs\"</span> : [</span><br><span class=\"line\">            <span class=\"string\">\"80.11.12.10\"</span></span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4-几种特殊的Service\"><a href=\"#4-几种特殊的Service\" class=\"headerlink\" title=\"4. 几种特殊的Service\"></a>4. 几种特殊的<code>Service</code></h2><h3 id=\"4-1-没有selector的Service\"><a href=\"#4-1-没有selector的Service\" class=\"headerlink\" title=\"4.1 没有selector的Service\"></a>4.1 没有<code>selector</code>的<code>Service</code></h3><p>上面说<code>Kubernetes</code>的<code>Service</code>抽象了到<code>Kubernetes</code>的<code>Pods</code>的访问。但是它也能抽象到其它类型的后端的访问。举几个场景：</p>\n<p>你想接入一个外部的数据库服务；<br>你想把一个服务指向另外一个<code>Namespace</code>或者集群的服务；<br>你把部分负载迁移到<code>Kubernetes</code>，而另外一部分后端服务运行在<code>Kubernetes</code>之外。<br>在这几种情况下，你可以定义一个没有<code>selector</code>的服务。如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>因为没有<code>selector</code>，<code>Kubernetes</code>不会自己创建<code>Endpoints</code>对象，你需要自己手动创建一个<code>Endpoints</code>对象，把Service映射到后端指定的<code>Endpoints</code>上。</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Endpoints\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"subsets\"</span>: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"addresses\"</span>: [</span><br><span class=\"line\">                &#123; <span class=\"attr\">\"IP\"</span>: <span class=\"string\">\"1.2.3.4\"</span> &#125;</span><br><span class=\"line\">            ],</span><br><span class=\"line\">            <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">                &#123; <span class=\"attr\">\"port\"</span>: <span class=\"number\">9376</span> &#125;</span><br><span class=\"line\">            ]</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>注意：<code>Endpoint IP</code>不能是<code>loopback（127.0.0.1）</code>地址、<code>link-local（169.254.0.0/16）</code>和<code>link-local multicast（224.0.0.0/24）</code>地址。</p>\n<p>看到这里，我们似乎明白了，这不就是<code>Kubernetes</code>提供的外部服务接入的方式吗？和<code>CloudFoundry</code>的<code>ServiceBroker</code>的功能类似。</p>\n<h3 id=\"4-2-多端口（multi-port）的Service\"><a href=\"#4-2-多端口（multi-port）的Service\" class=\"headerlink\" title=\"4.2 多端口（multi-port）的Service\"></a>4.2 多端口<code>（multi-port）</code>的<code>Service</code></h3><p><code>Kubernetes</code>的<code>Service</code>还支持多端口，比如同时暴露<code>80</code>和<code>443</code>端口。在这种情况下，你必须为每个端口定义一个名字以示区分。一个例子如下：</p>\n<figure class=\"highlight json\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"kind\"</span>: <span class=\"string\">\"Service\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"apiVersion\"</span>: <span class=\"string\">\"v1\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"metadata\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"my-service\"</span></span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">\"spec\"</span>: &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"selector\"</span>: &#123;</span><br><span class=\"line\">            <span class=\"attr\">\"app\"</span>: <span class=\"string\">\"MyApp\"</span></span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        <span class=\"attr\">\"ports\"</span>: [</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"http\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">80</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9376</span></span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"https\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"protocol\"</span>: <span class=\"string\">\"TCP\"</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"port\"</span>: <span class=\"number\">443</span>,</span><br><span class=\"line\">                <span class=\"attr\">\"targetPort\"</span>: <span class=\"number\">9377</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>注意：</p>\n<p>多端口必须指定<code>ports.name</code>以示区分，端口名称不能一样；<br>如果是<code>spec.type: NodePort</code>，则每个端口的<code>NodePort</code>必须不一样，否则<code>Kubernetes</code>不知道一个<code>NodePort</code>对应的是后端哪个<code>targetPort</code>；<br>协议<code>protocol</code>和<code>port</code>可以一样。</p>\n<h2 id=\"4-3-Headless-services\"><a href=\"#4-3-Headless-services\" class=\"headerlink\" title=\"4.3 Headless services\"></a>4.3 <code>Headless services</code></h2><p>有时候你不想或者不需要<code>Kubernetes</code>为你的服务做负载均衡，以及一个<code>Service</code>的<code>IP</code>地址。在这种情况下，你可以创建一个<code>headless</code>的<code>Service</code>，通过指定<code>spec.clusterIP: None</code>。</p>\n<p>对这类<code>Service</code>，不会分配<code>ClusterIP</code>。对这类<code>Service</code>的<code>DNS</code>查询会返回一堆<code>A</code>记录，即后端<code>Pods</code>的<code>IP</code>地址。另外，<code>kube-proxy</code>不会处理这类<code>Service</code>，<code>Kubernetes</code>不会对这类<code>Service</code>做负载均衡或者代理。但是<code>Endpoints Controller</code>还是会为此类<code>Service</code>创建<code>Endpoints</code>对象。</p>\n<p>这允许开发者减少和k<code>ubernetes</code>的耦合性，允许他们自己做服务发现等。我在最后讨论的一些基于<code>Kubernetes</code>的容器服务，除了彻底不使用<code>Service</code>的概念外，也可以创建这类<code>headless</code>的<code>Service</code>，自己直接通过<code>LoadBalancer</code>把流量转发（负载均衡和代理）到后端<code>Pods</code>。</p>\n<h2 id=\"5-Service的流量转发模式\"><a href=\"#5-Service的流量转发模式\" class=\"headerlink\" title=\"5. Service的流量转发模式\"></a>5. <code>Service</code>的流量转发模式</h2><h3 id=\"5-1-Proxy-mode-userspace\"><a href=\"#5-1-Proxy-mode-userspace\" class=\"headerlink\" title=\"5.1 Proxy-mode: userspace\"></a>5.1 <code>Proxy-mode: userspace</code></h3><p><code>userspace</code>的代理模式是指由用户态的<code>kube-proxy</code>转发流量到后端<code>Pods</code>。如下图所示。</p>\n<p><img src=\"https://d33wubrfki0l68.cloudfront.net/e351b830334b8622a700a8da6568cb081c464a9b/13020/images/docs/services-userspace-overview.svg\" alt=\"\"></p>\n<p>关于<code>userspace</code>：</p>\n<p><code>Kube-proxy</code>通过<code>apiserver</code>监控<code>（watch）Service</code>和E<code>ndpoints</code>的变化；<br><code>Kube-proxy</code>安装<code>iptables</code>规则；<br><code>Kube-proxy</code>把访问<code>Service</code>的流量转发到后端真正的<code>Pods</code>上<code>（Round-Robin）</code>；<br><code>Kubernetes v1.0</code>只支持这种转发方式；<br>通过设置<code>service.spec.sessionAffinity</code>: <code>ClientIP</code>支持基于<code>ClientIP</code>的会话亲和性。</p>\n<h3 id=\"5-2-proxy-mode-iptables\"><a href=\"#5-2-proxy-mode-iptables\" class=\"headerlink\" title=\"5.2 proxy-mode: iptables\"></a>5.2 <code>proxy-mode: iptables</code></h3><p><code>iptables</code>的代理模式是指由内核态的<code>iptables</code>转发流量到后端<code>Pods</code>。如下图所示。</p>\n<p><img src=\"https://d33wubrfki0l68.cloudfront.net/27b2978647a8d7bdc2a96b213f0c0d3242ef9ce0/e8c9b/images/docs/services-iptables-overview.svg\" alt=\"\"></p>\n<p>关于<code>iptables</code>：</p>\n<ul>\n<li><code>Kube-proxy</code>通过<code>apiserver</code>监控<code>（watch）Service</code>和<code>Endpoints</code>的变化；</li>\n<li><code>Kube-proxy</code>安装<code>iptables</code>规则；</li>\n<li><code>iptables</code>把访问<code>Service</code>的流量转发到后端真正的<code>Pods上（Random）</code>；</li>\n<li><code>Kubernetes v1.1</code>已支持，但不是默认方式，<code>v1.2</code>中将会是默认方式；</li>\n<li>通过设置<code>service.spec.sessionAffinity</code>: <code>ClientIP</code>支持基于<code>ClientIP</code>的会话亲和性；</li>\n<li>需要<code>iptables</code>和内核版本的支持。<code>iptables &gt; 1.4.11</code>，内核支持<code>route_localnet</code>参数<code>(kernel &gt;= 3.6)</code>；</li>\n</ul>\n<p>相比<code>userspace</code>的优点：</p>\n<ul>\n<li>1，数据包不需要拷贝到用户态的<code>kube-proxy</code>再做转发，因此效率更高、更可靠。</li>\n<li>2，不修改<code>Client IP</code>。</li>\n</ul>\n<h3 id=\"5-3-proxy-mode-iptables\"><a href=\"#5-3-proxy-mode-iptables\" class=\"headerlink\" title=\"5.3 proxy-mode: iptables\"></a>5.3 <code>proxy-mode: iptables</code></h3><blockquote>\n<p>kubernetes v1.8 引入， 1.11正式可用</p>\n</blockquote>\n<p>在 <code>ipvs</code> 模式下，<code>kube-proxy</code>监视<code>Kubernetes</code>服务和端点，调用 <code>netlink</code>接口相应地创建 <code>IPVS</code> 规则， 并定期将 <code>IPVS</code> 规则与 <code>Kubernetes</code> 服务和端点同步。 该控制循环可确保　<code>IPVS</code>　状态与所需状态匹配。 访问服务时，<code>IPVS</code>　将流量定向到后端Pod之一。</p>\n<p><img src=\"https://d33wubrfki0l68.cloudfront.net/2d3d2b521cf7f9ff83238218dac1c019c270b1ed/9ac5c/images/docs/services-ipvs-overview.svg\" alt=\"\"></p>\n<p><code>IPVS</code>代理模式基于类似于 <code>iptables</code>模式的 <code>netfilter</code> 挂钩函数，但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 <code>iptables</code> 模式下的 <code>kube-proxy</code> 相比<code>，IPVS</code> 模式下的 <code>kube-proxy</code> 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，<code>IPVS</code> 模式还支持更高的网络流量吞吐量。</p>\n<p><code>IPVS</code>提供了更多选项来平衡后端<code>Pod</code>的流量。 这些是：</p>\n<ul>\n<li>rr: round-robin</li>\n<li>lc: least connection (最小连接数)</li>\n<li>dh: destination hashing（目的地址has）</li>\n<li>sh: source hashing（源地址has）</li>\n<li>等等 </li>\n</ul>\n<h3 id=\"5-3-userspace和iptables转发方式的主要不同点\"><a href=\"#5-3-userspace和iptables转发方式的主要不同点\" class=\"headerlink\" title=\"5.3 userspace和iptables转发方式的主要不同点\"></a>5.3 <code>userspace</code>和<code>iptables</code>转发方式的主要不同点</h3><p><code>userspace</code>和<code>iptables</code>转发方式的主要不同点如下：</p>\n<table>\n<thead>\n<tr>\n<th>比较项</th>\n<th><code>userspace</code></th>\n<th><code>iptables</code></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p> 谁转发流量到<code>Pods</code>|    <code>kube-proxy</code>把访问<code>Service</code>的流量转发到后端真正的<code>Pods</code>上 |<code>iptables</code>把访问<code>Service</code>的流量转发到后端真正的<code>Pods</code>上<br>|转发算法    |轮询<code>Round-Robin</code>|    随机<code>Random</code><br>|用户态和内核态    |数据包需要拷贝到用户态的<code>kube-proxy</code>再做转发，因此效率低、不可靠    |数据包直接在内核态转发，因此效率更高、更可靠<br>|是否修改<code>Client IP</code>    |因为<code>kube-proxy</code>在中间做代理，会修改数据包的<code>Client IP</code>|    不修改数据包的<code>Client IP</code><br><code>iptables</code>版本和内核支持|    不依赖|    <code>iptables &gt; 1.4.11</code>，内核支持<code>route_localnet</code>参数(<code>kernel &gt;= 3.6</code>)</p>\n<p>通过设置<code>kube-proxy</code>的启动参数<code>--proxy-mode</code>设定使用<code>userspace</code>还是<code>iptables</code>代理模式。</p>\n<h2 id=\"6-Service发现方式\"><a href=\"#6-Service发现方式\" class=\"headerlink\" title=\"6. Service发现方式\"></a>6. Service发现方式</h2><p>现在服务创建了，得让别人来使用了。别人要使用首先得知道这些服务呀，服务治理很基本的一个功能就是提供服务发现。<code>Kubernetes</code>为我们提供了两种基本的服务发现方式：环境变量和<code>DNS</code>。</p>\n<h3 id=\"6-1-环境变量\"><a href=\"#6-1-环境变量\" class=\"headerlink\" title=\"6.1 环境变量\"></a>6.1 环境变量</h3><p>当一个<code>Pod</code>在节点Node上运行时，<code>kubelet</code>会为每个活动的服务设置一系列的环境变量。它支持<code>Docker links compatible</code>变量，以及更简单的<code>{SVCNAME}_SERVICE_HOST</code>和<code>{SVCNAME}_SERVICE_PORT</code>变量。后者是把服务名字大写，然后把中划线（-）转换为下划线（_）。</p>\n<p>以服务<code>redis-master</code>为例，它暴露TCP协议的6379端口，被分配了集群IP地址10.0.0.11，则会创建如下环境变量：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">REDIS_MASTER_SERVICE_HOST=10.0.0.11</span><br><span class=\"line\">REDIS_MASTER_SERVICE_PORT=6379</span><br><span class=\"line\">REDIS_MASTER_PORT=tcp://10.0.0.11:6379</span><br><span class=\"line\">REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379</span><br><span class=\"line\">REDIS_MASTER_PORT_6379_TCP_PROTO=tcp</span><br><span class=\"line\">REDIS_MASTER_PORT_6379_TCP_PORT=6379</span><br><span class=\"line\">REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11</span><br></pre></td></tr></table></figure>\n<p>这里有一个注意点是，如果一个<code>Pod</code>要访问一个<code>Service</code>，则必须在该Service之前创建，否则这些环境变量不会被注入此Pod。DNS方式的服务发现就没有此限制。</p>\n<h2 id=\"6-2-DNS\"><a href=\"#6-2-DNS\" class=\"headerlink\" title=\"6.2 DNS\"></a>6.2 DNS</h2><p>虽然<code>DNS</code>是一个<code>cluster add-on</code>特性，但是我们还是强烈推荐使用<code>DNS</code>作为服务发现的方式。<code>DNS</code>服务器通过<code>KubernetesAPI</code>监控新的<code>Service</code>的生成，然后为每个<code>Service</code>设置一堆<code>DNS</code>记录。如果集群设置了<code>DNS</code>，则该集群中所有的<code>Pods</code>都能够使用<code>DNS</code>解析<code>Sercice</code>。</p>\n<p>例如，如果在<code>Kubernertes</code>中的<code>my-ns</code>名字空间中有一个服务叫做<code>my-service</code>，则会创建一个<code>my-service.my-ns</code>的<code>DNS</code>记录。在同一个名字空间<code>my-ns</code>的<code>Pods</code>能直接通过服务名<code>my-service</code>查找到该服务。如果是其它的<code>Namespace</code>中的<code>Pods</code>，则需加上名字空间，例如<code>my-service.my-ns</code>。返回的结果是服务的<code>ClusterIP</code>。当然，对于我们上面讲的<code>headless</code>的<code>Service</code>，返回的则是该<code>Service</code>对应的一堆后端<code>Pods</code>的<code>IP</code>地址。</p>\n<p>对于知名服务端口，<code>Kubernetes</code>还支持<code>DNS SRV</code>记录。例如<code>my-service.my-ns</code>的服务支持<code>TCP</code>协议的<code>http</code>端口，则你可以通过一个<code>DNS SRV</code>查询<code>_http._tcp.my-service.my-ns</code>来发现http的端口。</p>\n<p>对于每个<code>Service</code>的<code>DNS</code>记录，<code>Kubernetes</code>还会加上一个集群域名的后缀，作为完全域名（FQDN）。这个集群域名通过svc+安装集群<code>DNS</code>的<code>DNS_DOMAIN</code>参数指定，默认是<code>svc.cluster.local</code>。如果不是一个标准的<code>Kubernetes</code>支持的安装，则启动<code>kubelet</code>的时候指定参数<code>--cluster-domain</code>，你还需要指定<code>--cluster-dns</code>告诉<code>kubelet</code>集群<code>DNS</code>的地址。</p>\n<h2 id=\"6-3-如何发现和使用服务？\"><a href=\"#6-3-如何发现和使用服务？\" class=\"headerlink\" title=\"6.3 如何发现和使用服务？\"></a>6.3 如何发现和使用服务？</h2><p>一般在创建<code>Pod</code>的时候，指定一个环境变量<code>GET_HOSTS_FROM</code>，值可以设为<code>env</code>或者<code>dns</code>。在<code>Pod</code>中的应用先获取这个环境变量，得到获取服务的方式。如果是<code>env</code>，则通过<code>getenv</code>获取相应的服务的环境变量，例如<code>REDIS_SLAVE_SERVICE_HOST</code>；如果是<code>dns</code>，则可以在<code>Pod</code>内通过标准的<code>gethostbyname</code>获取服务主机名。有个例外是P<code>od</code>的定义中，不能设置h<code>ostNetwork: true</code>。</p>\n<p>获取到服务的地址，就可以通过正常方式使用服务了。</p>\n<p>如下是<code>Kubernetes</code>自带的<code>guestbook.php</code>中的一段相关代码，供参考：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\">host = <span class=\"string\">'redis-slave'</span>;</span></span><br><span class=\"line\">if (getenv('GET_HOSTS_FROM') == 'env') &#123;</span><br><span class=\"line\"><span class=\"meta\">  $</span><span class=\"bash\">host = getenv(<span class=\"string\">'REDIS_SLAVE_SERVICE_HOST'</span>);</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\">client = new Predis\\Client([</span></span><br><span class=\"line\">  'scheme' =&gt; 'tcp',</span><br><span class=\"line\">  'host'   =&gt; $host,</span><br><span class=\"line\">  'port'   =&gt; 6379,</span><br><span class=\"line\">]);</span><br></pre></td></tr></table></figure>\n<h2 id=\"7-一些容器服务中的Service\"><a href=\"#7-一些容器服务中的Service\" class=\"headerlink\" title=\"7. 一些容器服务中的Service\"></a>7. 一些容器服务中的<code>Service</code></h2><p>虽然<code>Service</code>在<code>Kubernetes</code>中如此重要，但是对一些基于<code>Kubernetes</code>的容器服务，并没有使用<code>Service</code>，或者用的是上面讨论的<code>headless</code>类型的<code>Service</code>。这种方式基本上是把容器当做<code>VM</code>使用的典型，<code>LoadBalancer</code>和<code>Pods</code>网络互通，通过<code>LoadBalancer</code>直接把流量转发到<code>Pods</code>上，省却了中间由<code>kube-proxy</code>或者<code>iptables</code>的转发方式，从而提高了流量转发效率，但是也由<code>LoadBalancer</code>自己提供对后端P<code>ods</code>的维护，一般需要<code>LoadBalancer</code>提供动态路由的功能（即后端<code>Pods</code>可以动态地从<code>LoadBalancer</code>上注册/注销）。</p>\n"},{"layout":"post","title":"服务亲和性路由在华为云 k8s 中的实践【Kubernetes中的服务拓扑】","date":"2018-07-07T15:40:18.000Z","author":"zhaojizhuang","mathjax":true,"_content":"\n> 本文根据华为工程师DJ在LC3上的演讲整理：\n\n\n## 1. 拓扑概念\n\n首先，我们讲一下`kubernetes`中的拓扑。根据`kubernetes`现在的设计，我觉得拓扑可以是任意的。用户可以指定任何拓扑关系，比如az(available zone可用区)、region、机架、主机、交换机，甚至发电机。`Kubernetes`中拓扑的概念已经在调度器中被广泛使用。\n\n## 2. `Kubernetes`调度器中的拓扑\n\n在`kubernetes`中，`pod`是工作的基本单元，所以调度器的工作可以简化为“在哪里运行这些`pod`”，当然我们知道`pod`是运行在节点里，但是怎么选择节点，这是调度器要解决的问题。\n\n在`kubernetes`中，我们通过`label`选择节点，从而确定`pod`应该放在哪个节点中。下面是`kubernetes`原生提供的一些`label`：\n\n- k8s.io/hostname\n\n此外，`kubernetes`允许集群管理员和云提供商自定义label，比如机架、磁盘类型等。\n\n## 3.` Kubernetes`中基于拓扑的调度\n\n比如，如果我们要将`PostgreSQL`服务运行在不同的`zone`中，假设`zone`的名字分别是`1a`和`1b`，那么我们可以在`podSpec`中定义节点亲和，如下图所示。主服务器需要运行在`zone a`的节点中，备用服务器需要运行在属于`zone b`的节点中。\n\n![](/images/svctop1.jpeg)\n\n在k`ubernetes`中，`pod`与`node`的亲和或反亲和是刚性的要求。那么我们之前的问题“在哪里运行这些`pod`”就可以简化成“可以在这个节点运行`pod`吗”，答案取决于节点的一些情况，比如节点的名字，节点所属的`region`，节点是否有`SSD`盘等。\n\n另外，调度器还会考虑另外一个问题，“可以把`pod`和其他`pod`放在同一区域吗”，比如，`PostgreSQL`服务肯定不能和`MySQL`服务运行在同一个节点中。`Kubernetes`通过下面三个步骤解决这个问题：\n\n- 1. 定义“其他`pod`”。这个过程与选择`node`的过程类似，我们通过`label`选择`pod`。比如，我们可以选择带有“`app`=`web-frontend`”`label`的`pod`\n\n- 2. 定义“同一区域”。如图中的两个`pod`，是在同一区域吗?不是，因为它们`podSpec`中的t`opologyKey`字段，它是`node label中`的`key`，用于指定拓扑区域，比如`zone`、`rack`、`主机`等。比如如果我们使用“`k8s.io/hostname`”作为`topologyKey`，那么同一区域就表示在同一个主机上。\n\n![](/images/svctop2.jpeg)\n\n我们可以使用之前提到的任何`nodelabel`作为“同一区域”的标识，比如在同一个`zone`。或者使用自定义的`label`，下图给出了通过自定义拓扑`label`创建`node`组。\n\n![](/images/svctop3.jpeg)\n\n- 3. 最后是决定是否可以。这取决于亲和和反亲和（`affinity/anti-affinity`）。\n\n4. `Kubernetes`中其他依赖拓扑关系的特性\n\n上面讲到的部署`pod`时的拓扑感知。除了调度之外，还有许多特性会依赖拓扑关系：\n\n- 工作负载：在缩容或滚动升级的时候，控制器决定先杀掉哪些pod\n\n- 卷存储：卷存储会有拓扑限制，来决定可以挂在卷的node集。比如GCE的持久卷只能挂在在同一个zone的节点上，本地卷被所在节点访问。\n\n## 依赖拓扑的服务亲和性路由\n\n### 1. `Service`和`endpoint`\n\n首先我们来了解一下`kubernetes`中的`service`和`endpoint`的 概念。\n\n`Kubernetes`中的`service`是一个抽象的概念，它通过`label`选择一个`pod`的集合，并且定义了这些`pod`的访问策略。简言之，`service`指定一个虚拟`IP`，作为这些`pod`的访问入口，在4层协议上工作。\n\n`Kubernetes`中的`endpoint`是`service`后端的`pod`的地址列表。作为使用者，我们不需要感知它们，`service`创建的时候`endpoint`会自动创建，并且会根据后端的`pod`自动配置好。\n\n![](/images/svctop4.jpeg)\n\n### 2. `Service`的工作原理\n\n`Endpoints`控制器会`watch`到创建好的`service`和`pod`，然后创建相应的endpoint。`Kube-proxy`会`watch` `service`和`endpoint`，并创建相应的`proxy`规则。在`kubernetes1.8`之前`proxy`是通过底层的`iptables`实现，但是`iptables`只支持随机的负载均衡策略，并且可扩展性很差。\n\n在1.8之后，我们实现并在社区持续推动了基于`ipvs`的`proxy`，这种`proxy`模式相对原来的`iptables`模式有很多优势，比如支持很多负载均衡算法，并且在大规模场景下接近无限扩展性等。好消息是，现在`kubernetes`社区基于`ipvs`的`proxy`已经`svc`，大家可以在生产环境使用。那么问题来了，既然我们已经有`IPVS`加持了，为什么还需要服务亲和性路由呢？\n\n![](/images/svctop5.jpeg)\n\n### 3. 服务亲和性路由\n\n先看一下用户的使用场景，当我们将`pod`正确的放到了用户指定的区域之后，就会有下面的问题。\n\n- 1. **单节点通信**（访问`serviceIP`的时候只能访问到本节点的应用）\n\n    - 我们使用`daemonset`部署`fluent`的时候，应用只需要与当前节点的`fluent`通信。\n\n    - 一些用户出于安全考虑，希望确保他们只能与本地节点的服务通信，因为跨节点的流量可能会携带来自其他节点的敏感信息。\n\n- 2. **限制跨zone的流量**\n\n    -  因为跨`zone`的流量会收费，而同一个`zone`的流量则不会。而有些云提供商，比如阿里云甚至不允许跨`zone`的流量。\n\n    -  性能优势：显然，到达本区域（节点/`zone`等）的流量肯定比跨区访问的流量有更低的延时和更高的带宽。\n\n### 4. 亲和性路由的实现需要解决的问题\n\n正如我们前边讲到的，本地意味着一定的拓扑等级，我们需要有一个可以根据拓扑选择`endpoint`子集的机制。\n\n这样我们就面临着如下问题：\n\n- 是软亲和还是硬亲和。硬亲和意味着只需要本地的后端，而软亲和意味着首先尝试本地的，如果本地没有则尝试更广范围的。\n\n- 如果是软亲和，那么判定标准是什么呢？可能给每个拓扑区域增加权重是一个解决方案。\n\n- 如果多个后端满足条件，那么选择的依据又是什么呢？随机选择还是引入概率？\n\n### 5. 我们的方案\n\n我们提供了一个解决方案，引用一种新的资源“`ServicePolicy`”。集群管理员可以通过`ServicePolicy`配置“`local`”的选择标准，以及各种拓扑的权重。`ServicePolicy`是一种可选的`namespace`范围内的资源，有三种模式：`Required/Perferred/Ignored`，分别代表硬亲和/软亲和/忽略。\n\n![](/images/svctop6.jpeg)\n\n上图是我们引入和`ServicePolicy`资源和`endpoint`引入的字段示例。在我们的示例中，`ServicePolicy`会选择`namespace foo`中带有`label app=bar`的`service`。由于我们将`hostname`设置为`ServicePolicy`的拓扑依据，那么对这些`service`的访问会只路由到与`kube-proxy`有在同一个`host`的后端。\n\n需要说明的是，`service`和`ServicePolicy`是多对多的关系，一个`ServicePolicy`可以通过label选择多个`service`，一个`service`也可以被多个`ServicePolicy`选中，有多个亲和性要求。\n\n另外我们还希望`endpoint`携带节点的拓扑信息，因此我们为`endpoint`添加一个新的字段`Topology`，用于识别`pod`属于的拓扑区域，比如在哪个`host/rack/zone/region`等。\n\n这会改变现有的逻辑。如下图所示，`Endpoint`控制器需要watch两种新的资源，`node`和`ServicePolicy`，它需要维护`node`与`endpoint`的对应关系，并根据`node`的拓扑信息更新`endpoint`的`Topology`字段。另外，`kube-proxy`也会相应地作一些改动。它需要过滤掉与自身不在同一个拓扑区域的`endpoint`，这意味着`kube-proxy`会在不同的节点上创建不同的规则。\n\n![](/images/svctop7.jpeg)\n\n下图表示从`ServicePolicy`到`proxy`规则的数据流。首先`ServicePolicy`通过`label`选择一组`service`，我们可以根据这些`service`找到它们的`pod`。`Endpoint`控制器会将`pod`所在节点的拓扑`label`放到对应的`endpoint`中。`Kube-proxy`负责仅为处在同一拓扑区域的`endpoint`创建`proxy`规则，并且当多个`endpoint`满足要求时提供路由策略。\n\n![](/images/svctop8.jpeg)\n\n[总 结]\n\n目前我们已经在华为云的CCE服务上实现了服务亲和性路由，效果很好，欢迎大家体验。我们很乐意把这个特性开源出来，并且正在做这件事，相信它会像`IPVS`一样，成为`kubernetes`下一个版本的一个重要特性。","source":"_posts/servicetopo.md","raw":"---\nlayout: post\ntitle:  \"服务亲和性路由在华为云 k8s 中的实践【Kubernetes中的服务拓扑】\"\ndate:   2018-07-07 23:40:18 +0800\ncategories: k8s\ntags:  [\"k8s\"]\nauthor: zhaojizhuang\nmathjax: true\n---\n\n> 本文根据华为工程师DJ在LC3上的演讲整理：\n\n\n## 1. 拓扑概念\n\n首先，我们讲一下`kubernetes`中的拓扑。根据`kubernetes`现在的设计，我觉得拓扑可以是任意的。用户可以指定任何拓扑关系，比如az(available zone可用区)、region、机架、主机、交换机，甚至发电机。`Kubernetes`中拓扑的概念已经在调度器中被广泛使用。\n\n## 2. `Kubernetes`调度器中的拓扑\n\n在`kubernetes`中，`pod`是工作的基本单元，所以调度器的工作可以简化为“在哪里运行这些`pod`”，当然我们知道`pod`是运行在节点里，但是怎么选择节点，这是调度器要解决的问题。\n\n在`kubernetes`中，我们通过`label`选择节点，从而确定`pod`应该放在哪个节点中。下面是`kubernetes`原生提供的一些`label`：\n\n- k8s.io/hostname\n\n此外，`kubernetes`允许集群管理员和云提供商自定义label，比如机架、磁盘类型等。\n\n## 3.` Kubernetes`中基于拓扑的调度\n\n比如，如果我们要将`PostgreSQL`服务运行在不同的`zone`中，假设`zone`的名字分别是`1a`和`1b`，那么我们可以在`podSpec`中定义节点亲和，如下图所示。主服务器需要运行在`zone a`的节点中，备用服务器需要运行在属于`zone b`的节点中。\n\n![](/images/svctop1.jpeg)\n\n在k`ubernetes`中，`pod`与`node`的亲和或反亲和是刚性的要求。那么我们之前的问题“在哪里运行这些`pod`”就可以简化成“可以在这个节点运行`pod`吗”，答案取决于节点的一些情况，比如节点的名字，节点所属的`region`，节点是否有`SSD`盘等。\n\n另外，调度器还会考虑另外一个问题，“可以把`pod`和其他`pod`放在同一区域吗”，比如，`PostgreSQL`服务肯定不能和`MySQL`服务运行在同一个节点中。`Kubernetes`通过下面三个步骤解决这个问题：\n\n- 1. 定义“其他`pod`”。这个过程与选择`node`的过程类似，我们通过`label`选择`pod`。比如，我们可以选择带有“`app`=`web-frontend`”`label`的`pod`\n\n- 2. 定义“同一区域”。如图中的两个`pod`，是在同一区域吗?不是，因为它们`podSpec`中的t`opologyKey`字段，它是`node label中`的`key`，用于指定拓扑区域，比如`zone`、`rack`、`主机`等。比如如果我们使用“`k8s.io/hostname`”作为`topologyKey`，那么同一区域就表示在同一个主机上。\n\n![](/images/svctop2.jpeg)\n\n我们可以使用之前提到的任何`nodelabel`作为“同一区域”的标识，比如在同一个`zone`。或者使用自定义的`label`，下图给出了通过自定义拓扑`label`创建`node`组。\n\n![](/images/svctop3.jpeg)\n\n- 3. 最后是决定是否可以。这取决于亲和和反亲和（`affinity/anti-affinity`）。\n\n4. `Kubernetes`中其他依赖拓扑关系的特性\n\n上面讲到的部署`pod`时的拓扑感知。除了调度之外，还有许多特性会依赖拓扑关系：\n\n- 工作负载：在缩容或滚动升级的时候，控制器决定先杀掉哪些pod\n\n- 卷存储：卷存储会有拓扑限制，来决定可以挂在卷的node集。比如GCE的持久卷只能挂在在同一个zone的节点上，本地卷被所在节点访问。\n\n## 依赖拓扑的服务亲和性路由\n\n### 1. `Service`和`endpoint`\n\n首先我们来了解一下`kubernetes`中的`service`和`endpoint`的 概念。\n\n`Kubernetes`中的`service`是一个抽象的概念，它通过`label`选择一个`pod`的集合，并且定义了这些`pod`的访问策略。简言之，`service`指定一个虚拟`IP`，作为这些`pod`的访问入口，在4层协议上工作。\n\n`Kubernetes`中的`endpoint`是`service`后端的`pod`的地址列表。作为使用者，我们不需要感知它们，`service`创建的时候`endpoint`会自动创建，并且会根据后端的`pod`自动配置好。\n\n![](/images/svctop4.jpeg)\n\n### 2. `Service`的工作原理\n\n`Endpoints`控制器会`watch`到创建好的`service`和`pod`，然后创建相应的endpoint。`Kube-proxy`会`watch` `service`和`endpoint`，并创建相应的`proxy`规则。在`kubernetes1.8`之前`proxy`是通过底层的`iptables`实现，但是`iptables`只支持随机的负载均衡策略，并且可扩展性很差。\n\n在1.8之后，我们实现并在社区持续推动了基于`ipvs`的`proxy`，这种`proxy`模式相对原来的`iptables`模式有很多优势，比如支持很多负载均衡算法，并且在大规模场景下接近无限扩展性等。好消息是，现在`kubernetes`社区基于`ipvs`的`proxy`已经`svc`，大家可以在生产环境使用。那么问题来了，既然我们已经有`IPVS`加持了，为什么还需要服务亲和性路由呢？\n\n![](/images/svctop5.jpeg)\n\n### 3. 服务亲和性路由\n\n先看一下用户的使用场景，当我们将`pod`正确的放到了用户指定的区域之后，就会有下面的问题。\n\n- 1. **单节点通信**（访问`serviceIP`的时候只能访问到本节点的应用）\n\n    - 我们使用`daemonset`部署`fluent`的时候，应用只需要与当前节点的`fluent`通信。\n\n    - 一些用户出于安全考虑，希望确保他们只能与本地节点的服务通信，因为跨节点的流量可能会携带来自其他节点的敏感信息。\n\n- 2. **限制跨zone的流量**\n\n    -  因为跨`zone`的流量会收费，而同一个`zone`的流量则不会。而有些云提供商，比如阿里云甚至不允许跨`zone`的流量。\n\n    -  性能优势：显然，到达本区域（节点/`zone`等）的流量肯定比跨区访问的流量有更低的延时和更高的带宽。\n\n### 4. 亲和性路由的实现需要解决的问题\n\n正如我们前边讲到的，本地意味着一定的拓扑等级，我们需要有一个可以根据拓扑选择`endpoint`子集的机制。\n\n这样我们就面临着如下问题：\n\n- 是软亲和还是硬亲和。硬亲和意味着只需要本地的后端，而软亲和意味着首先尝试本地的，如果本地没有则尝试更广范围的。\n\n- 如果是软亲和，那么判定标准是什么呢？可能给每个拓扑区域增加权重是一个解决方案。\n\n- 如果多个后端满足条件，那么选择的依据又是什么呢？随机选择还是引入概率？\n\n### 5. 我们的方案\n\n我们提供了一个解决方案，引用一种新的资源“`ServicePolicy`”。集群管理员可以通过`ServicePolicy`配置“`local`”的选择标准，以及各种拓扑的权重。`ServicePolicy`是一种可选的`namespace`范围内的资源，有三种模式：`Required/Perferred/Ignored`，分别代表硬亲和/软亲和/忽略。\n\n![](/images/svctop6.jpeg)\n\n上图是我们引入和`ServicePolicy`资源和`endpoint`引入的字段示例。在我们的示例中，`ServicePolicy`会选择`namespace foo`中带有`label app=bar`的`service`。由于我们将`hostname`设置为`ServicePolicy`的拓扑依据，那么对这些`service`的访问会只路由到与`kube-proxy`有在同一个`host`的后端。\n\n需要说明的是，`service`和`ServicePolicy`是多对多的关系，一个`ServicePolicy`可以通过label选择多个`service`，一个`service`也可以被多个`ServicePolicy`选中，有多个亲和性要求。\n\n另外我们还希望`endpoint`携带节点的拓扑信息，因此我们为`endpoint`添加一个新的字段`Topology`，用于识别`pod`属于的拓扑区域，比如在哪个`host/rack/zone/region`等。\n\n这会改变现有的逻辑。如下图所示，`Endpoint`控制器需要watch两种新的资源，`node`和`ServicePolicy`，它需要维护`node`与`endpoint`的对应关系，并根据`node`的拓扑信息更新`endpoint`的`Topology`字段。另外，`kube-proxy`也会相应地作一些改动。它需要过滤掉与自身不在同一个拓扑区域的`endpoint`，这意味着`kube-proxy`会在不同的节点上创建不同的规则。\n\n![](/images/svctop7.jpeg)\n\n下图表示从`ServicePolicy`到`proxy`规则的数据流。首先`ServicePolicy`通过`label`选择一组`service`，我们可以根据这些`service`找到它们的`pod`。`Endpoint`控制器会将`pod`所在节点的拓扑`label`放到对应的`endpoint`中。`Kube-proxy`负责仅为处在同一拓扑区域的`endpoint`创建`proxy`规则，并且当多个`endpoint`满足要求时提供路由策略。\n\n![](/images/svctop8.jpeg)\n\n[总 结]\n\n目前我们已经在华为云的CCE服务上实现了服务亲和性路由，效果很好，欢迎大家体验。我们很乐意把这个特性开源出来，并且正在做这件事，相信它会像`IPVS`一样，成为`kubernetes`下一个版本的一个重要特性。","slug":"servicetopo","published":1,"updated":"2020-05-20T12:24:18.233Z","comments":1,"photos":[],"link":"","_id":"ckcnd4l8q000sr5fl19gc0xto","content":"<blockquote>\n<p>本文根据华为工程师DJ在LC3上的演讲整理：</p>\n</blockquote>\n<h2 id=\"1-拓扑概念\"><a href=\"#1-拓扑概念\" class=\"headerlink\" title=\"1. 拓扑概念\"></a>1. 拓扑概念</h2><p>首先，我们讲一下<code>kubernetes</code>中的拓扑。根据<code>kubernetes</code>现在的设计，我觉得拓扑可以是任意的。用户可以指定任何拓扑关系，比如az(available zone可用区)、region、机架、主机、交换机，甚至发电机。<code>Kubernetes</code>中拓扑的概念已经在调度器中被广泛使用。</p>\n<h2 id=\"2-Kubernetes调度器中的拓扑\"><a href=\"#2-Kubernetes调度器中的拓扑\" class=\"headerlink\" title=\"2. Kubernetes调度器中的拓扑\"></a>2. <code>Kubernetes</code>调度器中的拓扑</h2><p>在<code>kubernetes</code>中，<code>pod</code>是工作的基本单元，所以调度器的工作可以简化为“在哪里运行这些<code>pod</code>”，当然我们知道<code>pod</code>是运行在节点里，但是怎么选择节点，这是调度器要解决的问题。</p>\n<p>在<code>kubernetes</code>中，我们通过<code>label</code>选择节点，从而确定<code>pod</code>应该放在哪个节点中。下面是<code>kubernetes</code>原生提供的一些<code>label</code>：</p>\n<ul>\n<li>k8s.io/hostname</li>\n</ul>\n<p>此外，<code>kubernetes</code>允许集群管理员和云提供商自定义label，比如机架、磁盘类型等。</p>\n<h2 id=\"3-Kubernetes中基于拓扑的调度\"><a href=\"#3-Kubernetes中基于拓扑的调度\" class=\"headerlink\" title=\"3.Kubernetes中基于拓扑的调度\"></a>3.<code>Kubernetes</code>中基于拓扑的调度</h2><p>比如，如果我们要将<code>PostgreSQL</code>服务运行在不同的<code>zone</code>中，假设<code>zone</code>的名字分别是<code>1a</code>和<code>1b</code>，那么我们可以在<code>podSpec</code>中定义节点亲和，如下图所示。主服务器需要运行在<code>zone a</code>的节点中，备用服务器需要运行在属于<code>zone b</code>的节点中。</p>\n<p><img src=\"/images/svctop1.jpeg\" alt=\"\"></p>\n<p>在k<code>ubernetes</code>中，<code>pod</code>与<code>node</code>的亲和或反亲和是刚性的要求。那么我们之前的问题“在哪里运行这些<code>pod</code>”就可以简化成“可以在这个节点运行<code>pod</code>吗”，答案取决于节点的一些情况，比如节点的名字，节点所属的<code>region</code>，节点是否有<code>SSD</code>盘等。</p>\n<p>另外，调度器还会考虑另外一个问题，“可以把<code>pod</code>和其他<code>pod</code>放在同一区域吗”，比如，<code>PostgreSQL</code>服务肯定不能和<code>MySQL</code>服务运行在同一个节点中。<code>Kubernetes</code>通过下面三个步骤解决这个问题：</p>\n<ul>\n<li><ol>\n<li>定义“其他<code>pod</code>”。这个过程与选择<code>node</code>的过程类似，我们通过<code>label</code>选择<code>pod</code>。比如，我们可以选择带有“<code>app</code>=<code>web-frontend</code>”<code>label</code>的<code>pod</code></li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li>定义“同一区域”。如图中的两个<code>pod</code>，是在同一区域吗?不是，因为它们<code>podSpec</code>中的t<code>opologyKey</code>字段，它是<code>node label中</code>的<code>key</code>，用于指定拓扑区域，比如<code>zone</code>、<code>rack</code>、<code>主机</code>等。比如如果我们使用“<code>k8s.io/hostname</code>”作为<code>topologyKey</code>，那么同一区域就表示在同一个主机上。</li>\n</ol>\n</li>\n</ul>\n<p><img src=\"/images/svctop2.jpeg\" alt=\"\"></p>\n<p>我们可以使用之前提到的任何<code>nodelabel</code>作为“同一区域”的标识，比如在同一个<code>zone</code>。或者使用自定义的<code>label</code>，下图给出了通过自定义拓扑<code>label</code>创建<code>node</code>组。</p>\n<p><img src=\"/images/svctop3.jpeg\" alt=\"\"></p>\n<ul>\n<li><ol start=\"3\">\n<li>最后是决定是否可以。这取决于亲和和反亲和（<code>affinity/anti-affinity</code>）。</li>\n</ol>\n</li>\n</ul>\n<ol start=\"4\">\n<li><code>Kubernetes</code>中其他依赖拓扑关系的特性</li>\n</ol>\n<p>上面讲到的部署<code>pod</code>时的拓扑感知。除了调度之外，还有许多特性会依赖拓扑关系：</p>\n<ul>\n<li><p>工作负载：在缩容或滚动升级的时候，控制器决定先杀掉哪些pod</p>\n</li>\n<li><p>卷存储：卷存储会有拓扑限制，来决定可以挂在卷的node集。比如GCE的持久卷只能挂在在同一个zone的节点上，本地卷被所在节点访问。</p>\n</li>\n</ul>\n<h2 id=\"依赖拓扑的服务亲和性路由\"><a href=\"#依赖拓扑的服务亲和性路由\" class=\"headerlink\" title=\"依赖拓扑的服务亲和性路由\"></a>依赖拓扑的服务亲和性路由</h2><h3 id=\"1-Service和endpoint\"><a href=\"#1-Service和endpoint\" class=\"headerlink\" title=\"1. Service和endpoint\"></a>1. <code>Service</code>和<code>endpoint</code></h3><p>首先我们来了解一下<code>kubernetes</code>中的<code>service</code>和<code>endpoint</code>的 概念。</p>\n<p><code>Kubernetes</code>中的<code>service</code>是一个抽象的概念，它通过<code>label</code>选择一个<code>pod</code>的集合，并且定义了这些<code>pod</code>的访问策略。简言之，<code>service</code>指定一个虚拟<code>IP</code>，作为这些<code>pod</code>的访问入口，在4层协议上工作。</p>\n<p><code>Kubernetes</code>中的<code>endpoint</code>是<code>service</code>后端的<code>pod</code>的地址列表。作为使用者，我们不需要感知它们，<code>service</code>创建的时候<code>endpoint</code>会自动创建，并且会根据后端的<code>pod</code>自动配置好。</p>\n<p><img src=\"/images/svctop4.jpeg\" alt=\"\"></p>\n<h3 id=\"2-Service的工作原理\"><a href=\"#2-Service的工作原理\" class=\"headerlink\" title=\"2. Service的工作原理\"></a>2. <code>Service</code>的工作原理</h3><p><code>Endpoints</code>控制器会<code>watch</code>到创建好的<code>service</code>和<code>pod</code>，然后创建相应的endpoint。<code>Kube-proxy</code>会<code>watch</code> <code>service</code>和<code>endpoint</code>，并创建相应的<code>proxy</code>规则。在<code>kubernetes1.8</code>之前<code>proxy</code>是通过底层的<code>iptables</code>实现，但是<code>iptables</code>只支持随机的负载均衡策略，并且可扩展性很差。</p>\n<p>在1.8之后，我们实现并在社区持续推动了基于<code>ipvs</code>的<code>proxy</code>，这种<code>proxy</code>模式相对原来的<code>iptables</code>模式有很多优势，比如支持很多负载均衡算法，并且在大规模场景下接近无限扩展性等。好消息是，现在<code>kubernetes</code>社区基于<code>ipvs</code>的<code>proxy</code>已经<code>svc</code>，大家可以在生产环境使用。那么问题来了，既然我们已经有<code>IPVS</code>加持了，为什么还需要服务亲和性路由呢？</p>\n<p><img src=\"/images/svctop5.jpeg\" alt=\"\"></p>\n<h3 id=\"3-服务亲和性路由\"><a href=\"#3-服务亲和性路由\" class=\"headerlink\" title=\"3. 服务亲和性路由\"></a>3. 服务亲和性路由</h3><p>先看一下用户的使用场景，当我们将<code>pod</code>正确的放到了用户指定的区域之后，就会有下面的问题。</p>\n<ul>\n<li><ol>\n<li><p><strong>单节点通信</strong>（访问<code>serviceIP</code>的时候只能访问到本节点的应用）</p>\n<ul>\n<li><p>我们使用<code>daemonset</code>部署<code>fluent</code>的时候，应用只需要与当前节点的<code>fluent</code>通信。</p>\n</li>\n<li><p>一些用户出于安全考虑，希望确保他们只能与本地节点的服务通信，因为跨节点的流量可能会携带来自其他节点的敏感信息。</p>\n</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li><p><strong>限制跨zone的流量</strong></p>\n<ul>\n<li><p>因为跨<code>zone</code>的流量会收费，而同一个<code>zone</code>的流量则不会。而有些云提供商，比如阿里云甚至不允许跨<code>zone</code>的流量。</p>\n</li>\n<li><p>性能优势：显然，到达本区域（节点/<code>zone</code>等）的流量肯定比跨区访问的流量有更低的延时和更高的带宽。</p>\n</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"4-亲和性路由的实现需要解决的问题\"><a href=\"#4-亲和性路由的实现需要解决的问题\" class=\"headerlink\" title=\"4. 亲和性路由的实现需要解决的问题\"></a>4. 亲和性路由的实现需要解决的问题</h3><p>正如我们前边讲到的，本地意味着一定的拓扑等级，我们需要有一个可以根据拓扑选择<code>endpoint</code>子集的机制。</p>\n<p>这样我们就面临着如下问题：</p>\n<ul>\n<li><p>是软亲和还是硬亲和。硬亲和意味着只需要本地的后端，而软亲和意味着首先尝试本地的，如果本地没有则尝试更广范围的。</p>\n</li>\n<li><p>如果是软亲和，那么判定标准是什么呢？可能给每个拓扑区域增加权重是一个解决方案。</p>\n</li>\n<li><p>如果多个后端满足条件，那么选择的依据又是什么呢？随机选择还是引入概率？</p>\n</li>\n</ul>\n<h3 id=\"5-我们的方案\"><a href=\"#5-我们的方案\" class=\"headerlink\" title=\"5. 我们的方案\"></a>5. 我们的方案</h3><p>我们提供了一个解决方案，引用一种新的资源“<code>ServicePolicy</code>”。集群管理员可以通过<code>ServicePolicy</code>配置“<code>local</code>”的选择标准，以及各种拓扑的权重。<code>ServicePolicy</code>是一种可选的<code>namespace</code>范围内的资源，有三种模式：<code>Required/Perferred/Ignored</code>，分别代表硬亲和/软亲和/忽略。</p>\n<p><img src=\"/images/svctop6.jpeg\" alt=\"\"></p>\n<p>上图是我们引入和<code>ServicePolicy</code>资源和<code>endpoint</code>引入的字段示例。在我们的示例中，<code>ServicePolicy</code>会选择<code>namespace foo</code>中带有<code>label app=bar</code>的<code>service</code>。由于我们将<code>hostname</code>设置为<code>ServicePolicy</code>的拓扑依据，那么对这些<code>service</code>的访问会只路由到与<code>kube-proxy</code>有在同一个<code>host</code>的后端。</p>\n<p>需要说明的是，<code>service</code>和<code>ServicePolicy</code>是多对多的关系，一个<code>ServicePolicy</code>可以通过label选择多个<code>service</code>，一个<code>service</code>也可以被多个<code>ServicePolicy</code>选中，有多个亲和性要求。</p>\n<p>另外我们还希望<code>endpoint</code>携带节点的拓扑信息，因此我们为<code>endpoint</code>添加一个新的字段<code>Topology</code>，用于识别<code>pod</code>属于的拓扑区域，比如在哪个<code>host/rack/zone/region</code>等。</p>\n<p>这会改变现有的逻辑。如下图所示，<code>Endpoint</code>控制器需要watch两种新的资源，<code>node</code>和<code>ServicePolicy</code>，它需要维护<code>node</code>与<code>endpoint</code>的对应关系，并根据<code>node</code>的拓扑信息更新<code>endpoint</code>的<code>Topology</code>字段。另外，<code>kube-proxy</code>也会相应地作一些改动。它需要过滤掉与自身不在同一个拓扑区域的<code>endpoint</code>，这意味着<code>kube-proxy</code>会在不同的节点上创建不同的规则。</p>\n<p><img src=\"/images/svctop7.jpeg\" alt=\"\"></p>\n<p>下图表示从<code>ServicePolicy</code>到<code>proxy</code>规则的数据流。首先<code>ServicePolicy</code>通过<code>label</code>选择一组<code>service</code>，我们可以根据这些<code>service</code>找到它们的<code>pod</code>。<code>Endpoint</code>控制器会将<code>pod</code>所在节点的拓扑<code>label</code>放到对应的<code>endpoint</code>中。<code>Kube-proxy</code>负责仅为处在同一拓扑区域的<code>endpoint</code>创建<code>proxy</code>规则，并且当多个<code>endpoint</code>满足要求时提供路由策略。</p>\n<p><img src=\"/images/svctop8.jpeg\" alt=\"\"></p>\n<p>[总 结]</p>\n<p>目前我们已经在华为云的CCE服务上实现了服务亲和性路由，效果很好，欢迎大家体验。我们很乐意把这个特性开源出来，并且正在做这件事，相信它会像<code>IPVS</code>一样，成为<code>kubernetes</code>下一个版本的一个重要特性。</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>本文根据华为工程师DJ在LC3上的演讲整理：</p>\n</blockquote>\n<h2 id=\"1-拓扑概念\"><a href=\"#1-拓扑概念\" class=\"headerlink\" title=\"1. 拓扑概念\"></a>1. 拓扑概念</h2><p>首先，我们讲一下<code>kubernetes</code>中的拓扑。根据<code>kubernetes</code>现在的设计，我觉得拓扑可以是任意的。用户可以指定任何拓扑关系，比如az(available zone可用区)、region、机架、主机、交换机，甚至发电机。<code>Kubernetes</code>中拓扑的概念已经在调度器中被广泛使用。</p>\n<h2 id=\"2-Kubernetes调度器中的拓扑\"><a href=\"#2-Kubernetes调度器中的拓扑\" class=\"headerlink\" title=\"2. Kubernetes调度器中的拓扑\"></a>2. <code>Kubernetes</code>调度器中的拓扑</h2><p>在<code>kubernetes</code>中，<code>pod</code>是工作的基本单元，所以调度器的工作可以简化为“在哪里运行这些<code>pod</code>”，当然我们知道<code>pod</code>是运行在节点里，但是怎么选择节点，这是调度器要解决的问题。</p>\n<p>在<code>kubernetes</code>中，我们通过<code>label</code>选择节点，从而确定<code>pod</code>应该放在哪个节点中。下面是<code>kubernetes</code>原生提供的一些<code>label</code>：</p>\n<ul>\n<li>k8s.io/hostname</li>\n</ul>\n<p>此外，<code>kubernetes</code>允许集群管理员和云提供商自定义label，比如机架、磁盘类型等。</p>\n<h2 id=\"3-Kubernetes中基于拓扑的调度\"><a href=\"#3-Kubernetes中基于拓扑的调度\" class=\"headerlink\" title=\"3.Kubernetes中基于拓扑的调度\"></a>3.<code>Kubernetes</code>中基于拓扑的调度</h2><p>比如，如果我们要将<code>PostgreSQL</code>服务运行在不同的<code>zone</code>中，假设<code>zone</code>的名字分别是<code>1a</code>和<code>1b</code>，那么我们可以在<code>podSpec</code>中定义节点亲和，如下图所示。主服务器需要运行在<code>zone a</code>的节点中，备用服务器需要运行在属于<code>zone b</code>的节点中。</p>\n<p><img src=\"/images/svctop1.jpeg\" alt=\"\"></p>\n<p>在k<code>ubernetes</code>中，<code>pod</code>与<code>node</code>的亲和或反亲和是刚性的要求。那么我们之前的问题“在哪里运行这些<code>pod</code>”就可以简化成“可以在这个节点运行<code>pod</code>吗”，答案取决于节点的一些情况，比如节点的名字，节点所属的<code>region</code>，节点是否有<code>SSD</code>盘等。</p>\n<p>另外，调度器还会考虑另外一个问题，“可以把<code>pod</code>和其他<code>pod</code>放在同一区域吗”，比如，<code>PostgreSQL</code>服务肯定不能和<code>MySQL</code>服务运行在同一个节点中。<code>Kubernetes</code>通过下面三个步骤解决这个问题：</p>\n<ul>\n<li><ol>\n<li>定义“其他<code>pod</code>”。这个过程与选择<code>node</code>的过程类似，我们通过<code>label</code>选择<code>pod</code>。比如，我们可以选择带有“<code>app</code>=<code>web-frontend</code>”<code>label</code>的<code>pod</code></li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li>定义“同一区域”。如图中的两个<code>pod</code>，是在同一区域吗?不是，因为它们<code>podSpec</code>中的t<code>opologyKey</code>字段，它是<code>node label中</code>的<code>key</code>，用于指定拓扑区域，比如<code>zone</code>、<code>rack</code>、<code>主机</code>等。比如如果我们使用“<code>k8s.io/hostname</code>”作为<code>topologyKey</code>，那么同一区域就表示在同一个主机上。</li>\n</ol>\n</li>\n</ul>\n<p><img src=\"/images/svctop2.jpeg\" alt=\"\"></p>\n<p>我们可以使用之前提到的任何<code>nodelabel</code>作为“同一区域”的标识，比如在同一个<code>zone</code>。或者使用自定义的<code>label</code>，下图给出了通过自定义拓扑<code>label</code>创建<code>node</code>组。</p>\n<p><img src=\"/images/svctop3.jpeg\" alt=\"\"></p>\n<ul>\n<li><ol start=\"3\">\n<li>最后是决定是否可以。这取决于亲和和反亲和（<code>affinity/anti-affinity</code>）。</li>\n</ol>\n</li>\n</ul>\n<ol start=\"4\">\n<li><code>Kubernetes</code>中其他依赖拓扑关系的特性</li>\n</ol>\n<p>上面讲到的部署<code>pod</code>时的拓扑感知。除了调度之外，还有许多特性会依赖拓扑关系：</p>\n<ul>\n<li><p>工作负载：在缩容或滚动升级的时候，控制器决定先杀掉哪些pod</p>\n</li>\n<li><p>卷存储：卷存储会有拓扑限制，来决定可以挂在卷的node集。比如GCE的持久卷只能挂在在同一个zone的节点上，本地卷被所在节点访问。</p>\n</li>\n</ul>\n<h2 id=\"依赖拓扑的服务亲和性路由\"><a href=\"#依赖拓扑的服务亲和性路由\" class=\"headerlink\" title=\"依赖拓扑的服务亲和性路由\"></a>依赖拓扑的服务亲和性路由</h2><h3 id=\"1-Service和endpoint\"><a href=\"#1-Service和endpoint\" class=\"headerlink\" title=\"1. Service和endpoint\"></a>1. <code>Service</code>和<code>endpoint</code></h3><p>首先我们来了解一下<code>kubernetes</code>中的<code>service</code>和<code>endpoint</code>的 概念。</p>\n<p><code>Kubernetes</code>中的<code>service</code>是一个抽象的概念，它通过<code>label</code>选择一个<code>pod</code>的集合，并且定义了这些<code>pod</code>的访问策略。简言之，<code>service</code>指定一个虚拟<code>IP</code>，作为这些<code>pod</code>的访问入口，在4层协议上工作。</p>\n<p><code>Kubernetes</code>中的<code>endpoint</code>是<code>service</code>后端的<code>pod</code>的地址列表。作为使用者，我们不需要感知它们，<code>service</code>创建的时候<code>endpoint</code>会自动创建，并且会根据后端的<code>pod</code>自动配置好。</p>\n<p><img src=\"/images/svctop4.jpeg\" alt=\"\"></p>\n<h3 id=\"2-Service的工作原理\"><a href=\"#2-Service的工作原理\" class=\"headerlink\" title=\"2. Service的工作原理\"></a>2. <code>Service</code>的工作原理</h3><p><code>Endpoints</code>控制器会<code>watch</code>到创建好的<code>service</code>和<code>pod</code>，然后创建相应的endpoint。<code>Kube-proxy</code>会<code>watch</code> <code>service</code>和<code>endpoint</code>，并创建相应的<code>proxy</code>规则。在<code>kubernetes1.8</code>之前<code>proxy</code>是通过底层的<code>iptables</code>实现，但是<code>iptables</code>只支持随机的负载均衡策略，并且可扩展性很差。</p>\n<p>在1.8之后，我们实现并在社区持续推动了基于<code>ipvs</code>的<code>proxy</code>，这种<code>proxy</code>模式相对原来的<code>iptables</code>模式有很多优势，比如支持很多负载均衡算法，并且在大规模场景下接近无限扩展性等。好消息是，现在<code>kubernetes</code>社区基于<code>ipvs</code>的<code>proxy</code>已经<code>svc</code>，大家可以在生产环境使用。那么问题来了，既然我们已经有<code>IPVS</code>加持了，为什么还需要服务亲和性路由呢？</p>\n<p><img src=\"/images/svctop5.jpeg\" alt=\"\"></p>\n<h3 id=\"3-服务亲和性路由\"><a href=\"#3-服务亲和性路由\" class=\"headerlink\" title=\"3. 服务亲和性路由\"></a>3. 服务亲和性路由</h3><p>先看一下用户的使用场景，当我们将<code>pod</code>正确的放到了用户指定的区域之后，就会有下面的问题。</p>\n<ul>\n<li><ol>\n<li><p><strong>单节点通信</strong>（访问<code>serviceIP</code>的时候只能访问到本节点的应用）</p>\n<ul>\n<li><p>我们使用<code>daemonset</code>部署<code>fluent</code>的时候，应用只需要与当前节点的<code>fluent</code>通信。</p>\n</li>\n<li><p>一些用户出于安全考虑，希望确保他们只能与本地节点的服务通信，因为跨节点的流量可能会携带来自其他节点的敏感信息。</p>\n</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><ol start=\"2\">\n<li><p><strong>限制跨zone的流量</strong></p>\n<ul>\n<li><p>因为跨<code>zone</code>的流量会收费，而同一个<code>zone</code>的流量则不会。而有些云提供商，比如阿里云甚至不允许跨<code>zone</code>的流量。</p>\n</li>\n<li><p>性能优势：显然，到达本区域（节点/<code>zone</code>等）的流量肯定比跨区访问的流量有更低的延时和更高的带宽。</p>\n</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"4-亲和性路由的实现需要解决的问题\"><a href=\"#4-亲和性路由的实现需要解决的问题\" class=\"headerlink\" title=\"4. 亲和性路由的实现需要解决的问题\"></a>4. 亲和性路由的实现需要解决的问题</h3><p>正如我们前边讲到的，本地意味着一定的拓扑等级，我们需要有一个可以根据拓扑选择<code>endpoint</code>子集的机制。</p>\n<p>这样我们就面临着如下问题：</p>\n<ul>\n<li><p>是软亲和还是硬亲和。硬亲和意味着只需要本地的后端，而软亲和意味着首先尝试本地的，如果本地没有则尝试更广范围的。</p>\n</li>\n<li><p>如果是软亲和，那么判定标准是什么呢？可能给每个拓扑区域增加权重是一个解决方案。</p>\n</li>\n<li><p>如果多个后端满足条件，那么选择的依据又是什么呢？随机选择还是引入概率？</p>\n</li>\n</ul>\n<h3 id=\"5-我们的方案\"><a href=\"#5-我们的方案\" class=\"headerlink\" title=\"5. 我们的方案\"></a>5. 我们的方案</h3><p>我们提供了一个解决方案，引用一种新的资源“<code>ServicePolicy</code>”。集群管理员可以通过<code>ServicePolicy</code>配置“<code>local</code>”的选择标准，以及各种拓扑的权重。<code>ServicePolicy</code>是一种可选的<code>namespace</code>范围内的资源，有三种模式：<code>Required/Perferred/Ignored</code>，分别代表硬亲和/软亲和/忽略。</p>\n<p><img src=\"/images/svctop6.jpeg\" alt=\"\"></p>\n<p>上图是我们引入和<code>ServicePolicy</code>资源和<code>endpoint</code>引入的字段示例。在我们的示例中，<code>ServicePolicy</code>会选择<code>namespace foo</code>中带有<code>label app=bar</code>的<code>service</code>。由于我们将<code>hostname</code>设置为<code>ServicePolicy</code>的拓扑依据，那么对这些<code>service</code>的访问会只路由到与<code>kube-proxy</code>有在同一个<code>host</code>的后端。</p>\n<p>需要说明的是，<code>service</code>和<code>ServicePolicy</code>是多对多的关系，一个<code>ServicePolicy</code>可以通过label选择多个<code>service</code>，一个<code>service</code>也可以被多个<code>ServicePolicy</code>选中，有多个亲和性要求。</p>\n<p>另外我们还希望<code>endpoint</code>携带节点的拓扑信息，因此我们为<code>endpoint</code>添加一个新的字段<code>Topology</code>，用于识别<code>pod</code>属于的拓扑区域，比如在哪个<code>host/rack/zone/region</code>等。</p>\n<p>这会改变现有的逻辑。如下图所示，<code>Endpoint</code>控制器需要watch两种新的资源，<code>node</code>和<code>ServicePolicy</code>，它需要维护<code>node</code>与<code>endpoint</code>的对应关系，并根据<code>node</code>的拓扑信息更新<code>endpoint</code>的<code>Topology</code>字段。另外，<code>kube-proxy</code>也会相应地作一些改动。它需要过滤掉与自身不在同一个拓扑区域的<code>endpoint</code>，这意味着<code>kube-proxy</code>会在不同的节点上创建不同的规则。</p>\n<p><img src=\"/images/svctop7.jpeg\" alt=\"\"></p>\n<p>下图表示从<code>ServicePolicy</code>到<code>proxy</code>规则的数据流。首先<code>ServicePolicy</code>通过<code>label</code>选择一组<code>service</code>，我们可以根据这些<code>service</code>找到它们的<code>pod</code>。<code>Endpoint</code>控制器会将<code>pod</code>所在节点的拓扑<code>label</code>放到对应的<code>endpoint</code>中。<code>Kube-proxy</code>负责仅为处在同一拓扑区域的<code>endpoint</code>创建<code>proxy</code>规则，并且当多个<code>endpoint</code>满足要求时提供路由策略。</p>\n<p><img src=\"/images/svctop8.jpeg\" alt=\"\"></p>\n<p>[总 结]</p>\n<p>目前我们已经在华为云的CCE服务上实现了服务亲和性路由，效果很好，欢迎大家体验。我们很乐意把这个特性开源出来，并且正在做这件事，相信它会像<code>IPVS</code>一样，成为<code>kubernetes</code>下一个版本的一个重要特性。</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ckcnd4l7z0000r5fl0h98ckca","category_id":"ckcnd4l850003r5flr6a9azh5","_id":"ckcnd4l8e000dr5flzfzak5mk"},{"post_id":"ckcnd4l840002r5fltsc7y91y","category_id":"ckcnd4l8b0008r5flwxtqrtfa","_id":"ckcnd4l8h000jr5fl861vpy6a"},{"post_id":"ckcnd4l870005r5flbwdf4jdm","category_id":"ckcnd4l8e000er5flgs76yxya","_id":"ckcnd4l8q000qr5flgxh24lqt"},{"post_id":"ckcnd4l8g000ir5flmmi6e30s","category_id":"ckcnd4l8e000er5flgs76yxya","_id":"ckcnd4l8r000tr5fl4xokgc22"},{"post_id":"ckcnd4l8a0007r5flbysp2k1q","category_id":"ckcnd4l8e000er5flgs76yxya","_id":"ckcnd4l8r000ur5fll2t6l020"},{"post_id":"ckcnd4l8k000or5fltfwv9xns","category_id":"ckcnd4l8e000er5flgs76yxya","_id":"ckcnd4l8s000yr5flg8z6a7to"},{"post_id":"ckcnd4l8q000sr5fl19gc0xto","category_id":"ckcnd4l8e000er5flgs76yxya","_id":"ckcnd4l8s0010r5fl1yhfwt91"},{"post_id":"ckcnd4l8b000br5flo8nomy9e","category_id":"ckcnd4l8p000pr5flh05numnf","_id":"ckcnd4l8t0014r5fld979e8yh"},{"post_id":"ckcnd4l8d000cr5flsi6tmwun","category_id":"ckcnd4l8p000pr5flh05numnf","_id":"ckcnd4l8t0015r5flp54pxx7w"},{"post_id":"ckcnd4l8e000gr5flllcdi90o","category_id":"ckcnd4l8t0011r5fl6g2yyv19","_id":"ckcnd4l8u0019r5fl6bos3ukg"},{"post_id":"ckcnd4l8j000mr5fla2lhy1xn","category_id":"ckcnd4l8p000pr5flh05numnf","_id":"ckcnd4l8v001cr5fl7kqz9tt8"}],"PostTag":[{"post_id":"ckcnd4l7z0000r5fl0h98ckca","tag_id":"ckcnd4l870004r5flnmuhiyv8","_id":"ckcnd4l8b000ar5flj0szzm59"},{"post_id":"ckcnd4l840002r5fltsc7y91y","tag_id":"ckcnd4l8b0009r5flm1yyy5gn","_id":"ckcnd4l8g000hr5fl3fttxaiy"},{"post_id":"ckcnd4l870005r5flbwdf4jdm","tag_id":"ckcnd4l8e000fr5fl5spfknum","_id":"ckcnd4l8k000nr5fl6yrar920"},{"post_id":"ckcnd4l8q000sr5fl19gc0xto","tag_id":"ckcnd4l8e000fr5fl5spfknum","_id":"ckcnd4l8r000vr5flkd774t3e"},{"post_id":"ckcnd4l890006r5flxgkuv528","tag_id":"ckcnd4l8i000lr5flzjhxax01","_id":"ckcnd4l8s000zr5flmh61a338"},{"post_id":"ckcnd4l890006r5flxgkuv528","tag_id":"ckcnd4l8q000rr5fl918vh2hl","_id":"ckcnd4l8t0012r5fl9yad1905"},{"post_id":"ckcnd4l8a0007r5flbysp2k1q","tag_id":"ckcnd4l8e000fr5fl5spfknum","_id":"ckcnd4l8u0018r5flttsyadz9"},{"post_id":"ckcnd4l8a0007r5flbysp2k1q","tag_id":"ckcnd4l8t0013r5fl8v9i6s0j","_id":"ckcnd4l8u001ar5fldrc2coie"},{"post_id":"ckcnd4l8b000br5flo8nomy9e","tag_id":"ckcnd4l8u0017r5flqaxvqsou","_id":"ckcnd4l8w001fr5fl17hjshxi"},{"post_id":"ckcnd4l8b000br5flo8nomy9e","tag_id":"ckcnd4l8q000rr5fl918vh2hl","_id":"ckcnd4l8w001gr5flb3s5fhll"},{"post_id":"ckcnd4l8b000br5flo8nomy9e","tag_id":"ckcnd4l8i000lr5flzjhxax01","_id":"ckcnd4l8w001ir5fla9er90yr"},{"post_id":"ckcnd4l8d000cr5flsi6tmwun","tag_id":"ckcnd4l8u0017r5flqaxvqsou","_id":"ckcnd4l8w001jr5flnkzlutzz"},{"post_id":"ckcnd4l8e000gr5flllcdi90o","tag_id":"ckcnd4l8w001hr5flbxqb6n5j","_id":"ckcnd4l8y001mr5flagtesvgf"},{"post_id":"ckcnd4l8e000gr5flllcdi90o","tag_id":"ckcnd4l8w001kr5flitv6euuh","_id":"ckcnd4l8z001nr5fla6fulwle"},{"post_id":"ckcnd4l8g000ir5flmmi6e30s","tag_id":"ckcnd4l8e000fr5fl5spfknum","_id":"ckcnd4l90001pr5fl1had9j6c"},{"post_id":"ckcnd4l8g000ir5flmmi6e30s","tag_id":"ckcnd4l8x001lr5flgk0svwz8","_id":"ckcnd4l90001qr5flbkuzcxrv"},{"post_id":"ckcnd4l8j000mr5fla2lhy1xn","tag_id":"ckcnd4l8u0017r5flqaxvqsou","_id":"ckcnd4l91001tr5flgewi86nz"},{"post_id":"ckcnd4l8j000mr5fla2lhy1xn","tag_id":"ckcnd4l90001rr5flyijqt60d","_id":"ckcnd4l91001ur5fl7wgmqrse"},{"post_id":"ckcnd4l8k000or5fltfwv9xns","tag_id":"ckcnd4l8e000fr5fl5spfknum","_id":"ckcnd4l91001vr5fl9hi8i6sg"},{"post_id":"ckcnd4l8k000or5fltfwv9xns","tag_id":"ckcnd4l8x001lr5flgk0svwz8","_id":"ckcnd4l91001wr5flhwrsh15l"}],"Tag":[{"name":"docker","_id":"ckcnd4l870004r5flnmuhiyv8"},{"name":" Analytics","_id":"ckcnd4l8b0009r5flm1yyy5gn"},{"name":"k8s","_id":"ckcnd4l8e000fr5fl5spfknum"},{"name":"linux","_id":"ckcnd4l8i000lr5flzjhxax01"},{"name":"epoll","_id":"ckcnd4l8q000rr5fl918vh2hl"},{"name":"容器网络","_id":"ckcnd4l8t0013r5fl8v9i6s0j"},{"name":"Go","_id":"ckcnd4l8u0017r5flqaxvqsou"},{"name":"java","_id":"ckcnd4l8w001hr5flbxqb6n5j"},{"name":"线程","_id":"ckcnd4l8w001kr5flitv6euuh"},{"name":"iptables","_id":"ckcnd4l8x001lr5flgk0svwz8"},{"name":"内存管理","_id":"ckcnd4l90001rr5flyijqt60d"}]}}